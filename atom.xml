<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Andeper的个人博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://andeper.cn/"/>
  <updated>2019-04-03T14:28:19.561Z</updated>
  <id>http://andeper.cn/</id>
  
  <author>
    <name>Andeper</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>文献摘要之Short-Text-Clustering-via-Convolutional-Neural-Networks</title>
    <link href="http://andeper.cn/2019/04/03/%E6%96%87%E7%8C%AE%E6%91%98%E8%A6%81%E4%B9%8BShort-Text-Clustering-via-Convolutional-Neural-Networks/"/>
    <id>http://andeper.cn/2019/04/03/文献摘要之Short-Text-Clustering-via-Convolutional-Neural-Networks/</id>
    <published>2019-04-03T12:37:31.000Z</published>
    <updated>2019-04-03T14:28:19.561Z</updated>
    
    <content type="html"><![CDATA[<p>Short Text Clustering via Convolutional Neural Networks</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Short text clustering has become an increasing important task with the popularity of social media, and it is a challenging problem due to its sparseness of text representation. In this paper, we propose a Short Text Clustering via Convolutional neural networks (abbr. to STCC), which is more beneficial for clustering by considering one constraint on learned features through a self-taught learning framework without using any external tags/labels. First, we embed the original keyword features into compact binary codes with a localitypreserving constraint. Then, word embeddings are explored and fed into convolutional neural networks to learn deep feature representations, with the output units fitting the pre-trained binary code in the training process. After obtaining the learned representations, we use K-means to cluster them. Our extensive experimental study on two public short text datasets shows that the deep feature representation learned by our approach can achieve a significantly better performance than some other existing features, such as term frequency-inverse document frequency, Laplacian eigenvectors and average embedding, for clustering.<br>随着社交媒体的普及，短文本聚类已成为一项日益重要的任务，由于其文本表示的稀疏性，它是一个具有挑战性的问题。在本文中，我们通过卷积神经网络（简称STCC）提出了一种短文本聚类，通过自学习学习框架考虑学习特征的一个约束而不使用任何外部标签/标签，这对聚类更有利。首先，我们将原始关键字特征嵌入到具有局部保持约束的紧凑二进制代码中。然后，探索单词嵌入并将其馈入卷积神经网络以学习深度特征表示，其中输出单元在训练过程中拟合预训练的二进制代码。在获得学习的表示后，我们使用K-means来聚类它们。我们对两个公共短文本数据集的广泛实验研究表明，通过我们的方法学习的深度特征表示可以实现比其他一些现有特征明显更好的性能，例如术语频率 - 逆文档频率，拉普拉斯特征向量和平均嵌入，用于聚类。</p><h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h3><p>Different from the normal text clustering, short text clustering has the problem of sparsity(Aggarwal and Zhai, 2012). Most words only occur once in each short text, as a result, the term frequencyinverse document frequency (TF-IDF) measure cannot work well in the short text setting. In order to address this problem, some researchers work on expanding and enriching the context of data from Wikipedia (Banerjee et al., 2007) or an ontology (Fodeh et al., 2011). However, these methods involve solid natural language processing (NLP) knowledge and still use high-dimensional representation which may result in a waste of both memory and computation time. Another way to overcome these issues is to explore some sophisticated models to cluster short texts. For example, Yin and Wang (2014) proposed a Dirichlet multinomial mixture model-based approach for short text clustering and Cai et al. (2005) clustered texts using Locality Preserving Indexing (LPI) algorithm. Yet how to design an effective model is an open question, and most of these methods directly trained based on bagof-words (BoW) are shallow structures which cannot preserve the accurate semantic similarities.<br>与普通文本聚类不同，短文本聚类具有稀疏性问题（Aggarwal和Zhai，2012）。大多数单词仅在每个短文本中出现一次，因此，术语频率反向文档频率（TF-IDF）度量在短文本设置中不能很好地起作用。为了解决这个问题，一些研究人员致力于扩展和丰富维基百科（Banerjee等，2007）或本体论（Fodeh等，2011）的数据背景。然而，这些方法涉及固体自然语言处理（NLP）知识并且仍然使用高维表示，这可能导致浪费存储器和计算时间。克服这些问题的另一种方法是探索一些复杂的模型来聚类短文本。例如，Yin和Wang（2014）提出了一种基于Dirichlet多项式混合模型的短文本聚类方法和Cai等人。 （2005）使用局部保持索引（LPI）算法的聚类文本。然而，如何设计一个有效的模型是一个悬而未决的问题，而且大多数基于bagof-words（BoW）直接训练的方法都是浅层结构，不能保持准确的语义相似性。<br>With the recent revival of interest in Deep Neural Network (DNN), many researchers have concentrated on using Deep Learning to learn features. Hinton and Salakhutdinov (2006) use deep auto encoder (DAE) to learn text representation from raw text representation. Recently, with the help of word embedding, neural networks demonstrate their great performance in terms of constructing text representation, such as Recursive Neural Network (RecNN) (Socher et al., 2011; Socher et al., 2013) and Recurrent Neural Network (RNN) (Mikolov et al.,2011). However, RecNN exhibits high time complexity to construct the textual tree, and RNN, using the layer computed at the last word to represent the text, is a biased model (Lai et al., 2015). More recently, Convolution Neural Network (CNN), applying convolutional filters to capture local features, has achieved a better performance in many NLP applications, such as sentence modeling (Blunsom et al.,2014), relation classification (Zeng et al., 2014), and other traditional NLP tasks (Collobert et al., 2011).Most of the previous works focus CNN on solving supervised NLP tasks, while in this paper we aim to explore the power of CNN on one unsupervised NLP task, short text clustering.<br>随着最近人们对深度神经网络（DNN）兴趣的兴起，许多研究人员将注意力集中在使用深度学习来学习特征。 Hinton和Salakhutdinov（2006）使用深度自动编码器（DAE）来学习原始文本表示的文本表示。最近，在文字嵌入的帮助下，神经网络在构建文本表示方面表现出了很好的表现，如递归神经网络（RecNN）（Socher等，2011; Socher等，2013）和递归神经网络（ RNN）（Mikolov等，2011）。然而，RecNN表现出构建文本树的高时间复杂度，并且使用在最后一个词处计算的层来表示文本的RNN是偏向模型（Lai等人，2015）。最近，卷积神经网络（CNN）应用卷积滤波器捕获局部特征，在许多NLP应用中取得了更好的性能，例如句子建模（Blunsom等，2014），关系分类（Zeng et al。，2014） ）和其他传统的NLP任务（Collobert等，2011）。以前的大部分工作都集中在CNN上解决有监督的NLP任务，而在本文中我们的目的是探讨CNN在一个无监督的NLP任务，短文本聚类上的力量。 。<br><img src="/2019/04/03/文献摘要之Short-Text-Clustering-via-Convolutional-Neural-Networks/TIM图片20190403220837.png" alt="figure1"><br>To address the above challenges, we systematically introduce a short text clustering method via convolutional neural networks. An overall architecture of the proposed method is illustrated in Figure 1. Given a short text collection X, the goal of this work is to cluster these texts into clusters C based on the deep feature representation h learned from CNN models. In order to train the CNN models, we,inspired by (Zhang et al., 2010), utilize a self-taught learning framework in our work. In particular, we first embed the original features into compact binary code B with a locality-preserving constraint. Then word vectors S projected from word embeddings are fed into a CNN model to learn the feature representation h and the output units are used to fit the pretrained binary code B. After obtaining the learned features, traditional K-means algorithm is employed to cluster texts into clusters C. The main contributions of this paper are summarized as follows:<br>1). To the best of our knowledge, this is the first attempt to explore the feasibility and effectiveness of combining CNN and traditional semantic constraint, with the help of word embedding to solve one unsupervised learning task, short text clustering.<br>2). We learn deep feature representations with locality-preserving constraint through a self-taught learning framework, and our approach do not use any external tags/labels or complicated NLP preprocessing.<br>3). We conduct experiments on two short text datasets. The experimental results demonstrate that the proposed method achieves excellent performance in terms of both accuracy and normalized mutual information.The remainder of this paper is organized as follows: In Section 2, we first describe the proposed approach STCC and implementation details. Experimental results and analyses are presented in Section 3. In Section 4, we briefly survey several related works. Finally, conclusions are given in the last Section.<br>为了解决上述挑战，我们通过卷积神经网络系统地引入了一种短文本聚类方法。图1中示出了所提出方法的总体结构。给定短文本集X，该工作的目标是基于从CNN模型学习的深度特征表示将这些文本聚类成聚类C.为了训练CNN模型，我们受到（Zhang et al。，2010）的启发，在我们的工作中使用自学的学习框架。特别是，我们首先将原始特征嵌入到具有局部性保留约束的紧凑二进制代码B中。然后将从字嵌入投射的字向量S馈入CNN模型以学习特征表示h，并且使用输出单元来拟合预训练的二进制码B.在获得学习的特征之后，使用传统的K均值算法来对文本进行聚类。本文的主要贡献概括如下：<br>1）。据我们所知，这是首次尝试探索CNN与传统语义约束相结合的可行性和有效性，借助于单词嵌入来解决一个无监督学习任务，即短文本聚类。<br>2）。我们通过自学的学习框架学习具有局部性保留约束的深度特征表示，并且我们的方法不使用任何外部标签/标签或复杂的NLP预处理。<br>3）。我们对两个短文本数据集进行了实验。实验结果表明，该方法在准确性和规范化互信息方面均取得了良好的性能。本文的其余部分安排如下：第2节，我们首先描述了提出的方法STCC和实现细节。实验结果和分析见第3节。在第4节中，我们简要地调查了几个相关的工作。最后，最后一节给出了结论。</p><h3 id="2-Methodology"><a href="#2-Methodology" class="headerlink" title="2 Methodology"></a>2 Methodology</h3><h4 id="2-1-Convolutional-Neural-Networks"><a href="#2-1-Convolutional-Neural-Networks" class="headerlink" title="2.1 Convolutional Neural Networks"></a>2.1 Convolutional Neural Networks</h4><p><img src="/2019/04/03/文献摘要之Short-Text-Clustering-via-Convolutional-Neural-Networks/TIM图片20190403220922.png" alt="figure2"><br>In this section, we will briefly review one popular deep convolutional neural network, Dynamic Convolutional Neural Network (DCNN) (Blunsom et al., 2014), which is the foundation of our proposed method.<br>Taking a neural network with two convolutional layers in Figure 2 as an example, the network transforms raw input text to a powerful representation.Particularly, let $X = \{x_i:x_i \in \mathbb{R}^{d\times1} \}_{i=1,2,\cdots,n}$ denote the set of input $n$ texts, where $d$ is the dimensionality of the original keyword features. Each raw text vector $x_i$ is projected into a matrix representation $S \in \mathbb{R}^{d_w\times s}$ by looking up a word embedding E, where $d_w$ is the dimension of word embedding features and $s$ is the length of one text. We also let $\tilde W = \{W_i\}_{i=1,2}$ and $W_O$ denote the weights of the neural networks. The network defines a transformation $f(\cdot):\mathbb R^{d\times1}\to \mathbb R^{r\times1}(d\gg r)$ which transforms an raw input text $x$ to a r-dimensional deep representation h. There are three basic operations described as follows:<br><strong>– Wide one-dimensional convolution</strong> This operation is applied to an individual row of the sentence matrix $S\in \mathbb R^{d_w\times s}$, and yields a set of sequences $C_i\in \mathbb R^{s+m-1}$where $m$ is the width of convolutional filter.<br><strong>– Folding</strong> In this operation, every two rows in a feature map component-wise are simply summed.For a map of $d_w$ rows, folding returns a map of $d_w/2$ rows, thus halving the size of the representation.<br><strong>– Dymantic k-max pooling</strong> Given a fixed pooling parameter ktop for the topmost convolutional layer, the parameter k of k-max pooling in the l-th convolutional layer can be computed as follows:<script type="math/tex">k_l = max(k_{top},\left \lceil \frac{L-l}{L} \right \rceil)</script><br>where L is the total number of convolutional layers in the network.<br>在本节中，我们将简要回顾一种流行的深度卷积神经网络 - 动态卷积神经网络(DCNN) (Blunsom et al., 2014)，这是我们提出的方法的基础。<br>以图2中带有两个卷积层的神经网络为例，网络将原始输入文本转换为强大的表示。特别是，让$X = \{x_i:x_i \in \mathbb{R}^{d\times1} \}_{i=1,2,\cdots,n}$表示输入$n$ 个文本的集合，其中$d$是原始关键字要素的维度。通过查找嵌入E的单词，将每个原始文本向量$x_i$投影到矩阵表示$S \in \mathbb{R}^{d_w\times s}$中，其中$d_w$是单词嵌入要素的维度$s$是一个文本的长度。我们还让$\tilde W = \{W_i\}_{i=1,2}$和$W_O$表示神经网络的权重。网络定义转换$f(\cdot):\mathbb R^{d\times1}\to \mathbb R^{r\times1}(d\gg r)$将原始输入文本$x$转换为r维深度表示h。有三种基本操作描述如下：</p><ul><li>宽一维卷积 此操作适用于句子矩阵 $S\in \mathbb R^{d_w\times s}$的单个行，并产生一组序列$C_i\in \mathbb R^{s+m-1}$where $m$其中$m$是卷积滤波器的宽度。</li><li>折叠在此操作中，特征映射中的每两行都是简单求和的。对于$d_w$ rows的映射，folding返回$d_w/2$ rows的映射，从而将表示的大小减半。</li><li>Dymantic k-max pooling给定最顶层卷积层的固定池参数ktop，第l个卷积层中k-max池的参数k可以如下计算：<script type="math/tex">k_l = max(k_{top},\left \lceil \frac{L-l}{L} \right \rceil)</script><br>其中L是网络中卷积层的总数。<h4 id="2-2-Locality-preserving-Constraint"><a href="#2-2-Locality-preserving-Constraint" class="headerlink" title="2.2 Locality-preserving Constraint"></a>2.2 Locality-preserving Constraint</h4>Here, we first pre-train binary code B based on the keyword features with a locality-preserving constraint, and choose Laplacian affinity loss, also used in some previous works (Weiss et al., 2009; Zhang et al., 2010). The optimization can be written as:<script type="math/tex">\min_B\sum_{i,j=1}^nS_{ij}\left \| b_i-b_j \right \|_F^2   s.t.B\in\{-1,1\}^{n\times q},B^T1=0,B^TB=I</script><br>where $S_{ij}$ is the pairwise similarity between texts $x_i$ and $x_j$ , and $\left | \cdot \right |_F$ is the Frobenius norm. The problem is relaxed by discarding $B \in \{-1, 1\}^{n\times q}$, and the q-dimensional real-valued vectors $\tilde B$ can be learned from Laplacian Eigenmap. Then, we get the binary code B via the media vector $median(\tilde B)$. In particular, we construct the $n\times n$ local similarity matrix $S$ by using heat kernel as follows:<script type="math/tex">S_{ij}=\left\{\begin{matrix}exp(-\frac{\left \| x_i-x_j \right \|^2}{2\sigma^2}),&if\,x_i\in N_k(x_j)or\,vice\,versa \\0,&otherwise\end{matrix}\right.</script><br>where,$\sigma$is a tuning parameter (default is 1) and $N_k(x)$ represents the set of k-nearest-neighbors of x.The last layer of CNN is an output layer as follows:<script type="math/tex">O = W_Oh, (4)</script>where $h$ is the deep feature representation, $o\in \mathbb R^q$ is the output vector and $W_O\in \mathbb R^{q\times r}$is weight matrix. In order to fit the pre-trained binary code B, we apply q logistic operations to the output vector O as follows:<script type="math/tex">p_i=\frac{exp(O_i)}{1+exp(O_i)}(5)</script><br>在这里，我们首先基于具有局部性保留约束的关键字特征预先训练二进制代码B，并选择拉普拉斯亲和力损失，也用于先前的一些工作中(Weiss et al., 2009; Zhang et al., 2010)。优化可写为：<script type="math/tex">\min_B\sum_{i,j=1}^nS_{ij}\left \| b_i-b_j \right \|_F^2   s.t.B\in\{-1,1\}^{n\times q},B^T1=0,B^TB=I</script><br>其中$S_{ij}$是文本$x_i$和$x_j$之间的成对相似性,$\left | \cdot \right |_F$是Frobenius规范。通过丢弃$B \in \{-1, 1\}^{n\times q}$来放宽问题，并且可以从拉普拉斯算子图中学习q维实值向量$\tilde B$。然后，我们通过媒体向量$median(\tilde B)$得到二进制代码B.特别是，我们使用热内核构造$n\times n$局部相似性矩阵$S$，如下所示：<script type="math/tex">S_{ij}=\left\{\begin{matrix}exp(-\frac{\left \| x_i-x_j \right \|^2}{2\sigma^2}),&if\,x_i\in N_k(x_j)or\,vice\,versa \\0,&otherwise\end{matrix}\right.</script><br>其中，$ \sigma $是一个调整参数（默认为1），$N_k(x)$表示x的k-最近邻居的集合.CNN的最后一层是输出层，如下所示：<script type="math/tex">O =W_Oh(4)</script>其中$ h $是深度特征表示，$o\in \mathbb R^q$是输出向量而$W_O\in \mathbb R^{q\times r}$是权重矩阵。为了拟合预先训练的二进制代码B，我们将q逻辑运算应用于输出向量O，如下所示：<script type="math/tex">p_i=\frac{exp(O_i)}{1+exp(O_i)}(5)</script><h4 id="2-3-Learning"><a href="#2-3-Learning" class="headerlink" title="2.3 Learning"></a>2.3 Learning</h4>All of the parameters to be trained are defined as $\theta$.<script type="math/tex; mode=display">\theta=\{E,\tilde W,W_O\}(6)</script>Given the training text collection X, and the pretrained binary code B, the log likelihood of the parameters can be written down as follows:<script type="math/tex">J(\theta)=\sum_{i=1}^nlogp(b_i|x_i,\theta)(6)</script><br>Following the previous work (Blunsom et al.,2014), we train the network with mini-batches by back-propagation and perform the gradient-based optimization using the Adagrad update rule (Duchi et al., 2011). For regularization, we employ dropout with 50% rate to the penultimate layer (Blunsom et al., 2014; Kim, 2014).<br>所有要训练的参数都定义为$\theta$。<script type="math/tex; mode=display">\theta=\{E,\tilde W,W_O\}(6)</script>给定训练文本集合X和预训练二进制代码B，参数的对数似然可以写成如下：<script type="math/tex">J(\theta)=\sum_{i=1}^nlogp(b_i|x_i,\theta)(6)</script><br>在之前的工作(Blunsom et al.,2014)之后，我们通过反向传播对小批量网络进行训练，并使用Adagrad更新规则执行基于梯度的优化(Duchi et al., 2011)。 对于正则化，我们使用50％率的退出率到倒数第二层(Blunsom et al., 2014; Kim, 2014)<h4 id="2-4-K-means-for-Clustering"><a href="#2-4-K-means-for-Clustering" class="headerlink" title="2.4 K-means for Clustering"></a>2.4 K-means for Clustering</h4>With the given short texts, we first utilize the trained deep neural network to obtain the semantic representations h, and then employ traditional K-means algorithm to perform clustering.<br>利用给定的短文本，我们首先利用训练好的深度神经网络获得语义表示h，然后采用传统的K-means算法进行聚类。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Short Text Clustering via Convolutional Neural Networks&lt;/p&gt;
&lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning</title>
    <link href="http://andeper.cn/2019/04/01/%E6%96%87%E7%8C%AE%E6%91%98%E8%A6%81%E4%B9%8BSemi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/"/>
    <id>http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/</id>
    <published>2019-04-01T07:40:37.000Z</published>
    <updated>2019-04-01T12:19:09.975Z</updated>
    
    <content type="html"><![CDATA[<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong><br>Semi-supervised Clustering for Short Text via Deep Representation Learning<br>基于深度表示学习的短文本半监督聚类</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>In this work, we propose a semi-supervised method for short text clustering, where we represent texts as distributed vectors with neural networks, and use a small amount of labeled data to specify our intention for clustering. We design a novel objective to combine the representation learning process and the kmeans clustering process together, and optimize the objective with both labeled data and unlabeled data iteratively until convergence through three steps: (1) assign each short text to its nearest centroid based on its representation from the current neural networks; (2) reestimate the cluster centroids based on cluster assignments from step (1); (3) update neural networks according to the objective by keeping centroids and cluster assignments fixed.Experimental results on four datasets show that our method works significantly better than several other text clustering methods.</p><p>在这项工作中，我们提出了一种用于短文本聚类的半监督方法，其中我们将文本表示为具有神经网络的分布式向量，并使用少量标记数据来指定我们的聚类意图。 我们设计了一个新的目标，将表示学习过程和kmeans聚类过程结合在一起，迭代地用标记数据和未标记数据优化目标，直到通过三个步骤收敛：（1）基于其分配每个短文本到最近的质心 来自当前神经网络的表示; （2）根据步骤（1）中的聚类分配重新估计聚类质心; （3）通过保持质心和聚类分配固定，根据目标更新神经网络。对四个数据集的实验结果表明，我们的方法比其他几种文本聚类方法效果明显更好。</p><h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h3><p>Text clustering is a fundamental problem in text mining and information retrieval. Its task is to group similar texts together such that texts within a cluster are more similar to texts in other clusters. Usually, a text is represented as a bag-of-words or term frequency-inverse document frequency (TFIDF) vector, and then the k-means algorithm (MacQueen, 1967) is performed to partition a set of texts into homogeneous groups.<br>However, when dealing with short texts, the characteristics of short text and clustering task raise several issues for the conventional unsupervised clustering algorithms. First, the number of uniqe words in each short text is small, as a result, the lexcical sparsity issue usually leads to poor clustering quality (Dhillon and Guan, 2003). Second, for a specific short text clustering task, we have prior knowledge or paticular intenstions before clustering, while fully unsupervised approaches may learn some classes the other way around. Take the sentences in Table 1 for example, those sentences can be clustered into different partitions based on different intentions: apple{a, b, c} and orange {d, e, f} with a fruit type intension, or what-question {a, d}, when-question {b, e},and yes/no-question cluster {c, f} with a question type intension.<br>文本聚类是文本挖掘和信息检索中的基本问题。 其任务是将类似的文本组合在一起，使得群集内的文本更类似于其他群集中的文本。 通常，文本被表示为词袋或术语频率 - 逆文档频率（TFIDF）向量，然后执行k均值算法（MacQueen，1967）以将一组文本划分为同类组。<br>然而，在处理短文本时，短文本和聚类任务的特征为传统的无监督聚类算法提出了若干问题。首先，每个短文本中的单词数量很少，因此，词汇稀疏性问题通常会导致较差的聚类质量（Dhillon和Guan，2003）。其次，对于特定的短文本聚类任务，我们在聚类之前具有先验知识或特定强度，而完全无监督的方法可以反过来学习一些类。 以表1中的句子为例，这些句子可以根据不同的意图聚类到不同的分区：苹果{a，b，c}和橘子{d，e，f}，具有水果类型内涵，或者什么问题{a，d}，什么时候问题{b，e}，是否问题{c，f}，带有问题类型含义。<br><img src="/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/TIM图片20190401153334.png" alt="table1"><br>To address the lexical sparity issue, one direction is to enrich text representations by extracting features and relations from Wikipedia (Banerjee et al.,2007) or an ontology (Fodeh et al., 2011). But this approach requires the annotated knowlege, which is also language dependent. So the other direction, which directly encode texts into distributed vectors with neural networks (Hinton and Salakhutdinov,2006; Xu et al., 2015), becomes more interesing. To tackle the second problem, semi-supervised approaches (e.g. (Bilenko et al., 2004; Davidson and Basu, 2007; Bair, 2013)) have gained significant popularity in the past decades. Our question is can we have a unified model to integrate netural networks into the semi-supervied framework?<br>为了解决词汇间性问题，一个方向是通过从维基百科(Banerjee et al.,2007)或本体论(Fodeh et al., 2011)中提取特征和关系来丰富文本表示。但是这种方法需要带注释的知识，这也是语言依赖的。因此，使用神经网络将文本直接编码为分布式向量的另一个方向（Hinton和Salakhutdinov，2006; Xu等，2015）变得更加有意义。为了解决第二个问题，半监督方法 (e.g. (Bilenko et al., 2004; Davidson and Basu, 2007; Bair, 2013))在过去几十年中获得了极大的欢迎。我们的问题是，我们可以有一个统一的模型将神经网络整合到半监督框架中吗？</p><p>In this paper, we propose a unified framework for the short text clustering task. We employ a deep neural network model to represent short sentences, and integrate it into a semi-supervised algorithm. Concretely, we extend the objective in the classical unsupervised k-means algorithm by adding a penalty term from labeled data. Thus, the new objective covers three key groups of parameters: centroids of clusters, the cluster assignment for each text, and the parameters within deep neural networks. In the training procedure, we start from random initialization of centroids and neural networks, and then optimize the objective iteratively through three steps until converge:<br>(1) assign each short text to its nearest centroid based on its representation from the current neural networks;<br>(2) re-estimate cluster centroids based on cluster assignments from step (1);<br>(3) update neural networks according to the objective by keeping centroids and cluster assignments fixed.<br>Experimental results on four different datasets show that our method achieves significant improvements over several other text clustering methods In following parts, we first describe our neural network models for text representaion (Section 2).Then we introduce our semi-supervised clustering method and the learning algorithm (Section 3). Finally, we evaluate our method on four different datasets (Section 4).<br>在本文中，我们提出了一个用于短文本聚类任务的统一框架。我们采用深度神经网络模型来表示短句，并将其整合到半监督算法中。具体地说，我们通过从标记数据中添加惩罚项来扩展经典无监督k均值算法的目标。因此，新目标涵盖三个关键参数组：聚类的质心，每个文本的聚类分配以及深度神经网络中的参数。在训练过程中，我们从质心和神经网络的随机初始化开始，然后通过三个步骤迭代地优化目标直到收敛：<br>（1）根据当前神经网络的表示，将每个短文本分配到最近的质心;<br>（2）根据步骤（1）中的聚类分配重新估计聚类质心;<br>（3）通过保持质心和簇分配固定，根据目标更新神经网络。<br>四个不同数据集的实验结果表明，我们的方法比其他几种文本聚类方法有了显着的改进。在下面的部分中，我们首先描述了用于文本表示的神经网络模型（第2节）。然后我们介绍了我们的半监督聚类方法和学习。算法（第3节）。最后，我们在四个不同的数据集上评估我们的方法（第4节）。</p><h3 id="2-Representation-Learning-for-Short-Texts"><a href="#2-Representation-Learning-for-Short-Texts" class="headerlink" title="2 Representation Learning for Short Texts"></a>2 Representation Learning for Short Texts</h3><p>We represent each word with a dense vector w, so that a short text s is first represented as a matrix $S = [w_1, …, w_{|s|}]$, which is a concatenation of all vectors of w in s, |s| is the length of s. Then we design two different types of neural networks to ingest the word vector sequence S: the convolutional neural networks (CNN) and the long short-term memory(LSTM). More formally, we define the presentation function as $x = f(s)$, where x is the represent vector of the text s. We test two encoding functions (CNN and LSTM) in our experiments.<br>我们用密集向量w表示每个单词，因此短文本s首先表示为矩阵$S = [w_1, …, w_{|s|}]$，它是s中所有w的向量的连接，|s|是s的长度。然后我们设计了两种不同类型的神经网络来摄取单词矢量序列S：卷积神经网络（CNN）和长短期记忆（LSTM）。更正式地，我们将表示函数定义为$x = f(s)$，其中x是文本s的表示向量。我们在实验中测试了两种编码函数（CNN和LSTM）。<br><img src="/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/]DFHXJ5RX}G[A242WKCXNT6.png" alt="figure1"><br>Inspired from Kim (2014), our CNN model views the sequence of word vectors as a matrix, and applies two sequential operations: convolution and maxpooling. Then, a fully connected layer is employed to convert the final representation vector into a fixed size. Figure 1 gives the diagram of the CNN model. In the convolution operation, we define a list of filters ${w_o}$, where the shape of each filter is d × h, dis the dimension of word vectors and h is the window size. Each filter is applied to a patch (a window size h of vectors) of S, and generates a feature. We apply this filter to all possible patches in S, and produce a series of features. The number of features depends on the shape of the filter wo and the length of the input short text. To deal with variable feature size, we perform a max-pooling operation over all the features to select the maximum value. Therefore, after the two operations, each filter generates only one feature. We define several filters by varying the window size and the initial values. Thus, a vector of features is captured after the max-pooling operation, and the feature dimension is equal to the number of filters.<br>受Kim（2014）的启发，我们的CNN模型将单词向量序列视为矩阵，并应用两个连续运算：卷积和最大化。然后，使用完全连接的层将最终表示矢量转换为固定大小。图1给出了CNN模型的示意图。在卷积运算中，我们定义了一个过滤器列表${w_o}$，其中每个过滤器的形状是d×h，dis是单词向量的维度，h是窗口大小。每个滤波器应用于S的贴片（矢量的窗口大小h），并生成特征。我们将此过滤器应用于S中的所有可能的补丁，并生成一系列功能。特征的数量取决于滤波器的形状和输入短文本的长度。为了处理可变特征尺寸，我们对所有特征执行最大池操作以选择最大值。因此，在两次操作之后，每个过滤器仅生成一个特征。我们通过改变窗口大小和初始值来定义几个过滤器。因此，在最大池操作之后捕获特征向量，并且特征维度等于过滤器的数量。<br><img src="/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/图2.png" alt="tu2"><br>Figure 2 gives the diagram of our LSTM model. We implement the standard LSTM block described in Graves (2012). Each word vector is fed into the LSTM model sequentially, and the mean of the hidden states over the entire sentence is taken as the final representation vector.<br>图2给出了我们的LSTM模型图。我们实现了Graves（2012）中描述的标准LSTM块。每个单词矢量被顺序地馈送到LSTM模型中，并且整个句子上的隐藏状态的平均值被作为最终表示矢量。</p><h3 id="3-Semi-supervised-Clustering-for-Short-Texts"><a href="#3-Semi-supervised-Clustering-for-Short-Texts" class="headerlink" title="3 Semi-supervised Clustering for Short Texts"></a>3 Semi-supervised Clustering for Short Texts</h3><h4 id="3-1-Revisiting-K-means-Clustering"><a href="#3-1-Revisiting-K-means-Clustering" class="headerlink" title="3.1 Revisiting K-means Clustering"></a>3.1 Revisiting K-means Clustering</h4><p>Given a set of texts $\{s_1,s_2,\cdots,s_N\}$, we represent them as a set of data points $\{x_1,x_2,\cdots,x_N\}$, where $x_i$ can be a bag-of-words or TF-IDF vector in traditional approaches, or a dense vector in Section 2.The task of text clustering is to partition the data set into some numberK of clusters, such that the sum of the squared distance of each data point to its closest cluster centroid is minimized. For each data point $x_n$, we define a set of binary variables $r_{nk}\in\{0,1\}$,where $k\in\{1,\cdots,K\}$ describing which of the K clusters $x_n$ is assigned to. So that if $x_n$ is assigned to cluster k, then $r_{nk} = 1$, and $r_{nj} = 0$ for$j\neq k$.Let’s define $u_k$ as the centroid of the k-th cluster.We can then formulate the objective function as<script type="math/tex">J_{unsup} = \sum_{n=1}^N\sum_{k=1}^K r_{nk}\left \|x_n-u_k\right \|^2</script>Our goal is the find the values of $\{r_{nk}\}$ and $\{u_k\}$ so as to minimize Junsup.<br>给出一组文本$\{s_1,s_2,\cdots,s_N\}$，我们将它们表示为一组数据点$\{x_1,x_2,\cdots,x_N\}$，其中$x_i$可以是传统方法中的词袋或TF-IDF向量，或者是第2节中的密集向量。文本聚类的任务是将数据集划分为某些数量的簇，使得总和 每个数据点到其最近的聚类质心的平方距离最小化。 对于每个数据点$x_n$，我们定义一组二进制变量$r_{nk}\in\{0,1\}$，其中$k\in\{1,\cdots,K\}$描述$x_n$分配到哪个群集。 因此，如果将$x_n$分配给簇k，则$r_{nk} = 1$，并且对于$j\neq k,r_{nj} = 0$。让我们将$u_k$定义为第k个簇的质心。然后我们可以将目标函数表示为<script type="math/tex">J_{unsup} = \sum_{n=1}^N\sum_{k=1}^K r_{nk}\left \|x_n-u_k\right \|^2</script>我们的目标是找到$\{r_{nk}\}$和$\{u_k\}$的值，以便最小化$J_{unsup}$。</p><p>The k-means algorithm optimizes $J_{semi}$ through the gradient descent approach, and results in an iterative procedure (Bishop, 2006). Each iteration involves two steps: E-step and M-step. In the Estep, the algorithm minimizes $J_{semi}$ with respect to $\{r_{nk}\}$ by keeping $\{u_k\}$ fixed. $J_{semi}$ is a linear function for $\{r_{nk}\}$, so we can optimize for each data point separately by simply assigning the n-th data point to the closest cluster centroid. In the M-step,the algorithm minimizes $J_{semi}$ with respect to $\{u_k\}$ by keeping $\{r_{nk}\}$ fixed. $J_{semi}$ is a quadratic function of $\{u_k\}$, and it can be minimized by setting its derivative with respect to $\{u_k\}$ to zero.<script type="math/tex">\frac{\partial J_{unsup}}{\partial \mu_k} = 2\sum_{n=1}^Nr_{nk}(x_n-\mu_k)=0(2)</script><br>Then, we can easily solve $\{u_k\}$ as<script type="math/tex">\mu_k=\frac{\sum_{n=1}^Nr_{nk}x_n}{\sum_{n=1}^Nr_{nk}}(3)</script><br>In other words, $\{u_k\}$ is equal to the mean of all the data points assigned to cluster k.<br>k-means算法通过梯度下降方法优化$J_{semi}$，并产生迭代过程（Bishop，2006）。 每次迭代都涉及两个步骤：E步和M步。 在E-step中，算法通过保持$\{u_k\}$固定来最小化$J_{semi}$相对于$\{r_{nk}\}$。 $J_{semi}$是$\{r_{nk}\}$的线性函数，因此我们可以通过简单地将第n个数据点分配给最近的聚类质心来分别优化每个数据点。 在M步骤中，算法通过保持$\{r_{nk}\}$固定来最小化$J_{semi}$相对于$\{u_k\}$。 $J_{semi}$是$\{u_k\}$的二次函数，可以通过将其导数相对于$\{u_k\}$设置为零来最小化。<script type="math/tex">\frac{\partial J_{unsup}}{\partial \mu_k} = 2\sum_{n=1}^Nr_{nk}(x_n-\mu_k)=0(2)</script><br>我们很容易得到<script type="math/tex">\mu_k=\frac{\sum_{n=1}^Nr_{nk}x_n}{\sum_{n=1}^Nr_{nk}}(3)</script><br>换句话说，$\{u_k\}$等于分配给簇k的所有数据点的平均值。</p><h4 id="3-2-Semi-supervised-K-means-with-Neural-Networks"><a href="#3-2-Semi-supervised-K-means-with-Neural-Networks" class="headerlink" title="3.2 Semi-supervised K-means with Neural Networks"></a>3.2 Semi-supervised K-means with Neural Networks</h4><p>The classical k-means algorithm only uses unlabeled data, and solves the clustering problem under the unsupervised learning framework. As already mentioned, the clustering results may not be consistent to our intention. In order to acquire useful clustering results, some supervised information should be introduced into the learning procedure. To this end,we employ a small amount of labeled data to guide the clustering process.<br>Following Section 2, we represent each text s as a dense vector x via neural networks $f(s)$. Instead of training the text representation model separately,we integrate the training process into the k-means algorithm, so that both the labeled data and the unlabeled data can be used for representation learning and text clustering. Let us denote the labeled data set as $\{(s_1,y_1),(s_2,y_2),\cdots,(s_L,y_L)\}$, and the unlabeled data set as $\{s_{L+1},s_{L+2},\cdots,s_N\}$, where $y_i$ is the given label for $s_i$. We then define the objective function as:<script type="math/tex">J_{semi} =\alpha \sum_{n=1}^N\sum_{k=1}^Kr_{nk}\left \|  f(s_n)-\mu_k\right \|^2+(1-\alpha)\sum_{n=1}^L\{\left \|  f(s_n)-\mu_{g_n}\right \|^2+\sum_{j\neq g_n}[l+\sum_{n=1}^L\left \|  f(s_n)-\mu_{g_n}\right \|^2-\sum_{n=1}^L\left \|  f(s_n)-\mu_{j}\right \|^2]_+\}</script><br>经典的k-means算法仅使用未标记的数据，并解决了无监督学习框架下的聚类问题。 如前所述，聚类结果可能与我们的意图不一致。为了获得有用的聚类结果，应该在学习过程中引入一些监督信息。为此，我们使用少量标记数据来指导群集过程。<br>在第2节之后，我们通过神经网络$f(s)$将每个文本s表示为密集向量x。我们不是单独训练文本表示模型，而是将训练过程集成到k-means算法中，以便标记数据和未标记数据都可以用于表示学习和文本聚类。 让我们将标记数据集表示为$\{(s_1,y_1),(s_2,y_2),\cdots,(s_L,y_L)\}$，并将未标记数据集表示为$\{s_{L+1},s_{L+2},\cdots,s_N\}$，其中$y_i$是$s_i$的给定标签。 然后我们将目标函数定义为：</p><script type="math/tex; mode=display">J_{semi} =\alpha \sum_{n=1}^N\sum_{k=1}^Kr_{nk}\left \|  f(s_n)-\mu_k\right \|^2+(1-\alpha)\sum_{n=1}^L\{\left \|  f(s_n)-\mu_{g_n}\right \|^2+\sum_{j\neq g_n}[l+\sum_{n=1}^L\left \|  f(s_n)-\mu_{g_n}\right \|^2-\sum_{n=1}^L\left \|  f(s_n)-\mu_{j}\right \|^2]_+\}</script><p>The objective function contains two terms. The first term is adapted from the unsupervised k-means algorithm in Eq. (1), and the second term is defined to encourage labeled data being clustered in correlation with the given labels. $\alpha\in[0,1]$ is used to tune the importance of unlabeled data. The second term contains two parts. The first part penalizes large distance between each labeled instance and its correct cluster centroid, where $g_n=G(y_n)$ is the cluster ID mapped from the given label yn, and the mapping function $G(\cdot)$ is implemented with the Hungarian algorithm (Munkres, 1957). The second part is denoted as a hinge loss with a margin l, where $[x]_+ = max(x,0)$. This part incurs some loss if the distance to the correct centroid is not shorter (by the margin l) than distances to any of incorrect cluster centroids.<br>目标函数包含两个部分。第一项改编自方程式中的无监督k均值算法。(1)，并且第二项被定义为鼓励标记数据与给定标签相关联地聚类。$\alpha\in[0,1]$用于调整未标记数据的重要性。第二项包含两部分。第一部分惩罚每个标记实例与其正确的聚类质心之间的大距离，其中$g_n=G(y_n)$是从给定标签$y_n$映射的聚类ID，映射函数$G(\cdot)$用匈牙利算法(Munkres, 1957)实现。第二部分表示为具有边界l的铰链损耗，其中$[x]_+ = max(x,0)$。如果到正确质心的距离不短（通过边缘l）而不是到任何不正确的聚类质心的距离，则该部分会引起一些损失。</p><p>There are three groups of parameters in $J_{semi}$: the cluster assignment of each text $\{r_{nk}\}$, the cluster centroids $\{\mu_k\}$, and the parameters within the neural network model $f(\cdot)$. Our goal is the find the values of $\{r_{nk}\}$, $\{\mu_k\}$ and parameters in $f(\cdot)$, so as to minimize $J_{semi}$. Inspired from the k-means algorithm,we design an algorithm to successively minimize $J_{semi}$ with respect to $\{r_{nk}\}$, $\{\mu_k\}$, and parameters in $f(\cdot)$. Table 2 gives the corresponding pseudocode.First, we initialize the cluster centroids $\{\mu_k\}$ with the k-means++ strategy (Arthur and Vassilvitskii,2007), and randomly initialize all the parameters in the neural network model. Then, the algorithm iteratively goes through three steps (assign cluster, estimate centroid, and update parameter) until $J_{semi}$ converges.<br>$J_{semi}$中有三组参数：每个文本的集群分配$\{r_{nk}\}$，集群质心$\{\mu_k\}$，以及神经网络模型$f(\cdot)$中的参数。我们的目标是在$f(\cdot)$中找到$\{r_{nk}\}$，$\{\mu_k\}$和参数的值，以便最小化$J_{semi}$。受k-means算法的启发，我们设计了一种算法，相对于$\{r_{nk}\}$，$\{\mu_k\}$和$f(\cdot)$中的参数，连续地最小化$J_{semi}$。表2给出了相应的伪代码。首先，我们用k-means ++策略初始化聚类中心$\{\mu_k\}$（Arthur和Vassilvitskii，2007），并随机初始化神经网络模型中的所有参数。然后，算法迭代地经历三个步骤（分配簇，估计质心和更新参数）直到$J_{semi}$收敛。</p><p>The assign cluster step minimizes $J_{semi}$ with respect to $\{r_{nk}\}$ by keeping $f(\cdot)$ and $\{\mu_k\}$ fixed. Its goal is to assign a cluster ID for each data point. We can see that the second term in Eq. (4) has no relation with $\{r_{nk}\}$. Thus, we only need to minimize the first term by assigning each text to its nearest cluster centroid, which is identical to the E-step in the k-means algorithm. In this step, we also calculate the mappings between the given labels $\{y_i\}$ and the cluster IDs (with the Hungarian algorithm) based on cluster assignments of all labeled data.<br>分配簇步骤通过保持$f(\cdot)$和$\{\mu_k\}$固定来最小化$J_{semi}$相对于$\{r_{nk}\}$。其目标是为每个数据点分配一个集群ID。我们可以看到方程式(4)中的第二项与$\{r_{nk}\}$无关。因此，我们只需要通过将每个文本分配到其最近的聚类质心来最小化第一项，这与k均值算法中的E步骤相同。在此步骤中，我们还基于所有标记数据的集群分配来计算给定标签$\{y_i\}$与集群ID（使用匈牙利算法）之间的映射。</p><p>The estimate centroid step minimizes $J_{semi}$ with respect to $\{\mu_k\}$ by keeping $\{r_{nk}\}$ and $f(\cdot)$ fixed,which corresponds to the M-step in the k-means algorithm. It aims to estimate the cluster centroids$\{\mu_k\}$ based on the cluster assignments $\{r_{nk}\}$ from the assign cluster step. The second term in Eq.(4) makes each labeled instance involved in the estimating process of cluster centroids. By solving $\partial J_{semi}/\partial \mu_k = 0$, we get<script type="math/tex">u_k =\frac{\sum_{n=1}^N\alpha r_{nk}f(s_n)+\sum_{n=1}^L w_{nk}f(s_n)}{\sum_{n=1}^N\alpha r_{nk}+\sum_{n=1}^L w_{nk}} (5)</script><br>${I}’_{nk} = \delta(k,g_n)$<br>${I}’’_{nkj} = \delta(k,j)\cdot \delta’_{nj}$<br>${I}’’’_{nkj} = (1-\delta(k,j))\cdot \delta’_{nj}$<br>$\delta’_{nj} = \delta(l+\left|f(s_n)-\mu_{g_n}\right|^2-\left|f(s_n)-\mu_{j}\right|^2&gt;0)(6)$<br>where $\delta(x_1, x_2)=1$ if $x_1$ is equal to $x_2$, otherwise $\delta(x_1, x_2)=0$, and $\delta(x)=1$ if x is true, otherwise $\delta(x)=0$. The first term in the numerator of Eq. (5) is the contributions from all data points, and $\alpha r_{nk}$ is the weight of $s_n$ for $\mu_k$. The second term is acquired from labeled data, and $w_{nk}$ is the weight of a labeled instance $s_n$ for $\mu_k$.<br>通过保持$\{r_{nk}\}$和$f(\cdot)$固定，估计质心步骤使$J_{semi}$相对于$\{\mu_k\}$最小化，这对应于k均值算法中的M步。它旨在根据分配集群步骤中的集群分配$\{r_{nk}\}$估计集群质心$\{\mu_k\}$。方程(4)中的第二项使得每个标记实例参与聚类质心的估计过程。通过求解$\partial J_{semi}/\partial \mu_k = 0$，我们得到了<script type="math/tex">u_k =\frac{\sum_{n=1}^N\alpha r_{nk}f(s_n)+\sum_{n=1}^L w_{nk}f(s_n)}{\sum_{n=1}^N\alpha r_{nk}+\sum_{n=1}^L w_{nk}}</script><br>${I}’_{nk} = \delta(k,g_n)$<br>${I}’’_{nkj} = \delta(k,j)\cdot \delta’_{nj}$<br>${I}’’’_{nkj} = (1-\delta(k,j))\cdot \delta’_{nj}$<br>$\delta’_{nj} = \delta(l+\left|f(s_n)-\mu_{g_n}\right|^2-\left|f(s_n)-\mu_{j}\right|^2&gt;0)$<br>其中$ \delta(x_1,x_2)= 1 $如果$ x_1 $等于$ x_2 $，否则$ \delta(x_1,x_2)= 0 $，如果x为真，$ \delta(x)= 1 $ ，否则$ \delta(x)= 0 $。 方程式(5)分子中的第一项是所有数据点的贡献，$ \alpha r_{nk} $是$ \mu_k $的$ s_n $的权重。 第二个术语是从标记数据中获取的，$ w_{nk} $是$ \mu_k $的标记实例$s_n$的权重。</p><p>The update parameter step minimizes $J_{semi}$ with respect to $f(\cdot)$ by keeping $\{r_{nk}\}$ and $\{\mu_k\}$ fixed,which has no counterpart in the k-means algorithm.The main goal is to update parameters for the text representation model. We take $J_{semi}$ as the loss function, and train neural networks with the Adam algorithm (Kingma and Ba, 2014).<br>更新参数步骤通过保持$\{r_{nk}\}$和$\{\mu_k\}$固定来最小化$J_{semi}$相对于$f(\cdot)$，其在k均值算法中没有对应物。主要目标是更新文本表示模型的参数。 我们将$J_{semi}$作为损失函数，并使用Adam算法训练神经网络（Kingma和Ba，2014）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！&lt;/strong&gt;&lt;br&gt;Semi-supervised Clustering for Short Text via Deep Representation Learning&lt;br&gt;基于深度表示学
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>文献摘要之Convolutional Neural Networks for Sentence Classification</title>
    <link href="http://andeper.cn/2019/03/20/%E6%96%87%E7%8C%AE%E6%91%98%E8%A6%81%E4%B9%8BConvolutional-Neural-Networks-for-Sentence-Classification/"/>
    <id>http://andeper.cn/2019/03/20/文献摘要之Convolutional-Neural-Networks-for-Sentence-Classification/</id>
    <published>2019-03-20T06:51:10.000Z</published>
    <updated>2019-03-20T12:48:57.603Z</updated>
    
    <content type="html"><![CDATA[<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong><br>Convolutional Neural Networks for Sentence Classification<br>用于句子分类的卷积神经网络</p><h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h3><p>We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.<br>我们报告了一系列卷积神经网络（CNN）的实验，这些实验是在预训练的单词向量之上训练的，用于句子级分类任务。 我们展示了一个简单的CNN，它具有很少的超参数调整和静态向量，可以在多个基准测试中获得出色的结果。 通过微调学习任务特定的向量可以进一步提高性能。 我们还建议对架构进行简单修改，以允许使用任务特定和静态向量。 这里讨论的CNN模型在7个任务中的4个中改进了现有技术，包括情感分析和问题分类。</p><p>Deep learning models have achieved remarkable results in computer vision (Krizhevsky et al.,2012) and speech recognition (Graves et al., 2013) in recent years. Within natural language processing, much of the work with deep learning methods has involved learning word vector representations through neural language models (Bengio et al., 2003; Yih et al., 2011; Mikolov et al., 2013) and performing composition over the learned word vectors for classification (Collobert et al., 2011). Word vectors, wherein words are projected from a sparse, 1-of-V encoding (here V is the vocabulary size) onto a lower dimensional vector space via a hidden layer, are essentially feature extractors that encode semantic features of words in their dimensions. In such dense representations, semantically close words are likewise close—in euclidean or cosine distance—in the lower dimensional vector space.<br>深度学习模型近年来在计算机视觉（Krizhevsky等，2012）和语音识别（Graves等，2013）中取得了显着成果。在自然语言处理中，深度学习方法的大部分工作都涉及通过神经语言模型学习单词矢量表示（Bengio et al。，2003; Yih et al。，2011; Mikolov et al。，2013），并在学习了用于分类的单词向量（Collobert et al。，2011）。单词向量，其中单词通过隐藏层从稀疏的1-V编码（这里V是词汇量大小）投影到较低维度的向量空间上，本质上是特征提取器，其在其维度中编码单词的语义特征。在这种密集表示中，语义上接近的单词同样在较低维向量空间中接近欧几里德或余弦距离。</p><p>Convolutional neural networks (CNN) utilize layers with convolving filters that are applied to  local features (LeCun et al., 1998). Originally invented for computer vision, CNN models have subsequently been shown to be effective for NLP and have achieved excellent results in semantic parsing (Yih et al., 2014), search query retrieval (Shen et al., 2014), sentence modeling (Kalchbrenner et al., 2014), and other traditional NLP tasks (Collobert et al., 2011).<br>卷积神经网络（CNN）利用具有应用于局部特征的卷积滤波器的层（LeCun等，1998）。最初发明用于计算机视觉的CNN模型随后被证明对NLP有效，并且在语义分析（Yih等人，2014），搜索查询检索（Shen et al。，2014），句子建模（Kalchbrenner）方面取得了优异的成果。 et al。，2014）和其他传统的NLP任务（Collobert等，2011）。</p><p>In the present work, we train a simple CNN with one layer of convolution on top of word vectors obtained from an unsupervised neural language model. These vectors were trained by Mikolov et al. (2013) on 100 billion words of Google News, and are publicly available.1 We initially keep the word vectors static and learn only the other parameters of the model. Despite little tuning of hyperparameters, this simple model achieves excellent results on multiple benchmarks, suggesting that the pre-trained vectors are ‘universal’ feature extractors that can be utilized for various classification tasks. Learning task-specific vectors through fine-tuning results in further improvements. We finally describe a simple modification to the architecture to allow for the use of both pre-trained and task-specific vectors by having multiple channels.<br>Our work is philosophically similar to Razavian et al. (2014) which showed that for image classification, feature extractors obtained from a pretrained deep learning model perform well on a variety of tasks—including tasks that are very different from the original task for which the feature extractors were trained.<br>在目前的工作中，我们训练一个简单的CNN，在无人监督的神经语言模型中获得的单词向量之上有一层卷积。这些载体由Mikolov等人训练。 （2013）关于1000亿字的谷歌新闻，并且是公开可用的.1我们最初保持单词向量静态并仅学习模型的其他参数。尽管对超参数进行了很少的调整，但这个简单的模型在多个基准测试中取得了优异的成果，这表明预训练的矢量是可以用于各种分类任务的“通用”特征提取器。通过微调学习任务特定的向量可以进一步改进。我们最终描述了对体系结构的简单修改，以允许通过具有多个通道来使用预先训练的和任务特定的向量。<br>我们的工作在哲学上类似于Razavian等人。 （2014）表明，对于图像分类，从预训练深度学习模型获得的特征提取器在各种任务上表现良好 - 包括与训练特征提取器的原始任务非常不同的任务。</p><h3 id="2-Model"><a href="#2-Model" class="headerlink" title="2 Model"></a>2 Model</h3><p><img src="/2019/03/20/文献摘要之Convolutional-Neural-Networks-for-Sentence-Classification/TIM图片20190320203134.png" alt="figure1"></p><p>The model architecture, shown in figure 1, is a slight variant of the CNN architecture of Collobert et al. (2011). Let $x_i \in \mathbb{R}^k$ be the k-dimensional word vector corresponding to the i-th word in the sentence. A sentence of length n (padded where necessary) is represented as <script type="math/tex">x_{1:n} = x_1 \oplus x_2 \oplus \dots \oplus x_n, (1)</script>where $\oplus$ is the concatenation operator. In general, let $x_{i:i+j}$ refer to the concatenation of words $x_i, x_{i+1}, \dots , x_{i+j}$ . A convolution operation involves a filter $w \in \mathbb{R}^{hk}$, which is applied to a window of h words to produce a new feature. For example, a feature $c_i$ is generated from a window of words $x_{i:i+h-1}$ by <script type="math/tex">c_i = f(w · x_{i:i+h-1} + b). (2)</script></p><p>Here $b \in \mathbb{R}$ is a bias term and f is a non-linear function such as the hyperbolic tangent. This filter is applied to each possible window of words in the sentence $\{x_{1:h}, x_{2:h+1}, \dots , x_{n-h+1:n}\}$ to produce a feature map <script type="math/tex">c = [c_1, c_2, \dots , c_{n-h+1}], (3)</script> with $c \in R_{n-h+1}$. We then apply a max-overtime pooling operation (Collobert et al., 2011)over the feature map and take the maximum value $\hat{c} = max\{c\}$ as the feature corresponding to this particular filter. The idea is to capture the most important feature—one with the highest value—for each feature map. This pooling scheme naturally deals with variable sentence lengths.</p><p>We have described the process by which one feature is extracted from one filter. The model uses multiple filters (with varying window sizes) to obtain multiple features. These features form the penultimate layer and are passed to a fully connected softmax layer whose output is the probability distribution over labels.</p><p>In one of the model variants, we experiment with having two ‘channels’ of word vectors—one that is kept static throughout training and one that is fine-tuned via backpropagation (section 3.2).2 In the multichannel architecture, illustrated in figure 1, each filter is applied to both channels and the results are added to calculate $c_i$ in equation(2). The model is otherwise equivalent to the single channel architecture.</p><p>如图1所示，模型架构是Collobert等人的CNN架构的略微变体。（2011年）。 令$x_i \in \mathbb{R}^k$是对应于句子中的第i个单词的k维单词向量。长度为n的句子（必要时填充）表示为<script type="math/tex">x_{1:n} = x_1 \oplus x_2 \oplus \dots \oplus x_n, (1)</script>其中⊕是连接运算符。 通常，让 $x_{i:i+j}$指的是单词$x_i, x_{i+1}, \dots , x_{i+j}$的串联。卷积运算涉及滤波器$w \in \mathbb{R}^{hk}$，其应用于h字的窗口以产生新特征。 例如，通过<script type="math/tex">c_i = f(w · x_{i:i+h-1} + b). (2)</script>从单词$x_{i:i+h-1}$的窗口生成特征$c_i$。<br>这里$b \in \mathbb{R}$是偏置项，f是非线性函数，例如双曲正切。 此过滤器应用于句子$\{x_{1:h}, x_{2:h+1}, \dots , x_{n-h+1:n}\}$中的每个可能的单词窗口。生成特征映射<script type="math/tex">c = [c_1, c_2, \dots , c_{n-h+1}], (3)</script> $c \in R_{n-h+1}$。 然后，我们在特征图上应用最大超时池化操作（Collobert等，2011），并将最大值$\hat{c} = max\{c\}$作为与该特定过滤器对应的特征。 我们的想法是为每个要素图捕获最重要的特征——一个具有最高值的特征。 这种汇集方案自然地处理可变句子长度。<br>我们已经描述了从一个过滤器中提取一个特征的过程。 该模型使用多个过滤器（具有不同的窗口大小）来获得多个特征。 这些特征形成倒数第二层，并传递给完全连接的softmax层，其输出是标签上的概率分布。<br>在其中一个模型变体中，我们尝试使用两个“通道”的单词向量 - 一个在整个训练过程中保持静态，一个通过反向传播进行微调（第3.2节）.2在多通道架构中，如图1所示 ，将每个滤波器应用于两个通道，并将结果相加以计算等式（2）中的$c_i$。 该模型在其他方面等同于单通道架构。</p><h3 id="2-1-Regularization"><a href="#2-1-Regularization" class="headerlink" title="2.1 Regularization"></a>2.1 Regularization</h3><p>For regularization we employ dropout on the penultimate layer with a constraint on $l_2-$norms of the weight vectors (Hinton et al., 2012). Dropout prevents co-adaptation of hidden units by randomly dropping out—i.e., setting to zero—a proportion p of the hidden units during fowardbackpropagation. That is, given the penultimate layer $z = [\hat{c}_1, \dots , \hat{c}_m]$ (note that here we have m filters), instead of using <script type="math/tex">y = w · z + b (4)</script> for output unit y in forward propagation, dropout uses <script type="math/tex">y = w \cdot(z \circ r) + b, (5)</script> where $\circ$ is the element-wise multiplication operator and $r \in \mathbb{R}^m$ is a ‘masking’ vector of Bernoulli random variables with probability p of being 1. Gradients are backpropagated only through the unmasked units. At test time, the learned weight vectors are scaled by p such that $\hat{w} = pw$, and $\hat w$ is used (without dropout) to score unseen sentences. We additionally constrain $l_2$-norms of the weight vectors by rescaling w to have ${\lVert w \rVert}_2 = s$ whenever ${\lVert w \rVert}_2 &gt; s$ after a gradient descent step.</p><h3 id="2-1正规化"><a href="#2-1正规化" class="headerlink" title="2.1正规化"></a>2.1正规化</h3><p>对于正则化，我们在倒数第二层上使用丢失，并对权重向量的$l_2-$范数进行约束（Hinton等，2012）。Dropout通过随机丢弃来防止隐藏单元的共同适应，即在前向反向传播期间设置为零 - 隐藏单元的比例p。也就是说，给定倒数第二层$z = [\hat{c}_1, \dots , \hat{c}_m]$（请注意，这里我们有m个滤波器），而不是在前向传播中使用<script type="math/tex">y = w · z + b (4)</script>作为输出单位y，使用dropout <script type="math/tex">y = w \cdot(z \circ r) + b, (5)</script><br>其中$\circ$是逐元素乘法运算符，$r \in \mathbb{R}^m$是伯努利随机变量的’掩蔽’向量，概率p为1.梯度仅通过未掩蔽单元反向传播。在测试时，学习的权重向量通过p缩放，使得$\hat{w} = pw$，并且使用$\hat w$（没有丢失）来对看不见的句子进行评分。我们还通过在梯度下降步骤之后每当${\lVert w \rVert}_2 &gt; s$重新调整w以具有${\lVert w \rVert}_2 = s$来约束权重向量的$l_2$范数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！&lt;/strong&gt;&lt;br&gt;Convolutional Neural Networks for Sentence Classification&lt;br&gt;用于句子分类的卷积神经网络&lt;/p&gt;
&lt;h3 id
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>自然语言处理之词向量</title>
    <link href="http://andeper.cn/2019/02/26/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B9%8B%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    <id>http://andeper.cn/2019/02/26/自然语言处理之词向量/</id>
    <published>2019-02-26T13:53:18.000Z</published>
    <updated>2019-03-15T02:10:03.650Z</updated>
    
    <content type="html"><![CDATA[<h2 id="NLP-词向量"><a href="#NLP-词向量" class="headerlink" title="NLP-词向量"></a>NLP-词向量</h2><h3 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h3><h4 id="什么是词向量-词嵌入"><a href="#什么是词向量-词嵌入" class="headerlink" title="什么是词向量/词嵌入"></a>什么是词向量/词嵌入</h4><p>词向量(word embedding)是一个固定长度的实值向量<br>词向量是神经语言模型的副产品<br>词向量是针对词提出的，事实上，也可以针对更细或更粗的粒度进行推广——比如字向量、句向量、文档向量等</p><h4 id="词向量的理解"><a href="#词向量的理解" class="headerlink" title="词向量的理解"></a>词向量的理解</h4><p>在NLP任务中，我们将自然语言交给机器学习算法来处理，但机器无法直接理解人类的语言，因此首先要做的事情就是将语言数字化,如何将自然语言进行数字化呢？词向量提供了一种很好的方式。</p><p>一种最简单的词向量是one-hot representation,就是用一个很长的向量来表示一个词，向量的长度为词典D的大小N，向量的分量只有一个1，其他全为0,1的位置对应该词在词典中的索引。但这种词向量表示有一个缺点，就是受维度灾难的困扰，尤其是将其用于Deep Learning场景时，又如，它不能很好刻画词与词之间的相似性；</p><p>另一种词向量是Distributed Representation，它最早是Hinton在1986年提出的，可以克服one-hot representation 的上述缺点，其基本想法是：通过训练将某种语言中的每一个词映射成一个固定长度的短向量，所有这些向量构成一个词向量空间，而每一向量可视为该空间中的一个点，在这个空间上引入“距离”，就可以根据词之间的距离来判断它们之间的相似性了，word2vec中采用的就是这种Distributed Representation的词向量</p><p>为了更好理解上述思想，我们来举一个通俗的例子：<br>假设在二维平面上有a个不同的点，给定其中的某个点，想在想在平面上找到与这个点最近的一个点。<br>我们是怎么做的呢？首先建立一个直角坐标系，基于该坐标系，其上每一个点就唯一的对应一个坐标(x,y);接着引入欧氏距离；最后分别计算这个点与其他a-1个点的距离，对应最小距离值得那个点便是我们要找的点了。</p><p>上面的例子中，坐标(X，y)的地位就相当于词向量，它来将平面上的一个点的位置在数学上作量化。坐标系建立好之后，要得到某个点的坐标是很容易的。然而，在NLP任务中,要得到词向量就复杂多了，而且词向量不唯一，其质量依赖于训练语料，训练算法等因素。</p><p>如何得到词向量呢？有很多不同的模型来估计词向量，包括右面的LSA和LDA，神经网络算法</p><h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><p>在word2vec出现之前，已经有用神经网络DNN来用训练词向量进而处理词与词之间的关系了。采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。</p><p>这个模型是如何定义数据的输入和输出呢？一般分为CBOW(Continuous Bag-of-Words)与Skip-Gram两种模型。</p><p>CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。比如下面这段话，我们的上下文大小取值为4，特定的这个词是”Learning”，也就是我们需要的输出词向量,上下文对应的词有8个，前后各4个，这8个词是我们模型的输入。由于CBOW使用的是词袋模型，因此这8个词都是平等的，也就是不考虑他们和我们关注的词之间的距离大小，只要在我们上下文之内即可。<br><img src="/2019/02/26/自然语言处理之词向量/word_example.png" alt="word"><br>这样在我们例子中，我们的输入是8个词向量，输出是所有词的softmax概率(训练的目标是期望训练样本特定词的softmax概率最大)，对应的CBOW神经网络模型输入层有8个神经元，输出层有词汇表大小个神经元。隐藏层的神经元个数我们可以自己指定。通过DNN的反向传播算法，我们可以求出DNN模型的参数，同时得到所有的词对应的词向量。这样当我们有新的需求，要求出某8个词对应的最可能的输出中心词时，我们可以通过一个DNN前向传播算法并通过softmax函数找到概率最大的词对应的神经元即可。<br>Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量，我们上下文大小取值4，特定的这个词“learning”就是我们的输入，而这8个上下文词是我们的输出。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;NLP-词向量&quot;&gt;&lt;a href=&quot;#NLP-词向量&quot; class=&quot;headerlink&quot; title=&quot;NLP-词向量&quot;&gt;&lt;/a&gt;NLP-词向量&lt;/h2&gt;&lt;h3 id=&quot;背景知识&quot;&gt;&lt;a href=&quot;#背景知识&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>使用Python操作Mangodb</title>
    <link href="http://andeper.cn/2019/02/16/%E4%BD%BF%E7%94%A8Python%E6%93%8D%E4%BD%9CMangodb/"/>
    <id>http://andeper.cn/2019/02/16/使用Python操作Mangodb/</id>
    <published>2019-02-16T12:55:27.000Z</published>
    <updated>2019-02-17T12:42:32.645Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Python-MongoDB"><a href="#Python-MongoDB" class="headerlink" title="Python MongoDB"></a>Python MongoDB</h2><p>MongoDB是目前最流行的NoSQL数据库之一，使用的数据类型是BSON</p><h3 id="PyMongoDB"><a href="#PyMongoDB" class="headerlink" title="PyMongoDB"></a>PyMongoDB</h3><p>Python要链接MongoDB需要MongoDB驱动，这里我们使用PyMongo驱动来连接</p><h4 id="pip安装"><a href="#pip安装" class="headerlink" title="pip安装"></a>pip安装</h4><p>pip是一个通用的Python包管理工具，提供了对Python包的查找、下载、安装、卸载的功能。<br>安装pymongo<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python3 -m pip3 install pymongo</span><br></pre></td></tr></table></figure></p><p>也可以指定安装的版本<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python3 -m pip3 install pymongo==3.5.1</span><br></pre></td></tr></table></figure></p><p>更新pymongo命令：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python3 -m pip3 install --upgrade pymongo</span><br></pre></td></tr></table></figure></p><h4 id="测试pymongo"><a href="#测试pymongo" class="headerlink" title="测试pymongo"></a>测试pymongo</h4><p>在python控制台输入以下代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br></pre></td></tr></table></figure></p><p>如果没有出现错误，表示安装成功</p><h3 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h3><h4 id="创建一个数据库"><a href="#创建一个数据库" class="headerlink" title="创建一个数据库"></a>创建一个数据库</h4><p>创建数据库需要使用MongoClient对象，并且指定连接的URL地址和要创建的数据库名。<br>如下示例，我们需要创建数据库twittertopic<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line">myclient = pymongo.MongoClient(<span class="string">"mongodb://10.245.142.249:27017/"</span>)</span><br><span class="line">mydb = myclient[<span class="string">"twittertopic"</span>]</span><br></pre></td></tr></table></figure></p><p><strong>注意</strong>：在MongoDB中，数据库只有在内容插入后才会创建！就是说，数据库创建要创建集合（数据表）并插入一条数据，数据库才会真正创建。</p><h4 id="判断数据库是否已存在"><a href="#判断数据库是否已存在" class="headerlink" title="判断数据库是否已存在"></a>判断数据库是否已存在</h4><p>我们可以读取MongoDB中所有的数据库，并判断制定的数据库是否存在：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line">myclient = pymongo.MongoClient(<span class="string">"mongodb://10.245.142.249:27017/"</span>)</span><br><span class="line">dblist = myclient.list_database_names()</span><br><span class="line"><span class="comment"># dblist = myclient.database_names()</span></span><br><span class="line"><span class="keyword">if</span> <span class="string">"twittertopic"</span> <span class="keyword">in</span> dblist:</span><br><span class="line">    print(<span class="string">"数据库已存在！"</span>)</span><br></pre></td></tr></table></figure></p><p><strong>注意</strong>：database_names 在最新版本的 Python 中已废弃，Python3.7+ 之后的版本改为了 list_database_names()。</p><h4 id="创建集合"><a href="#创建集合" class="headerlink" title="创建集合"></a>创建集合</h4><p>MongoDB中的集合类似SQL的表。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line">myclient = pymongo.MongoClient(<span class="string">"mongodb://10.245.142.249:27017/"</span>)</span><br><span class="line">mydb = myclient[<span class="string">"twittertopic"</span>]</span><br><span class="line">mycol = mydb[<span class="string">"sites"</span>]</span><br></pre></td></tr></table></figure></p><h3 id="添加数据"><a href="#添加数据" class="headerlink" title="添加数据"></a>添加数据</h3><h4 id="插入集合"><a href="#插入集合" class="headerlink" title="插入集合"></a>插入集合</h4><p>集合中插入文档使用insertone()方法，该方法的第一参数是字典name-&gt;value键值对。如下示例<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line">myclient = pymongo.MongoClient(<span class="string">"mongodb://10.245.142.249:27017/"</span>)</span><br><span class="line">mydb = myclient[<span class="string">"twittertopic"</span>]</span><br><span class="line">mycol = mydb[<span class="string">"sites"</span>]</span><br><span class="line">mydict = &#123; <span class="string">"name"</span>: <span class="string">"RUNOOB"</span>, <span class="string">"alexa"</span>: <span class="string">"10000"</span>, <span class="string">"url"</span>: <span class="string">"https://www.runoob.com"</span> &#125;</span><br><span class="line">x = mycol.insert_one(mydict)</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure></p><p>执行输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;pymongo.results.InsertOneResult object at <span class="number">0x10a34b288</span>&gt;</span><br></pre></td></tr></table></figure></p><h4 id="返回-id字段"><a href="#返回-id字段" class="headerlink" title="返回_id字段"></a>返回_id字段</h4><p>insert_one()方法返回InsertOneResult对象，该对象包含inserted_id属性，它是插入文档的id值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line">myclient = pymongo.MongoClient(<span class="string">"mongodb://10.245.142.249:27017/"</span>)</span><br><span class="line">mydb = myclient[<span class="string">"twittertopic"</span>]</span><br><span class="line">mycol = mydb[<span class="string">"Apex"</span>]</span><br><span class="line">mydict = &#123; <span class="string">"name"</span>: <span class="string">"RUNOOB"</span>, <span class="string">"alexa"</span>: <span class="string">"10000"</span>, <span class="string">"url"</span>: <span class="string">"https://www.runoob.com"</span> &#125;</span><br><span class="line">x = mycol.insert_one(mydict)</span><br><span class="line">print(x.inserted_id)</span><br></pre></td></tr></table></figure></p><p>执行输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">5</span>b2369cac315325f3698a1cf</span><br></pre></td></tr></table></figure></p><p>如果我们在插入文档时候没有指定_id，MongoDB会为每条数据添加一个唯一的id</p><h4 id="插入多个文档"><a href="#插入多个文档" class="headerlink" title="插入多个文档"></a>插入多个文档</h4><p>集合中插入多个文档使用 insert_many() 方法，该方法的第一参数是字典列表。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line">myclient = pymongo.MongoClient(<span class="string">"mongodb://10.245.142.249:27017/"</span>)</span><br><span class="line">mydb = myclient[<span class="string">"twittertopic"</span>]</span><br><span class="line">mycol = mydb[<span class="string">"sites"</span>]</span><br><span class="line">mylist = [</span><br><span class="line">  &#123; <span class="string">"name"</span>: <span class="string">"Taobao"</span>, <span class="string">"alexa"</span>: <span class="string">"100"</span>, <span class="string">"url"</span>: <span class="string">"https://www.taobao.com"</span> &#125;,</span><br><span class="line">  &#123; <span class="string">"name"</span>: <span class="string">"QQ"</span>, <span class="string">"alexa"</span>: <span class="string">"101"</span>, <span class="string">"url"</span>: <span class="string">"https://www.qq.com"</span> &#125;,</span><br><span class="line">  &#123; <span class="string">"name"</span>: <span class="string">"Facebook"</span>, <span class="string">"alexa"</span>: <span class="string">"10"</span>, <span class="string">"url"</span>: <span class="string">"https://www.facebook.com"</span> &#125;,</span><br><span class="line">  &#123; <span class="string">"name"</span>: <span class="string">"知乎"</span>, <span class="string">"alexa"</span>: <span class="string">"103"</span>, <span class="string">"url"</span>: <span class="string">"https://www.zhihu.com"</span> &#125;,</span><br><span class="line">  &#123; <span class="string">"name"</span>: <span class="string">"Github"</span>, <span class="string">"alexa"</span>: <span class="string">"109"</span>, <span class="string">"url"</span>: <span class="string">"https://www.github.com"</span> &#125;</span><br><span class="line">]</span><br><span class="line">x = mycol.insert_many(mylist)</span><br><span class="line"><span class="comment"># 输出插入的所有文档对应的 _id 值</span></span><br><span class="line">print(x.inserted_ids)</span><br></pre></td></tr></table></figure></p><p>输出结果如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ObjectId(<span class="string">'5b236aa9c315325f5236bbb6'</span>), ObjectId(<span class="string">'5b236aa9c315325f5236bbb7'</span>), ObjectId(<span class="string">'5b236aa9c315325f5236bbb8'</span>), ObjectId(<span class="string">'5b236aa9c315325f5236bbb9'</span>), ObjectId(<span class="string">'5b236aa9c315325f5236bbba'</span>)]</span><br></pre></td></tr></table></figure></p><h4 id="指定id的插入"><a href="#指定id的插入" class="headerlink" title="指定id的插入"></a>指定id的插入</h4><p>在字典加入_id字段即可</p><h3 id="查询文档"><a href="#查询文档" class="headerlink" title="查询文档"></a>查询文档</h3><p>MongoDB中使用find和find_one方法来查询集合中的数据，它类似于SQL中的SELECT语句。<br>本文的测试数据如下：<br><img src="/2019/02/16/使用Python操作Mangodb/数据项.png" alt="数据项"><br>数据库连接代码如下，在之后的示例代码中不会重复：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line">myclient = pymongo.MongoClient(<span class="string">"mongodb://10.245.142.249:27017/"</span>)</span><br><span class="line">mydb = myclient[<span class="string">"twittertopic"</span>]</span><br><span class="line">mycol = mydb[<span class="string">"sites"</span>]</span><br></pre></td></tr></table></figure></p><h4 id="查询一条数据"><a href="#查询一条数据" class="headerlink" title="查询一条数据"></a>查询一条数据</h4><p>我们可以使用find_one()方法来查询一条数据<br>代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">x = mycol.find_one()</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72852'</span>), <span class="string">'name'</span>: <span class="string">'Taobao'</span>, <span class="string">'alexa'</span>: <span class="string">'100'</span>, <span class="string">'url'</span>: <span class="string">'https://www.taobao.com'</span>&#125;</span><br></pre></td></tr></table></figure></p><h4 id="查询集合中所有的数据"><a href="#查询集合中所有的数据" class="headerlink" title="查询集合中所有的数据"></a>查询集合中所有的数据</h4><p>find() 方法可以查询集合中的所有数据，类似 SQL 中的 SELECT * 操作。<br>以下实例查找 sites 集合中的所有数据：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> mycol.find():</span><br><span class="line">  print(x)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72852'</span>), <span class="string">'name'</span>: <span class="string">'Taobao'</span>, <span class="string">'alexa'</span>: <span class="string">'100'</span>, <span class="string">'url'</span>: <span class="string">'https://www.taobao.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72853'</span>), <span class="string">'name'</span>: <span class="string">'QQ'</span>, <span class="string">'alexa'</span>: <span class="string">'101'</span>, <span class="string">'url'</span>: <span class="string">'https://www.qq.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72854'</span>), <span class="string">'name'</span>: <span class="string">'Facebook'</span>, <span class="string">'alexa'</span>: <span class="string">'10'</span>, <span class="string">'url'</span>: <span class="string">'https://www.facebook.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72855'</span>), <span class="string">'name'</span>: <span class="string">'知乎'</span>, <span class="string">'alexa'</span>: <span class="string">'103'</span>, <span class="string">'url'</span>: <span class="string">'https://www.zhihu.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72856'</span>), <span class="string">'name'</span>: <span class="string">'Github'</span>, <span class="string">'alexa'</span>: <span class="string">'109'</span>, <span class="string">'url'</span>: <span class="string">'https://www.github.com'</span>&#125;</span><br></pre></td></tr></table></figure></p><h4 id="查询指定字段的数据"><a href="#查询指定字段的数据" class="headerlink" title="查询指定字段的数据"></a>查询指定字段的数据</h4><p>我们可以使用find()方法来查询指定字段的数据，将要返回的字段对应值设置为1.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> mycol.find(&#123;&#125;,&#123; <span class="string">"_id"</span>: <span class="number">0</span>, <span class="string">"name"</span>: <span class="number">1</span>, <span class="string">"alexa"</span>: <span class="number">1</span> &#125;):</span><br><span class="line">  print(x)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'name'</span>: <span class="string">'Taobao'</span>, <span class="string">'alexa'</span>: <span class="string">'100'</span>&#125;</span><br><span class="line">&#123;<span class="string">'name'</span>: <span class="string">'QQ'</span>, <span class="string">'alexa'</span>: <span class="string">'101'</span>&#125;</span><br><span class="line">&#123;<span class="string">'name'</span>: <span class="string">'Facebook'</span>, <span class="string">'alexa'</span>: <span class="string">'10'</span>&#125;</span><br><span class="line">&#123;<span class="string">'name'</span>: <span class="string">'知乎'</span>, <span class="string">'alexa'</span>: <span class="string">'103'</span>&#125;</span><br><span class="line">&#123;<span class="string">'name'</span>: <span class="string">'Github'</span>, <span class="string">'alexa'</span>: <span class="string">'109'</span>&#125;</span><br></pre></td></tr></table></figure></p><p>除了_id不能在一个对象中同时指定0和1，如果你设置了一个字段为0.则其他都为1，反之亦然。<br>以下实例除了alexa字段外，其他都返回：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> mycol.find(&#123;&#125;,&#123; <span class="string">"alexa"</span>: <span class="number">0</span> &#125;):</span><br><span class="line">  print(x)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72852'</span>), <span class="string">'name'</span>: <span class="string">'Taobao'</span>, <span class="string">'url'</span>: <span class="string">'https://www.taobao.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72853'</span>), <span class="string">'name'</span>: <span class="string">'QQ'</span>, <span class="string">'url'</span>: <span class="string">'https://www.qq.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72854'</span>), <span class="string">'name'</span>: <span class="string">'Facebook'</span>, <span class="string">'url'</span>: <span class="string">'https://www.facebook.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72855'</span>), <span class="string">'name'</span>: <span class="string">'知乎'</span>, <span class="string">'url'</span>: <span class="string">'https://www.zhihu.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72856'</span>), <span class="string">'name'</span>: <span class="string">'Github'</span>, <span class="string">'url'</span>: <span class="string">'https://www.github.com'</span>&#125;</span><br></pre></td></tr></table></figure></p><p><strong>以下代码同时指定了0和1则会报错</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> mycol.find(&#123;&#125;,&#123; <span class="string">"name"</span>: <span class="number">1</span>, <span class="string">"alexa"</span>: <span class="number">0</span> &#125;):</span><br><span class="line">  print(x)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pymongo.errors.OperationFailure: Projection cannot have a mix of inclusion <span class="keyword">and</span> exclusion.</span><br></pre></td></tr></table></figure></p><h4 id="根据指定条件查询"><a href="#根据指定条件查询" class="headerlink" title="根据指定条件查询"></a>根据指定条件查询</h4><p>我们可以在 find() 中设置参数来过滤数据。<br>以下实例查找 name 字段为 “Taobao” 的数据：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">myquery = &#123;<span class="string">"name"</span>: <span class="string">"Taobao"</span>&#125;</span><br><span class="line">mydoc = mycol.find(myquery)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> mydoc:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72852'</span>), <span class="string">'name'</span>: <span class="string">'Taobao'</span>, <span class="string">'alexa'</span>: <span class="string">'100'</span>, <span class="string">'url'</span>: <span class="string">'https://www.taobao.com'</span>&#125;</span><br></pre></td></tr></table></figure></p><h4 id="高级查询"><a href="#高级查询" class="headerlink" title="高级查询"></a>高级查询</h4><p>查询的条件语句中，我们还可以使用修饰符。<br>以下实例用于读取 name 字段中第一个字母 ASCII 值大于 “H” 的数据，大于的修饰符条件为 {“$gt”: “H”} :<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">myquery = &#123; <span class="string">"name"</span>: &#123; <span class="string">"$gt"</span>: <span class="string">"H"</span> &#125; &#125;</span><br><span class="line">mydoc = mycol.find(myquery)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> mydoc:</span><br><span class="line">  print(x)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72852'</span>), <span class="string">'name'</span>: <span class="string">'Taobao'</span>, <span class="string">'alexa'</span>: <span class="string">'100'</span>, <span class="string">'url'</span>: <span class="string">'https://www.taobao.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72853'</span>), <span class="string">'name'</span>: <span class="string">'QQ'</span>, <span class="string">'alexa'</span>: <span class="string">'101'</span>, <span class="string">'url'</span>: <span class="string">'https://www.qq.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72855'</span>), <span class="string">'name'</span>: <span class="string">'知乎'</span>, <span class="string">'alexa'</span>: <span class="string">'103'</span>, <span class="string">'url'</span>: <span class="string">'https://www.zhihu.com'</span>&#125;</span><br></pre></td></tr></table></figure></p><h4 id="使用正则表达式查询"><a href="#使用正则表达式查询" class="headerlink" title="使用正则表达式查询"></a>使用正则表达式查询</h4><p>我们还可以使用正则表达式作为修饰符。<br>正则表达式修饰符只用于搜索字符串的字段。<br>以下实例用于读取 name 字段中第一个字母为 “T” 的数据，正则表达式修饰符条件为 {“$regex”: “^T”} :<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">myquery = &#123; <span class="string">"name"</span>: &#123; <span class="string">"$gt"</span>: <span class="string">"H"</span> &#125; &#125;</span><br><span class="line">mydoc = mycol.find(myquery)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> mydoc:</span><br><span class="line">  print(x)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72852'</span>), <span class="string">'name'</span>: <span class="string">'Taobao'</span>, <span class="string">'alexa'</span>: <span class="string">'100'</span>, <span class="string">'url'</span>: <span class="string">'https://www.taobao.com'</span>&#125;</span><br></pre></td></tr></table></figure></p><h4 id="返回指定条数记录"><a href="#返回指定条数记录" class="headerlink" title="返回指定条数记录"></a>返回指定条数记录</h4><p>如果我们要对查询结果设置指定条数的记录可以使用 limit() 方法，该方法只接受一个数字参数。</p><p>以下实例返回 3 条文档记录：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">myresult = mycol.find().limit(<span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> myresult:</span><br><span class="line">    print(x)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72852'</span>), <span class="string">'name'</span>: <span class="string">'Taobao'</span>, <span class="string">'alexa'</span>: <span class="string">'100'</span>, <span class="string">'url'</span>: <span class="string">'https://www.taobao.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72853'</span>), <span class="string">'name'</span>: <span class="string">'QQ'</span>, <span class="string">'alexa'</span>: <span class="string">'101'</span>, <span class="string">'url'</span>: <span class="string">'https://www.qq.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72854'</span>), <span class="string">'name'</span>: <span class="string">'Facebook'</span>, <span class="string">'alexa'</span>: <span class="string">'10'</span>, <span class="string">'url'</span>: <span class="string">'https://www.facebook.com'</span>&#125;</span><br></pre></td></tr></table></figure></p><h3 id="修改文档"><a href="#修改文档" class="headerlink" title="修改文档"></a>修改文档</h3><p>我们可以在 MongoDB 中使用 update_one() 方法修改文档中的记录。该方法第一个参数为查询的条件，第二个参数为要修改的字段。<br><strong>如果查找到的匹配数据多余一条，则只会修改第一条。</strong><br>以下实例将 alexa 字段的值为 100 的改为 12345:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">myquery = &#123; <span class="string">"alexa"</span>: <span class="string">"100"</span> &#125;</span><br><span class="line">newvalues = &#123; <span class="string">"$set"</span>: &#123; <span class="string">"alexa"</span>: <span class="string">"12345"</span> &#125; &#125;</span><br><span class="line">mycol.update_one(myquery, newvalues)</span><br><span class="line"><span class="comment"># 输出修改后的  "sites"  集合</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> mycol.find():</span><br><span class="line">  print(x)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72852'</span>), <span class="string">'name'</span>: <span class="string">'Taobao'</span>, <span class="string">'alexa'</span>: <span class="string">'12345'</span>, <span class="string">'url'</span>: <span class="string">'https://www.taobao.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72853'</span>), <span class="string">'name'</span>: <span class="string">'QQ'</span>, <span class="string">'alexa'</span>: <span class="string">'101'</span>, <span class="string">'url'</span>: <span class="string">'https://www.qq.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72854'</span>), <span class="string">'name'</span>: <span class="string">'Facebook'</span>, <span class="string">'alexa'</span>: <span class="string">'10'</span>, <span class="string">'url'</span>: <span class="string">'https://www.facebook.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72855'</span>), <span class="string">'name'</span>: <span class="string">'知乎'</span>, <span class="string">'alexa'</span>: <span class="string">'103'</span>, <span class="string">'url'</span>: <span class="string">'https://www.zhihu.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72856'</span>), <span class="string">'name'</span>: <span class="string">'Github'</span>, <span class="string">'alexa'</span>: <span class="string">'109'</span>, <span class="string">'url'</span>: <span class="string">'https://www.github.com'</span>&#125;</span><br></pre></td></tr></table></figure></p><p>update_one() 方法只能修匹配到的第一条记录，如果要修改所有匹配到的记录，可以使用 update_many()。<br>以下实例将查找所有以 F 或 Q 开头的 name 字段，并将匹配到所有记录的 alexa 字段修改为 123：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">myquery = &#123;<span class="string">"name"</span>: &#123;<span class="string">"$regex"</span>: <span class="string">"^F|^Q"</span>&#125;&#125;</span><br><span class="line">newvalues = &#123;<span class="string">"$set"</span>: &#123;<span class="string">"alexa"</span>: <span class="string">"123"</span>&#125;&#125;</span><br><span class="line">x = mycol.update_many(myquery, newvalues)</span><br><span class="line">print(x.modified_count, <span class="string">"文档已修改"</span>)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2</span> 文档已修改</span><br></pre></td></tr></table></figure></p><h3 id="数据排序"><a href="#数据排序" class="headerlink" title="数据排序"></a>数据排序</h3><p>sort() 方法可以指定升序或降序排序。<br>sort() 方法第一个参数为要排序的字段，第二个字段指定排序规则，1 为升序，-1 为降序，默认为升序。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mydoc = mycol.find().sort(<span class="string">"alexa"</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> mydoc:</span><br><span class="line">  print(x)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72855'</span>), <span class="string">'name'</span>: <span class="string">'知乎'</span>, <span class="string">'alexa'</span>: <span class="string">'103'</span>, <span class="string">'url'</span>: <span class="string">'https://www.zhihu.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72856'</span>), <span class="string">'name'</span>: <span class="string">'Github'</span>, <span class="string">'alexa'</span>: <span class="string">'109'</span>, <span class="string">'url'</span>: <span class="string">'https://www.github.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72852'</span>), <span class="string">'name'</span>: <span class="string">'Taobao'</span>, <span class="string">'alexa'</span>: <span class="string">'1111'</span>, <span class="string">'url'</span>: <span class="string">'https://www.taobao.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72853'</span>), <span class="string">'name'</span>: <span class="string">'QQ'</span>, <span class="string">'alexa'</span>: <span class="string">'123'</span>, <span class="string">'url'</span>: <span class="string">'https://www.qq.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72854'</span>), <span class="string">'name'</span>: <span class="string">'Facebook'</span>, <span class="string">'alexa'</span>: <span class="string">'123'</span>, <span class="string">'url'</span>: <span class="string">'https://www.facebook.com'</span>&#125;</span><br></pre></td></tr></table></figure></p><p>可以看到并不是按数字大小排序，而是字母序排序</p><h3 id="删除文档"><a href="#删除文档" class="headerlink" title="删除文档"></a>删除文档</h3><p>我们可以使用 delete_one() 方法来删除一个文档，该方法第一个参数为查询对象，指定要删除哪些数据。<br>以下实例删除 name 字段值为 “Taobao” 的文档：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">myquery = &#123; <span class="string">"name"</span>: <span class="string">"Taobao"</span> &#125;</span><br><span class="line">mycol.delete_one(myquery)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> mycol.find():</span><br><span class="line">  print(x)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72853'</span>), <span class="string">'name'</span>: <span class="string">'QQ'</span>, <span class="string">'alexa'</span>: <span class="string">'123'</span>, <span class="string">'url'</span>: <span class="string">'https://www.qq.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72854'</span>), <span class="string">'name'</span>: <span class="string">'Facebook'</span>, <span class="string">'alexa'</span>: <span class="string">'123'</span>, <span class="string">'url'</span>: <span class="string">'https://www.facebook.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72855'</span>), <span class="string">'name'</span>: <span class="string">'知乎'</span>, <span class="string">'alexa'</span>: <span class="string">'103'</span>, <span class="string">'url'</span>: <span class="string">'https://www.zhihu.com'</span>&#125;</span><br><span class="line">&#123;<span class="string">'_id'</span>: ObjectId(<span class="string">'5c693d21b2871c1be4e72856'</span>), <span class="string">'name'</span>: <span class="string">'Github'</span>, <span class="string">'alexa'</span>: <span class="string">'109'</span>, <span class="string">'url'</span>: <span class="string">'https://www.github.com'</span>&#125;</span><br></pre></td></tr></table></figure></p><h4 id="删除多个文档"><a href="#删除多个文档" class="headerlink" title="删除多个文档"></a>删除多个文档</h4><p>我们可以使用 delete_many() 方法来删除多个文档，该方法第一个参数为查询对象，指定要删除哪些数据。<br>删除所有 name 字段中以 F 开头的文档:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">myquery = &#123;<span class="string">"name"</span>: &#123;<span class="string">"$regex"</span>: <span class="string">"^F"</span>&#125;&#125;</span><br><span class="line">x = mycol.delete_many(myquery)</span><br><span class="line">print(x.deleted_count, <span class="string">"个文档已删除"</span>)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> 个文档已删除</span><br></pre></td></tr></table></figure></p><h4 id="删除集合中所有文档"><a href="#删除集合中所有文档" class="headerlink" title="删除集合中所有文档"></a>删除集合中所有文档</h4><p>delete_many() 方法如果传入的是一个空的查询对象，则会删除集合中的所有文档：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = mycol.delete_many(&#123;&#125;)</span><br><span class="line">print(x.deleted_count, <span class="string">"个文档已删除"</span>)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">3</span> 个文档已删除</span><br></pre></td></tr></table></figure></p><h4 id="删除集合"><a href="#删除集合" class="headerlink" title="删除集合"></a>删除集合</h4><p>我们可以使用 drop() 方法来删除一个集合。<br>以下实例删除了 sites 集合：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mycol.drop()</span><br></pre></td></tr></table></figure></p><p>如果删除成功 drop() 返回 true，如果删除失败(集合不存在)则返回 false。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Python-MongoDB&quot;&gt;&lt;a href=&quot;#Python-MongoDB&quot; class=&quot;headerlink&quot; title=&quot;Python MongoDB&quot;&gt;&lt;/a&gt;Python MongoDB&lt;/h2&gt;&lt;p&gt;MongoDB是目前最流行的NoSQL数据库
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>论文翻译之A Survey on Learning to Hash</title>
    <link href="http://andeper.cn/2019/01/15/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E4%B9%8BA-Survey-on-Learning-to-Hash/"/>
    <id>http://andeper.cn/2019/01/15/论文翻译之A-Survey-on-Learning-to-Hash/</id>
    <published>2019-01-15T01:06:36.000Z</published>
    <updated>2019-01-16T09:03:34.040Z</updated>
    
    <content type="html"><![CDATA[<p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong></p><h3 id="A-Survey-on-Learning-to-Hash"><a href="#A-Survey-on-Learning-to-Hash" class="headerlink" title="A Survey on Learning to Hash"></a>A Survey on Learning to Hash</h3><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Nearest neighbor search is a problem of ﬁnding the data points from a database such that the distances from them to the query point are the smallest. Learning to hash is one of the major solutions to this problem and has been widely studied recently. In this paper, we present a comprehensive survey of the learning to hash algorithms, categorize them according to the manners of preserving the similarities into: pairwise similarity preserving, multiwise similarity preserving, implicit similarity preserving, as well as quantization, and discuss their relations. We separate quantization from pairwise similarity preserving as the objective function is very different though quantization, as we show, can be derived from preserving the pairwise similarities. In addition, we present the evaluation protocols, and the general performance analysis and point out that the quantization algorithms perform superiorly in terms of search accuracy, search time cost, and space cost. Finally, we introduce a few future directions.</p><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>最邻近搜索是从数据库里面查找数据点的问题，使他们到查询点的距离最小。学习哈希是这个问题的主要解决方案之一，并且最近被广泛研究，在本文中，我们对哈希算法的学习进行了全面的调查，并根据保持相似性的方式对它们进行分类:成对相似性保持、多向相似性保持，隐式相似性保持以及量化，并讨论它们之间的关系。我们将量化和成对相似性保持分开，因为目标函数是完全不同的，正如我们所示，量化可以从保成对相似性得出，此外，我们提出了评估协议和一般性能分析，并指出量化算法在搜索精度，搜索时间和空间成本方面表现优异，最后我们介绍一些未来的方向。</p><p>Index Terms—Similarity search, approximate nearest neighbor search, hashing, learning to hash, quantization, pairwise similarity preserving, multiwise similarity preserving, implicit similarity preserving.<br>索引词：相似性搜索，近似最近邻搜索，散列，学习哈希，量化，保成对相似性，保多向相似性，保隐含相似性</p><h3 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h3><p>the problem of nearest neighbor search, also known as similarity search, proximity search, or close item search, is aimed at ﬁnding an item, called nearest neighbor, which is the nearest to a query item under a certain distance measure from a search (reference) database. The cost of ﬁnding the exact nearest neighbor is prohibitively high in the case that the reference database is very large or that computing the distance between the query item and the database item is costly. The alternative approach, approximate nearest neighbor search, is more efﬁcient and is shown to be enough and useful for many practical problems, thus attracting an enormous number of research efforts.<br>最近邻搜索问题，也称为相似性搜索，邻近搜索或者关闭项搜索，旨在找到一个称为最近邻的项，该项在特定距离度量方法上与数据库的查询项最近。在参考数据库非常大或者计算查询量和数据库项之间的距离的代价很高的情况下，找到确切的最邻近的代价非常高。替代方法就是最近邻搜索，它更有效，并且被证明足以用于许多实际问题，因此有大量的研究工作。</p><p>Hashing, a widely-studied solution to approximate nearest neighbor search, aims to transforming the data item to a low-dimensional representation, or equivalently a short code consisting of a sequence of bits, called hash code. There are two main categories of hashing algorithms: locality sensitive hashing [29] and learning to hash. Locality sensitive hashing (LSH) is data-independent. The research efforts mainly come from the theory community, such as proposing random hash functions satisfying the local sensitive property for various distance measures [5], [6], [7], [10], [11], [69], [78], proving better search efﬁciency and accuracy [10], [80], and developing better search schemes [15], [67], and the machine learning community, such as developing hash functions providing a similarity estimator with smaller variance [47], [37], [51], [36], or smaller storage [49], [50], or faster computation of hash functions [48], [51], [36], [88].<br>哈希是一种广泛研究的最近邻搜索的解决方案，旨在将数据项转换成低维表示，或者等效成一个短的比特序列，称为哈希码。哈希算法主要有两类：局部敏感哈希和学习哈希算法，局部敏感哈希与数据无关。研究工作主要来自理论界，比如提出满足各种距离测量的局部铭感哈希性的随机哈希函数[5], [6], [7], [10], [11], [69], [78]，证明了更好的搜索效率和准确性[10], [80],并开发了更好的搜索方案[15], [67]和机器学习社区，例如开发哈希函数，提供具有较小方差的相似性估计更快地哈希函数计算[48], [51], [36], [88].</p><p>Learning to hash, the interest in this survey, is a datadependent hashing approach, which aims to learn hash functions from a speciﬁc dataset so that the nearest neighbor search result in the hash coding space is as close to the search result in the original space as possible, and the search cost and the space cost are also small. Since the pioneering algorithm, spectral hashing [107], learning to hash has been attracting a large amount of research interests in computer visionand machine learningand has beenappliedto a widerange of applications such as large scale object retrieval [33], image classiﬁcation and detection [85] [94], and so on.<br>学习哈希，是一种数据相关的哈希方法，旨在从特定的数据集中学习哈希函数，使哈希编码空间中最近邻搜索结果和原始空间中搜索结果尽可能接近，并且搜索成本和空间成本很小。自从开创算法，谱哈希以来没学习哈希已经吸引了大量计算机视觉和机器学习的研究，并且已经被广泛应用，如大规模检索，图像分类和检测等。</p><p>The main methodology of learning to hash is similarity preserving, i.e., minimizing the gap between the similarities or similarity orders computed/given in the original space and in the hash coding space in various forms. The similarity in the original space might be from the semantic (class) information, or from the distance (e.g., Euclidean distance) computed in the original space, which is more widely interested and studied in most real applications, e.g., large scale search by image and image classiﬁcation, and thus the main focus in this paper.<br>学习哈希的主要方法是保相似性，即以各种形式最小化在初始空间和哈希编码空间中计算得相似性或相似性次序之间的距离。原始空间中的相似性可以来自语义信息，或者来自原始空间中的计算的距离，其在大多数实际应用中被广泛的研究，例如大规模搜索图像和图像分类，因而是本文的主要焦点</p><p>This survey categorizes the algorithms according to the similarity preserving manners into: pairwise similarity preserving, multiwise similarity preserving, implicit similarity preserving, and quantization that, we show, is a form of pairwise similarity preserving, and discusses other problems, including evaluation datasets and schemes, online search given the hash codes, and so on. In addition, we present the empirical observation that the quantization approach outperforms other approaches and give some analysis about this observation. Finally, we point out the future directions, such as an end-to-end learning strategy for real applications, directly learning the hash codes from the object, e.g., image, instead of ﬁrst learning the representations and then learning the hash codes from the representations.<br>该调查根据相似性保持方法吧算法分类为：成对相似性保持、多向相似性保持，隐式相似性保持以及量化，我们表明，这是一种成对相似性保持的形式，并讨论了其他问题，包括评估数据集和方案，在线搜索出哈希码，等等。此外，我们提出了经验观察，即量化方法优于其他方法，并对此观察进行了一些分析。最后，我们指出未来的方向，例如实际应用端到端学习策略，直接从对象学习哈希码，例如图像，而不是先学习表示，然后从表示中学习哈希码。</p><h4 id="1-1-Organization-of-This-Paper"><a href="#1-1-Organization-of-This-Paper" class="headerlink" title="1.1 Organization of This Paper"></a>1.1 Organization of This Paper</h4><p>The organization of the remaining part is given as the following. Section 2 introduces the exact and approximate nearest neighbor searchproblems,and the searchalgorithms with hashing. Section 3 provides the basic concepts in the learning-to-hashing approach and categorizes the existing algorithms from the perspective of loss function into four main classes: pairwise alignment, multiple-wise alignment,implicit alignment and quantization, which are discussed in Sections 4, 5, 6, and 7, respectively. Section 8 presents other works in learning to hash. Sections 9 and 10 gives some evaluation protocols and performance analysis. Finally, Sections 11 and 12 point out the future research trends and conclude this survey, respectively.</p><h4 id="1-1-本文的组织"><a href="#1-1-本文的组织" class="headerlink" title="1.1 本文的组织"></a>1.1 本文的组织</h4><p>其余部分的组织如下，第2节介绍精确和近似的最近邻搜索问题，以及使用哈希的搜索算法，第3节提供了学习哈希的基本概念，并将现有算法从损失函数的角度分为四类，成对对齐,多向对齐，隐式对齐和量化，分别在第4,5,6，7节中讨论。第8节介绍了学习哈希的其他工作。第9节和第10节中给出了评估协议和性能分析。最后在11和12节分别指出了未来的研究趋势并总结了这项研究。</p><h3 id="2-BACKGROUND"><a href="#2-BACKGROUND" class="headerlink" title="2 BACKGROUND"></a>2 BACKGROUND</h3><h4 id="2-1-Nearest-Neighbor-Search"><a href="#2-1-Nearest-Neighbor-Search" class="headerlink" title="2.1 Nearest Neighbor Search"></a>2.1 Nearest Neighbor Search</h4><p>Exact nearest neighbor search is deﬁned as searching an item NN(q) (called nearest neighbor) for a query N item q from a set of items X = {x1,x2,··· ,xN} so that NN(q) = argminx∈X dist(q,x), where dist(q,x) is a distance computed between q and x. A straightforward generalization is K-NN search, where K nearest neighbors (KNN(q)) are needed to be found.</p><h3 id="2-背景"><a href="#2-背景" class="headerlink" title="2 背景"></a>2 背景</h3><h4 id="2-1-最近邻搜索"><a href="#2-1-最近邻搜索" class="headerlink" title="2.1 最近邻搜索"></a>2.1 最近邻搜索</h4><p>精确最近邻搜索被定义为搜索一个项NN(q)(称为最近邻居),从一组项$\chi=\{x_1,x_2,……,x_n\}$查询N项得到$NN(q) = argmin_{x\in \chi}dist(q,x)$,其中$dist(q,x)$是q和x之间计算的距离，简单的推广是KNN搜索，其中需要找到K个最近邻居$(KNN(q))$。</p><p>The distance between an arbitrary pair of items x and q depends on the speciﬁc nearest search problem. A typical example is that the search (reference) database X lies in a d-dimensional space Rd and the distance is induced by an ls norm, kx − qks = (Pd i=1 |xi − qi|s)1/s. The search problem under the Euclidean distance, i.e., the l2 norm, is widely studied. Other notions of the search database, for example, the data item is formed by a set, and distance measures,such as ℓ1 distance, cosine similarity and so on, are also possible.<br>任意一对项x和q之间的距离取决于特定的最近搜索问题。一个典型的例子就是搜索数据库$\chi$位于d维空间$\mathbb{R}^d$,距离是由$l_s$范数引起，$\left|x-q\right|_s = (\sum_{i=1}^{d}{\left|x_i-q_i\right|}^s)^{1/s}$。欧几里得距离下的搜索问题，即$l_2$范数，是广泛的研究。搜索数据库的其他概念，如数据项是由一组形成的，距离测量例如$l_1$距离，余弦相似性等等也是可能的。</p><p>There exist efﬁcient algorithms (e.g., k-d trees and its variants) for exact nearest neighbor search in lowdimensional cases. In large scale high-dimensional cases, it turns out that the problems become hard and most algorithms even take higher computational cost than the naive solution, linear scan. Therefore, a lot of recent efforts are moved to search approximate nearest neighbors: (1 + ǫ)approximate nearest neighbor search [29], which is studied mainly in the theory community, and time-ﬁxed approximate nearest neighbor search. Other nearest neighbor search problems include (approximate) ﬁxed-radius near neighbor (R-near neighbor) problem, and randomized nearest neighbor search which the locality sensitive hashing research community is typical interested in.<br>在低维情况下存在用于精确最近邻搜索的有效算法，例如k-d树及其变体。在大规模的高维情况下，事实证明问题变得困难，并且大多数算法甚至比简单的解决方案线性扫描花费更高的计算成本。因此，最近很多工作转移到搜索近似最近邻：$(1+\epsilon)$-近似最近邻搜索，其主要在理论界研究，并且时间固定近似最近邻走索。其他最近邻搜索问题包括(近似)固定半径近邻(R近邻)问题，以及局部敏感哈希研究社区通常感兴趣的随机最近邻搜索</p><p>Time-ﬁxed approximate nearest neighbor search is studied mainly in machine learning and computer vision for real applications, such as the learning to hash approach, though there is usually lack of elegant theory. The goal is to make the average search as accurate as possible by comparing the returned K approximate nearest neighbors and the K exact nearest neighbors, and the query cost as small as possible.<br>时间固定近似最近邻搜索主要在及其学习和计算机视觉中用于实际应用，例如学习哈希方法，尽管通常缺乏理论基础，目标是通过比较返回的K近似最近邻和K精确最近邻来使得平均搜索尽可能准确，并且查询成本尽可能小。</p><h4 id="2-2-Search-with-Hashing"><a href="#2-2-Search-with-Hashing" class="headerlink" title="2.2 Search with Hashing"></a>2.2 Search with Hashing</h4><p>The hashing approach aims to map the reference (and query) items to the target items so that approximate nearest neighbor search is efﬁciently and accurately performed by resorting to the target items and possibly a small subset of the raw reference items. The target items are called hash codes (also known as hash values, simply hashes). In this paper, we may also call it short/compact codes interchangeably.</p><h4 id="2-2-使用哈希搜索"><a href="#2-2-使用哈希搜索" class="headerlink" title="2.2 使用哈希搜索"></a>2.2 使用哈希搜索</h4><p>哈希方法旨在将参考（和查询）项映射到目标项，以便通过目标项和可能的原始参考项的一小部分来有效且准确地执行近似最近邻搜索。目标项被称为哈希码。在本文中，我们也可以将他称为短/紧凑码。</p><p>The hash function is formally deﬁned as: y = h(x), where y is the hash code, and may be a binary value, 1 and 0 (or −1) or an integer, and h(·) is the hash function. In the application to approximate nearest neighbor search, usually several hash functions are used together to compute the compound hash code: y = h(x), where y = [y1 y2 ··· yM]T and h(x) = [h1(x) h2(x) ··· hM(x)]T . Here we use a vector y to represent the compound hash code for convenience.<br>哈希函数形式上定义为：$y = h(x)$,其中y是哈希码，也可能是二进制值，1和0或证书，h(*)是哈希函数。在近似最近邻搜索的应用中，通常使用几个哈希函数来计算复合哈希码：$y = h(x)$,其中$y = {[y_1,y_2,\cdots,y_m]}^t$,并且$h(x) = [h_1(x)h_2(x)\cdots h_M(x)]^t$，为了方便起见这里我们使用一个向量来表示复合哈希码。</p><p>There are two basic strategies for using hash codes to perform nearest (near) neighbor search: hash table lookup and hash code ranking. The search strategies are illustrated in Figure 1.<br>使用哈希码有两种最基本的策略执行最近邻搜索：哈希表查找和哈希码排名。搜索策略如图1所示。<br><img src="/assets/图1.png" alt="图1"><br>Fig. 1. Illustrating the search strategies. (a) Multi table lookup: the list corresponding to the hash code of the query in each table is retrieved. (b) Single table lookup: the lists corresponding to and near to the hash code of the query are retrieved. (c) Hash code ranking: compare the query with each reference item in the coding space. (d) Non exhaustive search: hash table lookup (or other inverted index struture) retrieves the candidates, followed by hash code ranking over the candidates. The hash codes are different in two stages.<br>图1。说明搜索策略。（a）多表查找：检索与每个表中的查询的哈希码对应的列表。（b）单表查找：检索与查询的哈希码对应和接近的列表。（c）哈希码排名：将查询与编码空间中的每个参考项进行比较。（d）非穷举搜索：哈希表查找（或其他反向索引结构）检索候选者，然后对候选者进行哈希码排名。 哈希码在两个阶段中是不同的。</p><p>The main idea of hash table lookup for accelerating the search is to reduce the number of the distance computations from N to N′ (N ≫ N′), and thus the time complexity is reduced from O(Nd) to O(N′d). The data structure, called hash table (a form of inverted index), is composed of buckets with each indexed by a hash code. Each reference item x is placed into a bucket h(x). Different from the conventional hashing algorithm in computer science that avoids collisions (i.e., avoids mapping two items into some same bucket), the hashing approach using a hash table aims to maximize the probability of collision of near items. Given the query q, the items lying in the bucket h(q) are retrieved as the candidates of the nearest items of q, usually followed by a reranking step: rerank the retrieved nearest neighbor candidates according to the true distances computed using the original features and attain the K nearest neighbors or R-near neighbors<br>用于加速搜索的哈希表查找的主要思想是将距离计算的数量从$N$减少到$N’(N\to N’)$,从而减少时间复杂度，从$O(Nd)$减少到$O(N’d)$。数据结构称为哈希表（一种倒排索引的形式），由组成每个桶由哈希码索引。每个参考项目x被放入桶$h(x)$中。与避免冲突的计算机科学中的传统哈希算法不同（即避免两个项映射到同一个桶中），使用哈希表的哈希方法旨在最大化近项目的冲突概率。给定查询q，检索位于h(q)中的项作为q的最近项的候选，通常接着的是重新排名的步骤：重新排名被检索到的最近邻候选，根据使用原始特征计算的真实距离并获得K个最近邻或R近邻。</p><p>To improve the recall, two ways are often adopted. The ﬁrst way is to visit a few more buckets (but with a single hash table), whose corresponding hash codes are the nearest to (the hash code of) the query h(q) in terms of the distances in the coding space. The second way is to construct more hash tables. The items lying in the L hash buckets h1(q),··· ,hL(q) are retrieved as the candidates of near items of q. To guarantee the high precision, each of the L hash codes, yi, needs to be a long code. This means that the total number of the buckets is too large to index directly, and thus, the buckets that are nonempty are retained by using conventional hashing of the hash codes hl(x).<br>为了改善召回率，通常采用两种方式。第一种方法是访问几个桶（使用单个哈希表），其对应的哈希码就编码空间的距离而言最接近查询h(q)的哈希码。第二种方法是构造更多的哈希表。位于L哈希桶$h_1(q),h_2(q) \cdots,h_L(q)$中的项被检索为最靠近q的项目。为了保证高精度，每个L哈希码$y_i$需要是长码。这意味着桶的总数太大而不能直接索引，因此，通过使用散列码$h_1(x)$的常规哈希来保留非空的桶。</p><p>The second way essentially stores multiple copies of the id for each reference item. Consequently, the space cost is larger. In contrast, the space cost for the ﬁrst way is smaller as it only uses a single table and stores one copy of the id for each reference item, but it needs to access more buckets to guarantee the same recall with the second way. The multiple assignment scheme is also studied: construct a single table, but assign a reference item to multiple hash buckets. In essence, it is shown that the second way, multiple hash tables, can be regarded as a form of multiple assignment.<br>第二种方法基本上存储了多个副本和每个参考项的id，因此，空间成本更大。相比之下，第一种方式的空间成本较小，因为它只使用一个表并为每个参考项存储一个id的副本，但它需要访问更多的桶来保证与第二种方式相同的召回率。还研究了多重分配方案：构造单个表，但将参考项分配给多个哈希桶。本质上，它表明多个哈希表的方式可以被视为多重赋值的一种形式。</p><p>Hash code ranking performs an exhaustive search: compare the query with each reference item by fast evaluating their distance (e.g., using distance table lookup or using the function popcnt for Hamming distance) according to (the hash code of) the query and the hash code of the reference item, and retrieve the reference items with the smallest distances as the candidates of nearest neighbors. Usually this is followed by a reranking step: rerank the retrieved nearest neighbor candidates according to the true distances computed using the original features and attain the K nearest neighbors or R-near neighbors.<br>哈希码排名执行穷举搜索：通过快速评估距离，将查询与每个参考项进行比较，（例如，根据查询的哈希码和参考项的哈希码，使用距离表查找或使用popcnt函数计算汉明距离），并检索具有最小距离的参考项作为最近邻居的候选者。通常，这之后是重新排名步骤：根据使用原始特征计算的真实距离重新获得检索的最近邻居候选者，并获得K个最近邻居或R邻近邻居。</p><p>This strategy exploits one main advantage of hash codes: the distance using hash codes is efﬁciently computed and the cost is much smaller than that of the computation in the original input space, reduced from d to d′ where d ≫ d′ and the whole cost is reduced from Nd to Nd′.<br>该策略利用哈希码的一个主要优点：使用哈希码的距离得到有效计算，成本远小于原始输入空间中的计算成本，从$d$减少到$d’$，其中$d\to d’$和整个成本从$Nd$减少到$Nd’$。</p><p>Comments: Hash table lookup is mainly used in locality sensitive hashing, and has been used for evaluating learning to hash in a few publications. It is observed that hash table lookup with binary hash codes shows inferior performance and hence rarely adopted in reality, while hash table lookup with quantization-based hash codes, is widely used in the non-exhaustive strategy to retrieve coarse candidates. In comparison to hash table lookup, hash code ranking is superior in search accuracy while inferior in search efﬁciency, and overall performs better, and thus more widely used in real applications and in experimental evaluations.<br>注释：哈希表查找主要用于局部敏感哈希，并在一些出版物中已用于评估学习哈希。 据观察，使用二进制哈希码的哈希表查找显示出较差的性能，因此在现实中很少采用，而基于量化的哈希码的哈希表查找被广泛用于非穷举策略以检索粗略候选。与哈希表查找相比，哈希码排名在搜索精度方面优越，而在搜索效率方面较差，并且整体表现更好，因此在实际应用和实验评估中更广泛地使用。<br>A practical way is to do a non-exhaustive search: ﬁrst retrieve a small set of candidates using inverted index, and then compute the distances of the query with the candidates using the hash codes, providing the top candidates subsequently reranked using the original features. Other research efforts include organizing the hash codes to avoid exhaustive search with a data structure, such as a tree or a graph structure [73].<br>一种实用的方法是进行非详尽搜索：首先使用倒排索引检索一小组候选，然后使用哈希码计算查询与候选的距离，提供随后使用原始特征重新排序的顶级候选。其他研究工作包括组织哈希码以避免使用数据结构进行穷举搜索，例如树或图结构[73]。</p><h3 id="3-LEARNING-TO-HASH"><a href="#3-LEARNING-TO-HASH" class="headerlink" title="3 LEARNING TO HASH"></a>3 LEARNING TO HASH</h3><p>Learning to hash is a task of learning a (compound) hash function, y = h(x), mapping an input item x to a compact<br>code y, with the goals: the nearest neighbor search result for a query q is as close to the true nearest search result as possible and the search in the coding space is also efﬁcient. A learning-to-hash approach needs to consider three problems for computing the hash codes: what hash function h(x) is adopted, what similarity in the coding space is used and what similarity is provided in the input space, what loss function is chosen for the optimization objective.</p><h3 id="学习哈希"><a href="#学习哈希" class="headerlink" title="学习哈希"></a>学习哈希</h3><p>学习哈希是学习（复合）哈希函数的任务，y = h（x），将输入项x映射到紧凑码y，具有目标：查询q的最近邻搜索结果尽可能接近真实的最近搜索结果，并且在编码空间中的搜索也是有效的。学习哈希方法需要考虑计算哈希码的三个问题：采用什么哈希函数h（x），在编码空间中使用什么相似性以及在输入空间中提供什么相似性，什么是损失函数 被选择用于优化目标。</p><h4 id="3-1-Hash-Function"><a href="#3-1-Hash-Function" class="headerlink" title="3.1 Hash Function"></a>3.1 Hash Function</h4><p>The hash function can be a form based on linear projection, kernels, spherical function, neural network, a nonparametric function, and so on. One popular hash function is the linear hash function: <script type="math/tex">y = h(x) = sgn(w^Tx + b). (1)</script>where sgn(z) = 1 if z &gt; 0 and sgn(z) = 0 (or equivalently −1) otherwise, w is the projection vector, and b is the bias variable. The kernel function,<script type="math/tex">y = h(x) = sgn(\sum_{t=1}^{T}w_tK(s_t,x)+b.(2)</script>is also adopted in some approaches, where {st} is a set of representative samples that are randomly drawn from the dataset or cluster centers of the dataset. and {wt} are the weights. The non-parametric function based on nearest vector assignment is widely used for quantization-based solutions:<script type="math/tex">y = arg\min_{k\in\{1,···,K\}}\|x-c_k\|_2.(3)</script>where {c1,··· ,cK} is a set of centers computed by some algorithm, e.g., K-means and $y\in Z$ is an integer. In contrast to other hashing algorithms in which the distance, e.g., Hamming distance, is often directly computed from hash codes, the hash codes generated from the nearest vector assignment-based hash function are the indices of the nearest vectors, and the distance is computed using the centers corresponding to the hash codes.<br>哈希函数可以是基于线性投影，核函数，球函数，神经网络,非参数函数等形式，一种流行的哈希函数是线性哈希函数<script type="math/tex">y = h(x) = sgn(w^Tx + b). (1)</script>其中如果z&gt; 0，sgn(z)= 1否则sgn（z）= 0（或等效-1），w是投影矢量，b是偏差变量。内核函数<script type="math/tex">y = h(x) = sgn(\sum_{t=1}^{T}w_tK(s_t,x)+b.(2)</script>在一些方法中也被采用，其中{st}是从数据集的数据集或聚类中心随机抽取的一组代表性样本。 和{wt}是权重。 基于最近矢量分配的非参数函数广泛用于基于量化的解决方案：<script type="math/tex">y = arg\min_{k\in\{1,···,K\}}\|x-c_k\|_2.(3)</script>其中$\{c_1,\cdots,c_K\}$是由一些算法计算的一组中心，例如，K-means和$y\in Z$是整数。 与通常直接从哈希码计算距离（例如汉明距离）的其他哈希算法相比，从最近的基于向量分配的哈希函数生成的哈希码是最近向量的索引，并且计算距离使用与哈希码对应的中心。</p><p>Hash functions are an important factor inﬂuencing the search accuracy using the hash codes, as well as the time cost of computing hash codes. A linear function is efﬁciently evaluated, while the kernel function and the nearest vector assignment based function leads to better search accuracy as they are more ﬂexible. Almost all the methods using a linear hash function can be extended to kernelized hash functions. Thus we do not use hash functions to categorize the hash algorithms.<br>哈希函数是使用哈希码影响搜索准确性的重要因素，以及计算哈希码的时间成本。线性函数被有效地评估，而核函数和最近的基于向量分配的函数导致更好的搜索准确性，因为它们更灵活。几乎所有使用线性哈希函数的方法都可以扩展到内核哈希函数。因此，我们不使用哈希函数来分类哈希算法。<br>There are various algorithms developed and exploited to optimize the hash function parameters. We summarize the common ways to handle the sgn function which is a main factor leading to the difﬁculty of estimating the parameters (e.g., the projection vector w in the linear hash function). There are roughly three approximation estimation schemes. The ﬁrst one is a continuous relaxation, e.g., sigmoid relaxation sgn(z) ≈ φα(z) = 1 1+e−αz . The second one is directly dropping the sign function sgn(z) ≈ z. The third one is a two-step scheme [53], [54] with its extension to iterative two step optimization [17]: optimizing the binary codes without considering the hash function, followed by estimating the function parameters from the optimized hash codes.<br>开发并利用各种算法来优化哈希函数参数。我们总结了处理sgn函数的常用方法，这是导致估计参数的困难的主要因素（例如，线性散列函数中的投影向量w）。大致有三种近似估计方案。 第一个是连续松弛，例如，S形弛豫sgn（z）≈φα（z）= 11 + e-αz。 第二个是直接丢弃符号函数sgn（z）≈z。 第三个是两步方案[53]，[54]，它扩展到迭代两步优化[17]：优化二进制代码而不考虑哈希函数，然后从优化的哈希码估计函数参数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;A-Survey-on-Learning-to-Hash&quot;&gt;&lt;a href=&quot;#A-Survey-on-Learning-to-Hash&quot; class=
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>文本向量化</title>
    <link href="http://andeper.cn/2019/01/11/%E6%96%87%E6%9C%AC%E5%90%91%E9%87%8F%E5%8C%96/"/>
    <id>http://andeper.cn/2019/01/11/文本向量化/</id>
    <published>2019-01-11T02:18:42.000Z</published>
    <updated>2019-01-15T03:13:01.168Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>文本向量化是将文本表示成一系列能够表达文本语义的向量</p><h3 id="向量化算法word2vec"><a href="#向量化算法word2vec" class="headerlink" title="向量化算法word2vec"></a>向量化算法word2vec</h3><p>词袋模型是最早的以词语为基本单位的文本向量化方法，下面举例说明改方法的原理。首先给出两个简单的文本如下<br>John likes to watch movies, Mary likes too.<br>John also likes to watch football games.</p><p>基于上述文档中出现的单词构建如下词典(dictionary)<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">"John"</span>:<span class="number">1</span>,<span class="string">"likes"</span>:<span class="number">2</span>,<span class="string">"to"</span>:<span class="number">3</span>,<span class="string">"watch"</span>:<span class="number">4</span>,<span class="string">"movies"</span>:<span class="number">5</span>,<span class="string">"also"</span>:<span class="number">6</span>,<span class="string">"football"</span>:<span class="number">7</span>,<span class="string">"games"</span>:<span class="number">8</span>,<span class="string">"Mary"</span>:<span class="number">9</span>,<span class="string">"too"</span>:<span class="number">10</span>&#125;</span><br></pre></td></tr></table></figure></p><p>上面词典中包含10个单词，每个单词有唯一的索引，那么每个文本我们可以使用一个10维的向量来表示。如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p><p>上面是每个单词在文本中出现的频率，和原本文本中单词出现的顺序没有关系</p><p><strong>缺点：</strong><br>1.唯独灾难，如果有10000个单词，需要10000维的向量表示<br>2.无法保留词序信息<br>3.存在语义鸿沟的问题</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h3&gt;&lt;p&gt;文本向量化是将文本表示成一系列能够表达文本语义的向量&lt;/p&gt;
&lt;h3 id=&quot;向量化算法word2vec&quot;&gt;&lt;a href=&quot;#向量化算法w
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>改善深层神经网络之深度学习的实用层面</title>
    <link href="http://andeper.cn/2018/07/26/%E6%94%B9%E5%96%84%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2/"/>
    <id>http://andeper.cn/2018/07/26/改善深层神经网络之深度学习的实用层面/</id>
    <published>2018-07-26T01:40:53.000Z</published>
    <updated>2018-07-27T09:26:12.361Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Settiing-up-your-ML-application"><a href="#Settiing-up-your-ML-application" class="headerlink" title="Settiing up your ML application"></a>Settiing up your ML application</h3><p>Train/dev/test sets<br>应用型机器学习是一个高度的迭代过程</p><p>训练神经网络时通常要做出很多决策：<br>神经网络分多少层<br>每层有多少个隐藏单元<br>学习速率是多少<br>各层采用哪种激活函数</p><p>为了找到更好的神经网络需要不断迭代更新自己的方案</p><p>数据集通常被分为数据集 验证集 测试集</p><p>数据集规模较小时，适用传统的划分比例<br>数据集规模较大时，验证集和测试集可以占到数据总量的10%以下</p><p>要确保训练和测试集来自同一分布</p><h3 id="Bias-and-Variance"><a href="#Bias-and-Variance" class="headerlink" title="Bias and Variance"></a>Bias and Variance</h3><p><img src="/2018/07/26/改善深层神经网络之深度学习的实用层面/拟合.png" alt="拟合"><br>欠拟合，适度拟合和过拟合</p><p>可以通过几个指标来研究偏差和方差<br>理解偏差和方差两个关键指标数据是训练集误差和验证集误差<br>训练集误差小，验证集误差大：过拟合，高方差<br>训练集误差大，验证集误差大：欠拟合，高偏差<br>训练集误差大，验证集误差更大：可能部分过拟合部分欠拟合，高方差高偏差<br>最优误差/贝叶斯误差：如果最优误差很高，训练集误差验证集误差大也很正常</p><h3 id="Basic-recipe-for-machine-learning"><a href="#Basic-recipe-for-machine-learning" class="headerlink" title="Basic recipe for machine learning"></a>Basic recipe for machine learning</h3><p>高偏差：规模更大的网络结构，延长训练时间<br>高方差：采集更多数据，正则化减少过拟合</p><h3 id="Regularizing-your-neural-network"><a href="#Regularizing-your-neural-network" class="headerlink" title="Regularizing your neural network"></a>Regularizing your neural network</h3><h4 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h4><script type="math/tex; mode=display">\min_{w,b} J(w,b)</script><script type="math/tex; mode=display">J(w,b) = \frac {1}{m} \sum_{i=1}^{m} L({\hat{y}^{(i)},y^{(i)}}) + \frac{\lambda}{2m}{\left\|w\right\|}^2_2</script><script type="math/tex; mode=display">{\left\|w\right\|}^2_2 = \sum_{j=1}^{n_x}w^2_j = w^Tw</script><p>L2正则化</p><h3 id="Neural-network"><a href="#Neural-network" class="headerlink" title="Neural network"></a>Neural network</h3><p>在神经网络中实现正则化</p><script type="math/tex; mode=display">J(w^{[1]},b^{[1]},...,w^{[L]},b^{[L]},) = \frac{1}{m}\sum_{i=1}^{m}L({\hat{y}^{(i)},y^{(i)}})+\frac{\lambda}{2m}\sum_{l=1}^{L}{\left\|w^{[l]}\right\|}^2_F</script><script type="math/tex; mode=display">{\left\|w^{[l]}\right\|}^2_F = \sum_{i=1}^{n^{[l-1]}}\sum_{j=1}^{n^{[l]}}(w^{[l]}_{ij})^2</script><script type="math/tex; mode=display">w:(n^{[l-1]},n^{[l]})</script><p>L是网络层数。下面的矩阵范数被定义为矩阵中所有元素的平方求和。这个范数被称为Frobenius norm</p><p>如何使用该范数进行梯度下降？用backprop计算出$\frac{\partial J}{\partial w^{[L]}}$</p><p>然后加上正则项：</p><script type="math/tex; mode=display">dw^{[l]} = (form backup) +\frac{\lambda}{m}w^{[l]}</script><p>然后更新权重</p><script type="math/tex; mode=display">w^{[l]} := w^{[l]} - \alpha dw^{[l]}</script><p>这就是L2正则化有时被称为权重衰减的原因</p><h3 id="Why-regularization-reduces-overfitting"><a href="#Why-regularization-reduces-overfitting" class="headerlink" title="Why regularization reduces overfitting"></a>Why regularization reduces overfitting</h3><p>假设我们面对的是这样一个网络模型<br><img src="/2018/07/26/改善深层神经网络之深度学习的实用层面/网络模型.jpg" alt="网络模型"><br>如果正则话$\lambda$设置足够大，那么会导致权重矩阵W被设置为接近0的值，即$W[l] \approx 0$。本质上就是把许多隐藏层的权值设为0，尝试消除隐藏层对整个网络的影响。可以看作<br><img src="/2018/07/26/改善深层神经网络之深度学习的实用层面/网络模型2.jpg" alt="网络模型2"><br>这个被大大简化的网络会变成一个很小的网络，能小到如同线性单元，但深度还是很大，网络会从过拟合状态变得更接近高偏差的状态</p><p><img src="/2018/07/26/改善深层神经网络之深度学习的实用层面/fitting.jpg" alt="fitting"><br>过拟合和欠拟合都有些极端，需要找到一个这种的$\lambda$，使得拟合成拟合适中的情况</p><h3 id="Dropout-Regularization"><a href="#Dropout-Regularization" class="headerlink" title="Dropout Regularization"></a>Dropout Regularization</h3><p>随机失活<br>原理：随机删除神经网络单元，具体做法是对神经网络的各个单元，以抛硬币的方式来决定其去留，这样会大大简化神经网络，对简化后的神经网络进行训练</p><p>如何实施Dropout正则化？<br>Inverted dropout(反向随机失活)</p><p>举例在一层中如何实现的<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># keep_prob表示保留某个隐藏单元的概率</span></span><br><span class="line">l = <span class="number">3</span>,keep_prob = <span class="number">0.8</span></span><br><span class="line"><span class="comment"># 向量d3表示第三层的dropout向量：是一个随机矩阵，决定哪些单元归零</span></span><br><span class="line">d3 = np.random.rand(a3.shape[<span class="number">0</span>],a3.shape[<span class="number">1</span>])</span><br><span class="line">a3 = np.multiply(a3,d3)</span><br><span class="line"><span class="comment"># 通过除以keep_prob来确保a3的期望值不变</span></span><br><span class="line">a3 /= keep_prob</span><br></pre></td></tr></table></figure></p><h3 id="Understanding-Dropout"><a href="#Understanding-Dropout" class="headerlink" title="Understanding Dropout"></a>Understanding Dropout</h3><p>为什么随机失活会起作用</p><p><strong>第一种理解：</strong> Drop通过每次迭代时，神经网络都会变得比以前小，使用一个更小的神经网络看起来和正则化是一样的</p><p><strong>第二种理解：</strong> 从单个神经元的角度，神经元的作用是接收输入并生成一些有意义的输出，而通过dropout，神经元会随机删除，也就是当前神经元不能依赖任一输入神经元，所以不会给任何一个输入加上太多权重，因此单元将通过这种方式积极传播开，并为其每个输入单元各加适量权重，通过传播所有权重，dropout将产生压缩权重的平方范数的效果，这就和L2正则化类似，通过压缩权重，防止过拟合，但对不同权重的衰减是不同的，取决于倍增激活函数的大小</p><p>dropout正则化是预防过拟合，一般不会应用dropout正则化，而dropout主要应用于计算机视觉，因为这个领域数据量不足够打，且输入层的维度很高，容易存在过拟合的问题。</p><p>dropout的一个缺点是成本函数J不再被明确定义，因为每次迭代都会随机删除一些节点，成本函数的计算变得非常困难，所以绘制成本函数和迭代次数的曲线就非常困难，通常的做法是先关掉dropout函数，运行单吗，确保成本函数是单调递减的，然后再打开dropout函数</p><h3 id="Other-regularization-methods"><a href="#Other-regularization-methods" class="headerlink" title="Other regularization methods"></a>Other regularization methods</h3><ol><li><p>扩增数据<br>如果扩增数据的代价很大，可以增加假数据，比如对于图片数据，可以对其水平翻转或其他角度旋转，剪裁，扭曲，这样既节省花费又能有效预防过拟合。<br><img src="/2018/07/26/改善深层神经网络之深度学习的实用层面/dataaugmentation.png" alt="dataaugmentation"></p></li><li><p>提早停止训练神经网络<br>在梯度下降中，一般会绘制成本函数和迭代次数的曲线，在early-stopping中需要加上验证集误差Jdev，成本函数应该是单调递减，二验证集误差一般是先下降后上升，一旦验证集误差开始上升，就可以停止梯度下降，选择验证集误差最小的点，即可以认为是最优解。</p></li></ol><p>优点：只运行一次梯度下降，就能找到最合适的参数</p><p>缺点：机器学习的步骤中，其中之一是选择一个算法来优化代价函数J，如梯度下降、momentum、RMSprop、Adam等;优化代价函数之后又不想过拟合，在机器学习中已经有很多超参数要选择，要在那么多算法中选出最合适的算法也变得越来越复杂，为了能让问题变得更简单点，当我们用工具优化代价函数J的时候，只关心w和b，使得J越小越好，只要想办法减小J；而防止过拟合，用另外一套工具实现。一个时间只做一件事，这种思路称为正交化，而提前停止训练却将两件事都做了，使得需要考虑的问题变得更复杂，可能使代价函数不够小。</p><h3 id="Normalizing-Inputs"><a href="#Normalizing-Inputs" class="headerlink" title="Normalizing Inputs"></a>Normalizing Inputs</h3><p>归一化输入需要两步:</p><ol><li>均值归零： $x = x - \mu$  $\mu$是均值</li><li>归一化方差： $x = \frac{x}{\sigma ^2}$</li></ol><p><img src="/2018/07/26/改善深层神经网络之深度学习的实用层面/normlize.png" alt="normlize"></p><p><strong>为什么要归一化输入数据</strong><br>输入数据取值范围相差大，会导致对应参数w也相差较大，就必须使用一个比较小的学习率，来避免J发生震荡，这样可能需要很多次迭代才能找到最小值，均值归一化之后的在梯度下降算法中可以使用较大的步长。</p><h3 id="vanishing-exploding-gradients"><a href="#vanishing-exploding-gradients" class="headerlink" title="vanishing/exploding gradients"></a>vanishing/exploding gradients</h3><p>神经网络中，由于权重的叠加效应，激活函数以及梯度都会随着层数的增加而呈指数增长或降低，当层数较大时，激活函数或梯度就容易出现爆炸或消失的情况</p><p>可以通过对权重初始化的优化来改善，输入层神经元越多，得到的z就越大，为了防止梯度消失或爆炸，可以使其权重除以输入层神经元$n^{[l-1]}$的个数</p><p>对于ReLU：$W = W <em> \sqrt {\frac {1}{n^{[l-1]}}}$<br>对于tanh：$W = W </em> \sqrt {\frac {2}{n^{[l-1]}}}$<br>还有一种是：$W = W * \sqrt {\frac {2}{n^{[l-1]}+n^{[l]}}}$</p><h3 id="Gradient-Checking"><a href="#Gradient-Checking" class="headerlink" title="Gradient Checking"></a>Gradient Checking</h3><ol><li><p>梯度的数值逼近<br>双边误差的值近似导数值，所以可以利用这一特征检查梯度正确性</p></li><li><p>梯度检验<br>可以通过梯度检验来检查backprop的实施是否正确</p></li></ol><p>梯度检查首先要做的是分别将这些矩阵构造成一维向量，然后将这些一维向量组合起来构成一个更大的一维向量。这样cost function 就可以表示成。</p><p>然后将反向传播过程通过梯度下降算法得到的按照一样的顺序构造成一个一维向量。的维度与一致。</p><p>接着利用对每个计算近似梯度，其值与反向传播算法得到的相比较，检查是否一致。例如，对于第i个元素，近似梯度为：</p><p>计算完所有的近似梯度后，可以计算与的欧氏（Euclidean）距离来比较二者的相似度。公式如下：</p><p>一般来说，如果欧氏距离越小，例如，甚至更小，则表明与越接近，即反向梯度计算是正确的，没有bugs。如果欧氏距离较大，例如，则表明梯度计算可能出现问题，需要再次检查是否有bugs存在。如果欧氏距离很大，例如，甚至更大，则表明与差别很大，梯度下降计算过程有bugs，需要仔细检查。</p><p>3.梯度检查的使用技巧及注意事项</p><p>不要在训练过程使用梯度检查！！！因为梯度检查计算量大耗时长，所以一般只在debug的时候使用。</p><p>如果梯度检查失败，那么需要检查所有项，并试着找出bug！！ 通过比较与，找到差异较大的dθ[i]，检查其计算导数的过程是否有bug</p><p>如果梯度检查中有正则化项，一定要记得正则化项！！  即如果对成本函数进行了正则化，梯度千万不要忘记正则化项。</p><p>梯度检查不能与dropout同时使用！！</p><p>随机初始化之后进行梯度检查，反复训练网络之后，再重新进行梯度检查（不常用）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Settiing-up-your-ML-application&quot;&gt;&lt;a href=&quot;#Settiing-up-your-ML-application&quot; class=&quot;headerlink&quot; title=&quot;Settiing up your ML applicatio
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>python爬虫之模拟登陆新浪微博</title>
    <link href="http://andeper.cn/2018/07/12/python%E7%88%AC%E8%99%AB%E4%B9%8B%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E6%96%B0%E6%B5%AA%E5%BE%AE%E5%8D%9A/"/>
    <id>http://andeper.cn/2018/07/12/python爬虫之模拟登陆新浪微博/</id>
    <published>2018-07-12T08:15:22.000Z</published>
    <updated>2018-07-13T07:54:24.504Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>最近在做新浪微博爬虫这一块，准备把此项目遇到的问题写成博客的一个系列，首先遇到的问题就是新浪的登陆问题，现在大概有两种解决方案吗，第一种是使用python中的selenium模块实现模拟登陆，原理就是打开一个真正的浏览器，提取获取到的html页面中的标签实现点击，输入等事件，来实现模拟登录，这个方法的缺点就是要打开一个真正的浏览器，对资源的利用不是很有效，扩展性也不是很好，优点就是比较简单，考虑到之后要爬取微博的量级，我选择采用第二种方法，也就是抓包+模拟网络请求的方法。</p><h3 id="抓包"><a href="#抓包" class="headerlink" title="抓包"></a>抓包</h3><p>抓包就采用chrome自带的开发者工具，微博登陆界面如下图<br><img src="/2018/07/12/python爬虫之模拟登陆新浪微博/loginreview.png" alt="login"></p><p>在输入账号后回发送prelogin的请求，随意点击任意地方会出现验证码的输入框</p><p>在抓包的时候要勾选preserve log选项，防止页面跳转后log消失，抓到log后不难发现其中prelogin是登陆前比较重要的网络请求，pin.php是获取验证码的网络请求，login是真正登陆时的网络请求，如下图所示<br><img src="/2018/07/12/python爬虫之模拟登陆新浪微博/catch.png" alt="抓包"></p><p>首先分析prelogin请求，点开请求详情可以看到<br><img src="/2018/07/12/python爬虫之模拟登陆新浪微博/relogin.png" alt="prelogin"></p><p>这是一个get请求，请求参数为<br>entry: weibo<br>callback: sinaSSOController.preloginCallBack<br>su: MTUyMDA2MjEwNTk=<br>rsakt: mod<br>checkpin: 1<br>client: ssologin.js(v1.4.19)<br>_: 1531447140449  </p><p>其中只有su:MTUyMDA2MjEwNTk=和_:1531447140449,其中su是用base64加密过的用户名，很明显_是时间戳<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &apos;retcode&apos;: 0,</span><br><span class="line">    &apos;servertime&apos;: 1531448377,</span><br><span class="line">    &apos;pcid&apos;: &apos;tc-25f15c5d820bebb252b6f3ac5603e30c979d&apos;,</span><br><span class="line">    &apos;nonce&apos;: &apos;FHLJRZ&apos;,</span><br><span class="line">    &apos;pubkey&apos;: &apos;EB2A38568661887FA180BDDB5CABD5F21C7BFD59C090CB2D245A87AC253062882729293E5506350508E7F9AA3BB77F4333231490F915F6D63C55FE2F08A49B353F444AD3993CACC02DB784ABBB8E42A9B1BBFFFB38BE18D78E87A0E41B9B8F73A928EE0CCEE1F6739884B9777E4FE9E88A1BBE495927AC4A799B3181D6442443&apos;,</span><br><span class="line">    &apos;rsakv&apos;: &apos;1330428213&apos;,</span><br><span class="line">    &apos;exectime&apos;: 8</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>其中servertime，pcid，nonce，pubkey，rsakv在后面的请求都会用到。</p><p>接下来看第二个获取验证码的网络请求<br><img src="/2018/07/12/python爬虫之模拟登陆新浪微博/pin.png" alt="验证码"></p><p>请求参数只有三个r,s,p。<br>r暂时不知道，s是固定的0，p就是之前得到的pcid。<br>分析js代码可以知道<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pinCode: <span class="function"><span class="keyword">function</span>(<span class="params">obj</span>) </span>&#123;</span><br><span class="line">                <span class="keyword">var</span> codePic = sinaSSOController.getPinCodeUrl();</span><br><span class="line">                nodes.pincode.src = codePic;</span><br><span class="line">                nodes.vcode.value = <span class="string">""</span>;</span><br><span class="line">                vcodeBox.style.display = <span class="string">""</span>;</span><br><span class="line">                checkCode = <span class="literal">true</span>;</span><br><span class="line">                obj &amp;&amp; obj.reason &amp;&amp; tip.setContent(obj.reason).show();</span><br><span class="line">                loginFuns.getAllTabIndexObjects()</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>.getPinCodeUrl = <span class="function"><span class="keyword">function</span>(<span class="params">a</span>) </span>&#123;</span><br><span class="line">        a == <span class="literal">undefined</span> &amp;&amp; (a = <span class="number">0</span>);</span><br><span class="line">        pcid &amp;&amp; (me.loginExtraQuery.pcid = pcid);</span><br><span class="line">        <span class="keyword">return</span> pincodeUrl + <span class="string">"?r="</span> + <span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random() * <span class="number">1e8</span>) + <span class="string">"&amp;s="</span> + a + (pcid.length &gt; <span class="number">0</span> ? <span class="string">"&amp;p="</span> + pcid : <span class="string">""</span>)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>r就是一个随机生成的8位的随机数，但实验发现，尽管随机数相同获取到的验证码也是不同的，因此还是需要破解验证码，在目前只是接入阿里云的打码接口进行识别，以后有时间也会发布自己写的打码方法。</p><p>最后看第三个网络请求<br><img src="/2018/07/12/python爬虫之模拟登陆新浪微博/login.png" alt="login"></p><p>其中pcid，ervertime，nonce，rsakv是之前获取到的参数，door是验证码，其中su是用base64加密过的用户名，sp是base64加密之后的密码<br>返回的是一个html，用正则表达式匹配到其中的链接，然后再进行一次网络请求就可以登陆成功了。  </p><p>登陆成功之后可以干的事情就很多了</p><h3 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h3><p>Python版本：3.7<br>由于python版本的关系，，urllib的使用和python有些许区别<br>首先要做的就是设置网络请求中使用cookie<br>代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">enableCookies</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#建立一个cookies 容器</span></span><br><span class="line">        cookie_container = http.cookiejar.CookieJar()</span><br><span class="line">        <span class="comment">#将一个cookies容器和一个HTTP的cookie的处理器绑定</span></span><br><span class="line">        cookie_support = urllib.request.HTTPCookieProcessor(cookie_container)</span><br><span class="line">        <span class="comment">#创建一个opener,设置一个handler用于处理http的url打开</span></span><br><span class="line">        opener = urllib.request.build_opener(cookie_support, urllib.request.HTTPHandler)</span><br><span class="line">        <span class="comment">#安装opener，此后调用urlopen()时会使用安装过的opener对象</span></span><br><span class="line">        urllib.request.install_opener(opener)</span><br></pre></td></tr></table></figure></p><p>然后就是写加密的算法，导入一个base64的包<br>用户名的加密如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_encrypted_name</span><span class="params">(self)</span>:</span></span><br><span class="line">    username_urllike   = urllib.request.quote(self.username)</span><br><span class="line">    username_encrypted = base64.b64encode(bytes(username_urllike,encoding=<span class="string">'utf-8'</span>))</span><br><span class="line">    <span class="keyword">return</span> username_encrypted.decode(<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure></p><p>密码的加密更复杂，混合了servertime和nonce<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_encrypted_pw</span><span class="params">(self,data)</span>:</span></span><br><span class="line">    rsa_e = <span class="number">65537</span> <span class="comment">#0x10001</span></span><br><span class="line">    pw_string = str(data[<span class="string">'servertime'</span>]) + <span class="string">'\t'</span> + str(data[<span class="string">'nonce'</span>]) + <span class="string">'\n'</span> + str(self.password)</span><br><span class="line">    key = rsa.PublicKey(int(data[<span class="string">'pubkey'</span>],<span class="number">16</span>),rsa_e) <span class="comment">#创建公钥</span></span><br><span class="line">    pw_encypted = rsa.encrypt(pw_string.encode(<span class="string">'utf-8'</span>), key)  <span class="comment">#加密</span></span><br><span class="line">    self.password = <span class="string">''</span>   <span class="comment">#清空password</span></span><br><span class="line">    passwd = binascii.b2a_hex(pw_encypted) <span class="comment">#加密信息转化为16进制</span></span><br><span class="line">    print(passwd)</span><br><span class="line">    <span class="keyword">return</span> passwd</span><br></pre></td></tr></table></figure></p><p>然后进行第一个网络请求<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_prelogin_args</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment">#该函数用于模拟预登录过程,并获取服务器返回的 nonce , servertime , pub_key 等信息  </span></span><br><span class="line">    json_pattern = re.compile(<span class="string">'\((.*)\)'</span>)</span><br><span class="line">    url = <span class="string">'http://login.sina.com.cn/sso/prelogin.php?entry=weibo&amp;callback=sinaSSOController.preloginCallBack&amp;su=&amp;'</span> + self.get_encrypted_name() + <span class="string">'&amp;rsakt=mod&amp;checkpin=1&amp;client=ssologin.js(v1.4.19)'</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        request = urllib.request.Request(url)</span><br><span class="line">        response = urllib.request.urlopen(request)</span><br><span class="line">        raw_data = response.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">        json_data = json_pattern.search(raw_data).group(<span class="number">1</span>)</span><br><span class="line">        data = json.loads(json_data)</span><br><span class="line">        print(data)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line">    <span class="keyword">except</span> urllib.error <span class="keyword">as</span> e:</span><br><span class="line">        print(<span class="string">"%d"</span>%e.code)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br></pre></td></tr></table></figure></p><p>得到服务器返回的nonce , servertime , pub_key等信息</p><p>进行第二个网络请求，获取验证码，并接入识别验证码平台，得到验证码的字符串<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_door</span><span class="params">(self,pcid)</span>:</span></span><br><span class="line">    url = <span class="string">"https://login.sina.com.cn/cgi/pin.php?r="</span>+str(random.randint(<span class="number">10000000</span>,<span class="number">99999999</span>))+<span class="string">"&amp;s=0&amp;p="</span>+pcid</span><br><span class="line">    print(url)</span><br><span class="line">    r= urllib.request.Request(url)</span><br><span class="line">    res = urllib.request.urlopen(r)</span><br><span class="line">    content = res.read()</span><br><span class="line">    print(base64.b64encode(content))                 </span><br><span class="line">    host = <span class="string">'http://txyzmsb.market.alicloudapi.com'</span></span><br><span class="line">    path = <span class="string">'/yzm'</span></span><br><span class="line">    appcode = <span class="string">'你的应用密钥'</span></span><br><span class="line">    fields = urllib.parse.urlencode(&#123;</span><br><span class="line">        <span class="string">'v_pic'</span>:base64.b64encode(content),</span><br><span class="line">        <span class="string">'v_type'</span>:<span class="string">'ne5'</span></span><br><span class="line">    &#125;).encode(encoding=<span class="string">'UTF8'</span>)</span><br><span class="line">    url = host + path</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'Authorization'</span>: <span class="string">'APPCODE '</span> + appcode,</span><br><span class="line">        <span class="string">'Content-Type'</span>: <span class="string">'application/x-www-form-urlencoded; charset=UTF-8'</span></span><br><span class="line">    &#125;</span><br><span class="line">    req = urllib.request.Request(url, fields,headers)</span><br><span class="line">    f = urllib.request.urlopen(req)</span><br><span class="line">    content = f.read()</span><br><span class="line">    print(content)</span><br><span class="line">    data = json.loads(content)</span><br><span class="line">    <span class="keyword">return</span> data[<span class="string">'v_code'</span>]</span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line">进行第三个网络请求，完成登陆</span><br><span class="line">```Python</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_post_data</span><span class="params">(self,raw)</span>:</span></span><br><span class="line">    door = self.get_door(raw[<span class="string">'pcid'</span>])</span><br><span class="line">    post_data = &#123;</span><br><span class="line">        <span class="string">"entry"</span>:<span class="string">"weibo"</span>,</span><br><span class="line">        <span class="string">"gateway"</span>:<span class="string">"1"</span>,</span><br><span class="line">        <span class="string">"from"</span>:<span class="string">""</span>,</span><br><span class="line">        <span class="string">"savestate"</span>:<span class="string">"7"</span>,</span><br><span class="line">        <span class="string">"qrcode_flag"</span>:<span class="string">"false"</span>,</span><br><span class="line">        <span class="string">"useticket"</span>:<span class="string">"1"</span>,</span><br><span class="line">        <span class="string">"pagerefer"</span>:<span class="string">"https://login.sina.com.cn/crossdomain2.php?action=logout&amp;r=https%3A%2F%2Fweibo.com%2Flogout.php%3Fbackurl%3D%252F"</span>,</span><br><span class="line">        <span class="string">"pcid"</span>:raw[<span class="string">'pcid'</span>],</span><br><span class="line">        <span class="string">"door"</span>:door,</span><br><span class="line">        <span class="string">"vsnf"</span>:<span class="string">"1"</span>,</span><br><span class="line">        <span class="string">"su"</span>:self.get_encrypted_name(),</span><br><span class="line">        <span class="string">"service"</span>:<span class="string">"miniblog"</span>,</span><br><span class="line">        <span class="string">"servertime"</span>:raw[<span class="string">'servertime'</span>],</span><br><span class="line">        <span class="string">"nonce"</span>:raw[<span class="string">'nonce'</span>],</span><br><span class="line">        <span class="string">"pwencode"</span>:<span class="string">"rsa2"</span>,</span><br><span class="line">        <span class="string">"rsakv"</span>:raw[<span class="string">'rsakv'</span>],</span><br><span class="line">        <span class="string">"sp"</span>:self.get_encrypted_pw(raw),</span><br><span class="line">        <span class="string">"sr"</span>:<span class="string">"1920*1080"</span>,</span><br><span class="line">        <span class="string">"encoding"</span>:<span class="string">"UTF-8"</span>,</span><br><span class="line">        <span class="string">"prelt"</span>:<span class="string">"25"</span>,</span><br><span class="line">        <span class="string">"url"</span>:<span class="string">"https://weibo.com/ajaxlogin.php?framelogin=1&amp;callback=parent.sinaSSOController.feedBackUrlCallBack"</span>,</span><br><span class="line">        <span class="string">"returntype"</span>:<span class="string">"META"</span></span><br><span class="line">    &#125;</span><br><span class="line">    data = urllib.parse.urlencode(post_data).encode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self)</span>:</span></span><br><span class="line">    url = <span class="string">'http://login.sina.com.cn/sso/login.php?client=ssologin.js(v1.4.19)'</span></span><br><span class="line">    self.enableCookies()</span><br><span class="line">    data = self.get_prelogin_args()</span><br><span class="line">    post_data = self.build_post_data(data)</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"User-Agent"</span>:<span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36"</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        request = urllib.request.Request(url=url,data=post_data,headers=headers)</span><br><span class="line">        response = urllib.request.urlopen(request)</span><br><span class="line">        html = response.read().decode(<span class="string">'GBK'</span>)</span><br><span class="line">        print(html)</span><br><span class="line">    <span class="keyword">except</span> urllib.error <span class="keyword">as</span> e:</span><br><span class="line">        print(e.code)</span><br><span class="line"></span><br><span class="line">    p = re.compile(<span class="string">'location\.replace\(\'(.*?)\'\)'</span>)</span><br><span class="line">    p2 = re.compile(<span class="string">r'"userdomain":"(.*?)"'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        login_url = p.search(html).group(<span class="number">1</span>)</span><br><span class="line">        print(login_url)</span><br><span class="line">        request = urllib.request.Request(login_url)</span><br><span class="line">        response = urllib.request.urlopen(request)</span><br><span class="line">        page = response.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">        print(page)</span><br><span class="line">        login_url = <span class="string">'http://weibo.com/'</span> + p2.search(page).group(<span class="number">1</span>)</span><br><span class="line">        request = urllib.request.Request(login_url)</span><br><span class="line">        response = urllib.request.urlopen(request)</span><br><span class="line">        final = response.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">        print(<span class="string">"Login success!"</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">'Login error!'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h3&gt;&lt;p&gt;最近在做新浪微博爬虫这一块，准备把此项目遇到的问题写成博客的一个系列，首先遇到的问题就是新浪的登陆问题，现在大概有两种解决方案吗，第一种是使
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>深层神经网络</title>
    <link href="http://andeper.cn/2018/02/21/%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://andeper.cn/2018/02/21/深层神经网络/</id>
    <published>2018-02-21T10:20:38.000Z</published>
    <updated>2018-02-21T12:39:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>上节课我们主要介绍了浅层神经网络。首先介绍神经网络的基本结构，包括输入层，隐藏层和输出层。然后以简单的2 layer NN为例，详细推导了其正向传播过程和反向传播过程，使用梯度下降的方法优化神经网络参数。同时，我们还介绍了不同的激活函数，比较各自优缺点，讨论了激活函数必须是非线性的原因。最后介绍了神经网络参数随机初始化的必要性，特别是权重W，不同神经元的W不能初始化为同一零值。本节课是对上节课的延伸和扩展，讨论更深层的神经网络。</p><h3 id="Deep-L-layer-neural-network"><a href="#Deep-L-layer-neural-network" class="headerlink" title="Deep L-layer neural network"></a>Deep L-layer neural network</h3><p>深层神经网络其实就是包含更多的隐藏层神经网络。如下图所示，分别列举了逻辑回归、1个隐藏层的神经网络、2个隐藏层的神经网络和5个隐藏层的神经网络它们的模型结构。<br><img src="/2018/02/21/深层神经网络/deepLNN" alt="deepLNN"></p><p>命名规则上，一般只参考隐藏层个数和输出层。例如，上图中的逻辑回归又叫1 layer NN，1个隐藏层的神经网络叫做2 layer NN，2个隐藏层的神经网络叫做3 layer NN，以此类推。如果是L-layer NN，则包含了L-1个隐藏层，最后的L层是输出层。</p><p>下面以一个4层神经网络为例来介绍关于神经网络的一些标记写法。如下图所示，首先，总层数用L表示，L=4。输入层是第0层，输出层是第L层。$n^{[l]}$表示第l层包含的单元个数，l=0,1,⋯,L。这个模型中，$n^{[0]}=n_x=3$，表示三个输入特征$x_1,x_2,x_3$。$n^{[1]}=5$，$n^{[2]}=5$，$n^{[3]}=3$，$n^{[4]}=n^{[L]}=1$。第l层的激活函数输出用$a^{[l]}$表示，$a^{[l]}=g^{[l]}(z^{[l]})$。$W^{[l]}$表示第l层的权重，用于计算$z^{[l]}$。另外，我们把输入x记为$a^{[0]}$，把输出层$\hat{y}$记为$a^{[L]}$。</p><p>注意，$a^{[l]}$和$W^{[l]}$中的上标l都是从1开始的，l=1,⋯,L。<br><img src="/2018/02/21/深层神经网络/DNNN" alt="DNNN"></p><h3 id="Forward-Propagation-in-a-Deep-Network"><a href="#Forward-Propagation-in-a-Deep-Network" class="headerlink" title="Forward Propagation in a Deep Network"></a>Forward Propagation in a Deep Network</h3><p>接下来，我们来推导一下深层神经网络的正向传播过程。仍以上面讲过的4层神经网络为例，对于单个样本：</p><p>第1层，l=1：</p><script type="math/tex; mode=display">z^{[1]}=W^{[1]}x+b^{[1]}=W^{[1]}a^{[0]}+b^{[1]}</script><script type="math/tex; mode=display">a^{[1]}=g^{[1]}(z^{[1]})</script><p>第2层，l=2：</p><script type="math/tex; mode=display">z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}</script><script type="math/tex; mode=display">a^{[2]}=g^{[2]}(z^{[2]})</script><p>第3层，l=3：</p><script type="math/tex; mode=display">z^{[3]}=W^{[3]}a^{[2]}+b^{[3]}</script><script type="math/tex; mode=display">a^{[3]}=g^{[3]}(z^{[3]})</script><p>第4层，l=4：</p><script type="math/tex; mode=display">z^{[4]}=W^{[4]}a^{[3]}+b^{[4]}</script><script type="math/tex; mode=display">a^{[4]}=g^{[4]}(z^{[4]})</script><p>如果有m个训练样本，其向量化矩阵形式为：</p><p>第1层，l=1：</p><script type="math/tex; mode=display">Z^{[1]}=W^{[1]}X+b^{[1]}=W^{[1]}A^{[0]}+b^{[1]}</script><script type="math/tex; mode=display">A^{[1]}=g^{[1]}(Z^{[1]})</script><p>第2层，l=2：</p><script type="math/tex; mode=display">Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}</script><script type="math/tex; mode=display">A^{[2]}=g^{[2]}(Z^{[2]})</script><p>第3层，l=3：</p><script type="math/tex; mode=display">Z^{[3]}=W^{[3]}A^{[2]}+b^{[3]}</script><script type="math/tex; mode=display">A^{[3]}=g^{[3]}(Z^{[3]})</script><p>第4层，l=4：</p><script type="math/tex; mode=display">Z^{[4]}=W^{[4]}A^{[3]}+b^{[4]}</script><script type="math/tex; mode=display">A^{[4]}=g^{[4]}(Z^{[4]})</script><p>综上所述，对于第l层，其正向传播过程的Z[l]和A[l]可以表示为：</p><script type="math/tex; mode=display">Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}</script><script type="math/tex; mode=display">A^{[l]}=g^{[l]}(Z^{[l]})</script><p>其中l=1,⋯,L</p><h3 id="Getting-your-matrix-dimensions-right"><a href="#Getting-your-matrix-dimensions-right" class="headerlink" title="Getting your matrix dimensions right"></a>Getting your matrix dimensions right</h3><p>对于单个训练样本，输入x的维度是$(n^{[0]},1)$,神经网络的参数$W^{[l]}$和$b^{[l]}$的维度分别是：</p><script type="math/tex; mode=display">W^{[l]}:\ (n^{[l]},n^{[l-1]})</script><script type="math/tex; mode=display">b^{[l]}:\ (n^{[l]},1)</script><p>其中，l=1,⋯,L，$n^{[l]}$和$n^{[l−1]}$分别表示第l层和l−1层的所含单元个数。$n^{[0]}=n_x$，表示输入层特征数目。</p><p>顺便提一下，反向传播过程中的$dW^{[l]}$和$db^{[l]}$的维度分别是：</p><script type="math/tex; mode=display">dW^{[l]}:\ (n^{[l]},n^{[l-1]})</script><script type="math/tex; mode=display">db^{[l]}:\ (n^{[l]},1)</script><p>注意到，$W^{[l]}$与$dW^{[l]}$维度相同，$b^{[l]}$与$db^{[l]}$维度相同。这很容易理解。</p><p>正向传播过程中的$z^{[l]}$和$a^{[l]}$的维度分别是：</p><script type="math/tex; mode=display">z^{[l]}:\ (n^{[l]},1)</script><script type="math/tex; mode=display">a^{[l]}:\ (n^{[l]},1)</script><p>$z^{[l]}$和$a^{[l]}$的维度是一样的，且$dz^{[l]}$和$da^{[l]}$的维度均与$z^{[l]}$和$a^{[l]}$的维度一致。</p><p>对于m个训练样本，输入矩阵X的维度是$(n^{[0]},m)$。需要注意的是$W^{[l]}$和$b^{[l]}$的维度与只有单个样本是一致的：</p><script type="math/tex; mode=display">W^{[l]}:\ (n^{[l]},n^{[l-1]})</script><script type="math/tex; mode=display">b^{[l]}:\ (n^{[l]},1)</script><p>只不过在运算$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$中，$b^{[l]}$会被当成$(n^{[l]},m)$矩阵进行运算，这是因为python的广播性质，且$b^{[l]}$每一列向量都是一样的。$dW^{[l]}$和$db^{[l]}$的维度分别与$W^{[l]}$和$b^{[l]}$的相同。</p><p>但是，$Z^{[l]}$和$A^{[l]}$的维度发生了变化：</p><script type="math/tex; mode=display">Z^{[l]}:\ (n^{[l]},m)</script><script type="math/tex; mode=display">A^{[l]}:\ (n^{[l]},m)</script><p>$dZ^{[l]}$和$dA^{[l]}$的维度分别与$Z^{[l]}$和$A^{[l]}$的相同。</p><h3 id="Why-deep-representations"><a href="#Why-deep-representations" class="headerlink" title="Why deep representations?"></a>Why deep representations?</h3><p>我们都知道神经网络能处理很多问题，而且效果显著。其强大能力主要源自神经网络足够“深”，也就是说网络层数越多，神经网络就更加复杂和深入，学习也更加准确。接下来，我们从几个例子入手，看一下为什么深度网络能够如此强大。</p><p>先来看人脸识别的例子，如下图所示。经过训练，神经网络第一层所做的事就是从原始图片中提取出人脸的轮廓与边缘，即边缘检测。这样每个神经元得到的是一些边缘信息。神经网络第二层所做的事情就是将前一层的边缘进行组合，组合成人脸一些局部特征，比如眼睛、鼻子、嘴巴等。再往后面，就将这些局部特征组合起来，融合成人脸的模样。可以看出，随着层数由浅到深，神经网络提取的特征也是从边缘到局部特征到整体，由简单到复杂。可见，如果隐藏层足够多，那么能够提取的特征就越丰富、越复杂，模型的准确率就会越高。</p><p>语音识别模型也是这个道理。浅层的神经元能够检测一些简单的音调，然后较深的神经元能够检测出基本的音素，更深的神经元就能够检测出单词信息。如果网络够深，还能对短语、句子进行检测。记住一点，神经网络从左到右，神经元提取的特征从简单到复杂。特征复杂度与神经网络层数成正相关。特征越来越复杂，功能也越来越强大。<br><img src="/2018/02/21/深层神经网络/intutition" alt="intutition"></p><p>除了从提取特征复杂度的角度来说明深层网络的优势之外，深层网络还有另外一个优点，就是能够减少神经元个数，从而减少计算量。例如下面这个例子，使用电路理论，计算逻辑输出：</p><script type="math/tex; mode=display">y=x_1\oplus x_2\oplus x_3\oplus\cdots\oplus x_n</script><p>其中，$\oplus$表示异或操作。对于这个逻辑运算，如果使用深度网络，深度网络的结构是每层将前一层的两两单元进行异或，最后到一个输出，如下图左边所示。这样，整个深度网络的层数是$log_2(n)$，不包含输入层。总共使用的神经元个数为：</p><script type="math/tex; mode=display">1+2+\cdots+2^{log_2(n)-1}=1\cdot\frac{1-2^{log_2(n)}}{1-2}=2^{log_2(n)}-1=n-1</script><p>可见，输入个数是n，这种深层网络所需的神经元个数仅仅是n-1个。</p><p>如果不用深层网络，仅仅使用单个隐藏层，那么需要的神经元个数将是指数级别那么大。Andrew指出，由于包含了所有的逻辑位（0和1），则需要$2^{n-1}$ ?个神经元。</p><p>比较下来，处理同一逻辑问题，深层网络所需的神经元个数比浅层网络要少很多。这也是深层神经网络的优点之一。</p><p>尽管深度学习有着非常显著的优势，Andrew还是建议对实际问题进行建模时，尽量先选择层数少的神经网络模型，这也符合奥卡姆剃刀定律（Occam’s Razor）。对于比较复杂的问题，再使用较深的神经网络模型。</p><h3 id="Building-blocks-of-deep-neural-networks"><a href="#Building-blocks-of-deep-neural-networks" class="headerlink" title="Building blocks of deep neural networks"></a>Building blocks of deep neural networks</h3><p>下面用流程块图来解释神经网络正向传播和反向传播过程。如下图所示，对于第l层来说，正向传播过程中：</p><p>输入：$a^{[l-1]}$<br>输出：$a^{[l]}$<br>参数：$W^{[l]}$,$b^{[l]}$<br>缓存变量：$z^{[l]}$</p><p>反向传播过程中：<br>输入：$da^{[l]}$<br>输出：$da^{[l-1]}$,$dW^{[l]}$,$db^{[l]}$<br>参数：$W^{[l]}$,$b^{[l]}$<br><img src="/2018/02/21/深层神经网络/Ilayer" alt="Ilayer"></p><p>刚才这是第l层的流程块图，对于神经网络所有层，整体的流程块图正向传播过程和反向传播过程如下所示：<br><img src="/2018/02/21/深层神经网络/Ilayer2" alt="Ilayer2"></p><h3 id="Forward-and-Backward-Propagation"><a href="#Forward-and-Backward-Propagation" class="headerlink" title="Forward and Backward Propagation"></a>Forward and Backward Propagation</h3><p>我们继续接着上一部分流程块图的内容，推导神经网络正向传播过程和反向传播过程的具体表达式。</p><p>首先是正向传播过程，令层数为第l层，输入是$a^{[l-1]}$，输出是$a^{[l]}$，缓存变量是$z^{[l]}$。其表达式如下：</p><script type="math/tex; mode=display">z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}</script><script type="math/tex; mode=display">a^{[l]}=g^{[l]}(z^{[l]})</script><p>m个训练样本，向量化形式为：</p><script type="math/tex; mode=display">Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}</script><script type="math/tex; mode=display">A^{[l]}=g^{[l]}(Z^{[l]})</script><p>然后是反向传播过程，输入是$da^{[l]}$，输出是$da^{[l-1]}$,$dw^{[l]}$,$db^{[l]}$。其表达式如下：</p><script type="math/tex; mode=display">dz^{[l]}=da^{[l]}\ast g^{[l]'}(z^{[l]})</script><script type="math/tex; mode=display">dW^{[l]}=dz^{[l]}\cdot a^{[l-1]}</script><script type="math/tex; mode=display">db^{[l]}=dz^{[l]}</script><script type="math/tex; mode=display">da^{[l-1]}=W^{[l]T}\cdot dz^{[l]}</script><p>由上述第四个表达式可得$da^{[l]}=W^{[l+1]T}\cdot dz^{[l+1]}$，将$da^{[l]}$代入第一个表达式中可以得到：</p><script type="math/tex; mode=display">dz^{[l]}=W^{[l+1]T}\cdot dz^{[l+1]}\ast g^{[l]'}(z^{[l]})</script><p>该式非常重要，反映了$dz^{[l+1]}$与<script type="math/tex">dz^{[l]}</script>的递推关系。</p><p>m个训练样本，向量化形式为：</p><script type="math/tex; mode=display">dZ^{[l]}=dA^{[l]}\ast g^{[l]'}(Z^{[l]})</script><script type="math/tex; mode=display">dW^{[l]}=\frac1mdZ^{[l]}\cdot A^{[l-1]T}</script><script type="math/tex; mode=display">db^{[l]}=\frac1mnp.sum(dZ^{[l]},axis=1,keepdim=True)</script><script type="math/tex; mode=display">dA^{[l-1]}=W^{[l]T}\cdot dZ^{[l]}</script><script type="math/tex; mode=display">dZ^{[l]}=W^{[l+1]T}\cdot dZ^{[l+1]}\ast g^{[l]'}(Z^{[l]})</script><h3 id="Parameters-vs-Hyperparameters"><a href="#Parameters-vs-Hyperparameters" class="headerlink" title="Parameters vs Hyperparameters"></a>Parameters vs Hyperparameters</h3><p>该部分介绍神经网络中的参数（parameters）和超参数（hyperparameters）的概念。</p><p>神经网络中的参数就是我们熟悉的$W^{[l]}$和$b^{[l]}$。而超参数则是例如学习速率$\alpha$，训练迭代次数N，神经网络层数L，各层神经元个数$n^{[l]}$，激活函数g(z)等。之所以叫做超参数的原因是它们决定了参数$W^{[l]}$和$b^{[l]}$的值。在后面的第二门课我们还将学习其它的超参数，这里先不讨论。</p><p>如何设置最优的超参数是一个比较困难的、需要经验知识的问题。通常的做法是选择超参数一定范围内的值，分别代入神经网络进行训练，测试cost function随着迭代次数增加的变化，根据结果选择cost function最小时对应的超参数值。这类似于validation的方法。</p><h3 id="What-does-this-have-to-do-with-the-brain"><a href="#What-does-this-have-to-do-with-the-brain" class="headerlink" title="What does this have to do with the brain?"></a>What does this have to do with the brain?</h3><p>那么，神经网络跟人脑机制到底有什么联系呢？究竟有多少的相似程度？神经网络实际上可以分成两个部分：正向传播过程和反向传播过程。神经网络的每个神经元采用激活函数的方式，类似于感知机模型。这种模型与人脑神经元是类似的，可以说是一种非常简化的人脑神经元模型。如下图所示，人脑神经元可分为树突、细胞体、轴突三部分。树突接收外界电刺激信号（类比神经网络中神经元输入），传递给细胞体进行处理（类比神经网络中神经元激活函数运算），最后由轴突传递给下一个神经元（类比神经网络中神经元输出）。</p><p>值得一提的是，人脑神经元的结构和处理方式要复杂的多，神经网络模型只是非常简化的模型。人脑如何进行学习？是否也是通过反向传播和梯度下降算法现在还不清楚，可能会更加复杂。这是值得生物学家探索的事情。也许发现重要的新的人脑学习机制后，让我们的神经网络模型抛弃反向传播和梯度下降算法，能够实现更加准确和强大的神经网络模型！</p><p><img src="/2018/02/21/深层神经网络/propagation" alt="propagation"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了深层神经网络，是上一节浅层神经网络的拓展和归纳。首先，我们介绍了建立神经网络模型一些常用的标准的标记符号。然后，用流程块图的方式详细推导正向传播过程和反向传播过程的输入输出和参数表达式。我们也从提取特征复杂性和计算量的角度分别解释了深层神经网络为什么优于浅层神经网络。接着，我们介绍了超参数的概念，解释了超参数与参数的区别。最后，我们将神经网络与人脑做了类别，人工神经网络是简化的人脑模型。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;上节课我们主要介绍了浅层神经网络。首先介绍神经网络的基本结构，包括输入层，隐藏层和输出层。然后以简单的2 layer NN为例，详细推导了其正向传播过程和反向传播过程，使用梯度下降的方法优化神经网络参数。同时，我们还介绍了不同的激活函数，比较各自优缺点，讨论了激活函数必须是
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>浅层神经网络</title>
    <link href="http://andeper.cn/2018/02/19/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://andeper.cn/2018/02/19/浅层神经网络/</id>
    <published>2018-02-19T12:37:59.000Z</published>
    <updated>2018-02-23T07:18:58.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Neural-Network-Overview"><a href="#Neural-Network-Overview" class="headerlink" title="Neural Network Overview"></a>Neural Network Overview</h3><p>前面的课程中，我们已经使用计算图的方式介绍了逻辑回归梯度下降算法的正向传播和反向传播。神经网络的结构与逻辑回归类似，只是神经网络的层数比逻辑回归多一层，多出来的中间那层称为隐藏层或中间层。这样从计算上来说，神经网络的正向传播和反向传播过程只是比逻辑回归多了一次重复的计算。正向传播过程分为两层，第一层是出入层到隐藏层，用上标[1]来表示：</p><script type="math/tex; mode=display">z^{[1]}=W^{[1]}x + b^{[1]}</script><script type="math/tex; mode=display">a^{[1]}=\sigma(z^{[1]})</script><p>第二层是隐藏层到输出层，用上标[2]来表示：</p><script type="math/tex; mode=display">z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}</script><script type="math/tex; mode=display">a^{[2]} = \sigma(z^{[2]})</script><p>在写法上值得注意的是，方括号上标[i]表示当前所处的层数；圆括号上标(i)表示第i个样本。</p><p>同样，反向传播过程也分成两层。第一层是输出层到隐藏层，第二层是隐藏层到输入层。其细节部分我们之后再来讨论。<br><img src="/2018/02/19/浅层神经网络/neuralnetwork" alt="neuralnetwork"></p><h3 id="Neural-Network-Representation"><a href="#Neural-Network-Representation" class="headerlink" title="Neural Network Representation"></a>Neural Network Representation</h3><p>下面我们以图示的方式来介绍单隐藏层的神经网络结构。如下图所示，单隐藏神经网络就是典型的浅层(shallow)神经网络<br><img src="/2018/02/19/浅层神经网络/shallowneuralnetwork" alt="shallowneuralnetwork"><br>结构上，从左到右，可以分为三层：输入层(Input layer),隐藏层(Hidden layer)和输出层(Output layer)。输入层和输出层，顾名思义，对应训练样本的输入和输出，隐藏层是抽象的非线性的中间层，在训练集中，这些中间节点的真正数值我们是不知道的，我们在训练集中看不到它们的数值，这也是被命名为隐藏层的原因。<br>在写法上，我们通常吧输入矩阵X记为$a^{[0]}$,把隐藏层输出记为$a^{[1]}$,上标从0开始。用下标表示第几个神经元，注意下标从1开始。例如$a_1^{[1]}$表示隐藏层第1个神经元，$a_2^{[1]}$表示隐藏层第二个神经元，等等。这样，隐藏层有4个神经元就可以将其输出$a^{[1]}$写成矩阵的形式：</p><script type="math/tex; mode=display">{a^{[1]}}= \left[ \begin{matrix} a_1^{[1]} \\ a_2^{[1]} \\ a_3^{[1]} \\ a_4^{[1]} \end{matrix} \right]</script><p>最后相应的输出层记为$a^{[2]}$,即$\hat y$。这种单隐藏层神经网络也被称为两层神经网络(2 layer NN)。之所以叫两层神经网络是因为，通常我们只会计算隐藏层输出和输出层的输出，输入层是不用计算的。这也是我们把输入层层数上标记为0的原因(a^{[0]})</p><p>关于隐藏层对应的权重$W^{[1]}$和常数项$b^{[1]}$,$W^{[1]}$的维度是(4,3)。这里的4对应着隐藏层神经元个数，3对应这输入层x特征向量包含元素个数。常数项$b^{[1]}$的维度是(4,1),这里的4同样对应着隐藏层神经元个数。关于输出层对应的权重$W^{[2]}$和常数项$b^{[2]}$，$W^{[2]}$的维度是(1,4),这里的1对应着输出层神经元个数，4对应着隐藏层神经元个数。常数项$b^{[2]}$的维度是(1,1),因为输出只有一个神经元。总结一下，第i层的权重$W^{[i]}$维度的行等于i层神经元的个数，列等于i-1层神经元的个数；第i层常数项$b^{[i]}$维度的行等于i层神经元的个数，列始终为1。</p><h3 id="Computing-a-Neural-Network’s-Output"><a href="#Computing-a-Neural-Network’s-Output" class="headerlink" title="Computing a Neural Network’s Output"></a>Computing a Neural Network’s Output</h3><p>接下来我们详细推导神经网络的计算过程。回顾一下，我们前面讲过两层神经网络可以看成是逻辑回归再重复计算一次。如下图所示，逻辑回归的正向计算可以分解成z和a的两部分：</p><script type="math/tex; mode=display">z=w^Tx+b</script><script type="math/tex; mode=display">a= \sigma(z)</script><p><img src="/2018/02/19/浅层神经网络/ComputingNNoutput" alt="ComputingNNoutput"></p><p>对于两层神经网络，从输入层到隐藏层对应一次逻辑回归运算；从隐藏层到输出层对应一次逻辑回归运算。每层计算时，要主义对应的上标和下标，一般我们记上标方括号表示layer，下标表示第几个神经元。例如$a_i^{[I]}$表示第I层的第i个神经元，注意，i从1开始，I从0开始。</p><p>下面，我们将从输入层到输出层的计算公式列出来</p><script type="math/tex; mode=display">z_1^{[1]}=w_1^{[1]T}x+b_1^{[1]},\ a_1^{[1]}=\sigma(z_1^{[1]})</script><script type="math/tex; mode=display">z_2^{[1]}=w_2^{[1]T}x+b_2^{[1]},\ a_2^{[1]}=\sigma(z_2^{[1]})</script><script type="math/tex; mode=display">z_3^{[1]}=w_3^{[1]T}x+b_3^{[1]},\ a_3^{[1]}=\sigma(z_3^{[1]})</script><script type="math/tex; mode=display">z_4^{[1]}=w_4^{[1]T}x+b_4^{[1]},\ a_4^{[1]}=\sigma(z_4^{[1]})</script><p>然后，从隐藏层到输出层的计算公式为：</p><script type="math/tex; mode=display">z_1^{[2]}=w_1^{[2]T}a^{[1]}+b_1^{[2]},\ a_1^{[2]}=\sigma(z_1^{[2]})</script><p>其中$a^{[1]}$为：</p><script type="math/tex; mode=display">{a^{[1]}}= \left[ \begin{matrix} a_1^{[1]} \\ a_2^{[1]} \\ a_3^{[1]} \\ a_4^{[1]} \end{matrix} \right]</script><p>上述每个节点的计算都对应着一次逻辑运算的过程，分别由计算z和a两部分组成。</p><p>为了提高程序的运算速度，我们引入向量化和矩阵运算的思想，将上述表达式转换成矩阵运算的形式：</p><script type="math/tex; mode=display">z^{[1]}=W^{[1]}x+b^{[1]}</script><script type="math/tex; mode=display">a^{[1]}=\sigma(z^{[1]})</script><script type="math/tex; mode=display">z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}</script><script type="math/tex; mode=display">a^{[2]}=\sigma(z^{[2]})</script><p><img src="/2018/02/19/浅层神经网络/ComputingNNoutput2" alt="ComputingNNoutput2"><br>之前也介绍过，$W^{[1]}$的维度是(4,3),$b^{[1]}$的维度是(4,1),$W^{[2]}$的维度是(1,4),$b^{[2]}$的维度是(1,1)。</p><h3 id="Vectorizing-across-multiple-examples"><a href="#Vectorizing-across-multiple-examples" class="headerlink" title="Vectorizing across multiple examples"></a>Vectorizing across multiple examples</h3><p>上一部分我们介绍了单个样本的神经网络正向传播矩阵运算过程。而对于m个训练样本，我们也可以使用矩阵相乘的形式来提高计算效率。而且它的形式与上一部分单个样本的矩阵运算十分相似。</p><p>用上标(i)来表示第i个样本，例如$X^{(i)},Z^{(i)},a^{<a href="i">2</a>}$。对于每个样本i，可以使用for循环来求解其正向输出。</p><p>for i=1 to m:<br>    $z^{<a href="i">1</a>}=W^{[1]}x^{(i)}+b^{[1]}$<br>    $a^{<a href="i">1</a>}=\sigma(z^{<a href="i">1</a>})$<br>    $z^{<a href="i">2</a>}=W^{[2]}a^{<a href="i">1</a>}+b^{[2]}$<br>    $a^{<a href="i">2</a>}=\sigma(z^{<a href="i">2</a>})$</p><p>不使用for循环，利用矩阵运算的思想，输入矩阵X的维度为(n_x,m)。这样，我们可以把上面的for循环写成矩阵运算的形式：</p><script type="math/tex; mode=display">Z^{[1]}=W^{[1]}X+b^{[1]}</script><script type="math/tex; mode=display">A^{[1]}=\sigma(Z^{[1]})</script><script type="math/tex; mode=display">Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}</script><script type="math/tex; mode=display">A^{[2]}=\sigma(Z^{[2]})</script><p>其中，$Z^{[1]}$的维度是（4,m），4是隐藏层神经元的个数；$A^{[1]}$的维度与$Z^{[1]}$相同；$Z^{[2]}$和$A^{[2]}$的维度均为（1,m）。对上面这四个矩阵来说，均可以这样来理解：行表示神经元个数，列表示样本数目m。</p><h3 id="Explanation-for-Vectorized-Implementation"><a href="#Explanation-for-Vectorized-Implementation" class="headerlink" title="Explanation for Vectorized Implementation"></a>Explanation for Vectorized Implementation</h3><p>只要记住上述四个矩阵的行表示神经元个数，列表示样本数目m就行了。</p><h3 id="Activation-functions"><a href="#Activation-functions" class="headerlink" title="Activation functions"></a>Activation functions</h3><p>神经网络隐藏层和输出层都需要激活函数（activation function），在之前的课程中我们都默认使用Sigmoid函数$\sigma(X)$作为激活函数。其实，还有其它激活函数可供使用，不同的激活函数有各自的优点。下面我们就来介绍几个不同的激活函数g(x)。</p><p><strong>sigmoid函数</strong><br><img src="/2018/02/19/浅层神经网络/sigmoid" alt="sigmoid"></p><p><strong>tanh函数</strong><br><img src="/2018/02/19/浅层神经网络/tanh" alt="tanh"></p><p><strong>ReLU函数</strong><br><img src="/2018/02/19/浅层神经网络/ReLU" alt="ReLU"></p><p><strong>Leaky ReLU</strong><br><img src="/2018/02/19/浅层神经网络/LeakyReLU" alt="LeakyReLU"></p><p>如上图所示，不同激活函数形状不同，a的取值范围也有差异。</p><p>如何选择合适的激活函数呢？首先我们来比较sigmoid函数和tanh函数。对于隐藏层的激活函数，一般来说，tanh函数要比sigmoid函数表现更好一些。因为tanh函数的取值范围在[-1,+1]之间，隐藏层的输出被限定在[-1,+1]之间，可以看成是在0值附近分布，均值为0。这样从隐藏层到输出层，数据起到了归一化（均值为0）的效果。因此，隐藏层的激活函数，tanh比sigmoid更好一些。而对于输出层的激活函数，因为二分类问题的输出取值为{0,+1}，所以一般会选择sigmoid作为激活函数。</p><p>观察sigmoid函数和tanh函数，我们发现有这样一个问题，就是当|z|很大的时候，激活函数的斜率（梯度）很小。因此，在这个区域内，梯度下降算法会运行得比较慢。在实际应用中，应尽量避免使z落在这个区域，使|z|尽可能限定在零值附近，从而提高梯度下降算法运算速度。</p><p>为了弥补sigmoid函数和tanh函数的这个缺陷，就出现了ReLU激活函数。ReLU激活函数在z大于零时梯度始终为1；在z小于零时梯度始终为0；z等于零时的梯度可以当成1也可以当成0，实际应用中并不影响。对于隐藏层，选择ReLU作为激活函数能够保证z大于零时梯度始终为1，从而提高神经网络梯度下降算法运算速度。但当z小于零时，存在梯度为0的缺点，实际应用中，这个缺点影响不是很大。为了弥补这个缺点，出现了Leaky ReLU激活函数，能够保证z小于零是梯度不为0。</p><p>最后总结一下，如果是分类问题，输出层的激活函数一般会选择sigmoid函数。但是隐藏层的激活函数通常不会选择sigmoid函数，tanh函数的表现会比sigmoid函数好一些。实际应用中，通常会会选择使用ReLU或者Leaky ReLU函数，保证梯度下降速度不会太小。其实，具体选择哪个函数作为激活函数没有一个固定的准确的答案，应该要根据具体实际问题进行验证（validation）。</p><h3 id="Why-do-you-need-non-linear-activation-functions"><a href="#Why-do-you-need-non-linear-activation-functions" class="headerlink" title="Why do you need non-linear activation functions"></a>Why do you need non-linear activation functions</h3><p>为什么不能用线性激活函数</p><p>假设所有激活函数都是线性的，为了简化计算，我们直接令激活函数g(z)=z,即a=z。那么，浅层神经网络的各层输出为：</p><script type="math/tex; mode=display">z^{[1]}=W^{[1]}x+b^{[1]}</script><script type="math/tex; mode=display">a^{[1]}=z^{[1]}</script><script type="math/tex; mode=display">z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}</script><script type="math/tex; mode=display">a^{[2]}=z^{[2]}</script><p>我们对上式中$a^{[2]}$进行化简计算：</p><script type="math/tex; mode=display">a^{[2]}=z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}=W^{[2]}(W^{[1]}x+b^{[1]})+b^{[2]}=(W^{[2]}W^{[1]})x+(W^{[2]}b^{[1]}+b^{[2]})=W'x+b'</script><p>经过推导我们发现$a^{[2]}$仍是输入变量x的线性组合。这表明，使用神经网络与直接使用线性模型的效果并没有什么两样。即便是包含多层隐藏层的神经网络，如果使用线性函数作为激活函数，最终的输出仍然是输入x的线性模型。这样的话神经网络就没有任何作用了。因此，隐藏层的激活函数必须要是非线性的。</p><p>另外，如果所有的隐藏层全部使用线性激活函数，只有输出层使用非线性激活函数，那么整个神经网络的结构就类似于一个简单的逻辑回归模型，而失去了神经网络模型本身的优势和价值。</p><p>值得一提的是，如果是预测问题而不是分类问题，输出y是连续的情况下，输出层的激活函数可以使用线性函数。如果输出y恒为正值，则也可以使用ReLU激活函数，具体情况，具体分析。</p><h3 id="Derivatives-of-activation-functions"><a href="#Derivatives-of-activation-functions" class="headerlink" title="Derivatives of activation functions"></a>Derivatives of activation functions</h3><p>在梯度下降反向计算过程中少不了计算激活函数的导数即梯度。</p><p>我们先来看一下sigmoid函数的导数：</p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{(-z)}}</script><script type="math/tex; mode=display">g'(z)=\frac{d}{dz}g(z)=g(z)(1-g(z))=a(1-a)</script><p>tanh函数的导数：</p><script type="math/tex; mode=display">g(z)=\frac{e^{(z)}-e^{(-z)}}{e^{(z)}+e^{(-z)}}</script><script type="math/tex; mode=display">g'(z)=\frac{d}{dz}g(z)=1-(g(z))^2=1-a^2</script><p>ReLU函数的导数</p><script type="math/tex; mode=display">g(z)=max(0,z)</script><script type="math/tex; mode=display">g'(z)=\begin{cases} 0, & z<0\\ 1, & z\geq0 \end{cases}</script><p>LeakyReLU函数的导数</p><script type="math/tex; mode=display">g(z)=max(0.01z,z)</script><script type="math/tex; mode=display">g'(z)=\begin{cases} 0.01, & z<0\\ 1, & z\geq0 \end{cases}</script><h3 id="Gradient-descent-for-neural-networks"><a href="#Gradient-descent-for-neural-networks" class="headerlink" title="Gradient descent for neural networks"></a>Gradient descent for neural networks</h3><p>在神经网络中进行梯度计算</p><p>仍然是浅层神经网络，包含的参数为$W^{[1]}$，$b^{[1]}$，$W^{[2]}$，$b^{[2]}$。令输入层的特征向量个数$n_x=n^{[0]}$，隐藏层神经元个数为$n^{[1]}$，输出层神经元个数为$n^{[2]}=1$。则$W^{[1]}$的维度为$(n^{[1]},n^{[0]})$，$b^{[1]}$的维度为$(n^{[1]},1)$，$W^{[2]}$的维度为$(n^{[2]},n^{[1]})$，b[2]的维度为$(n^{[2]},1)$。</p><p>该神经网络正向传播过程为：</p><script type="math/tex; mode=display">Z^{[1]}=W^{[1]}X+b^{[1]}</script><script type="math/tex; mode=display">A^{[1]}=g(Z^{[1]})</script><script type="math/tex; mode=display">Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}</script><script type="math/tex; mode=display">A^{[2]}=g(Z^{[2]})</script><p>其中，g(⋅)表示激活函数。</p><p>反向传播是计算导数（梯度）的过程，这里先列出来Cost function对各个参数的梯度：</p><script type="math/tex; mode=display">dZ^{[2]}=A^{[2]}-Y</script><script type="math/tex; mode=display">dW^{[2]}=\frac1mdZ^{[2]}A^{[1]T}</script><script type="math/tex; mode=display">db^{[2]}=\frac1mnp.sum(dZ^{[2]},axis=1,keepdim=True)</script><script type="math/tex; mode=display">dZ^{[1]}=W^{[2]T}dZ^{[2]}\ast g'(Z^{[1]})</script><script type="math/tex; mode=display">dW^{[1]}=\frac1mdZ^{[1]}X^T</script><p>$db^{[1]}=\frac1mnp.sum(dZ^{[1]},axis=1,keepdim=True)$$</p><p>反向传播的具体推导过程下一部分进行详细说明</p><h3 id="Backpropagation-intuition-optional"><a href="#Backpropagation-intuition-optional" class="headerlink" title="Backpropagation intuition(optional)"></a>Backpropagation intuition(optional)</h3><p>我们仍然使用计算图的方式来推导神经网络反向传播过程。记得之前介绍逻辑回归时，我们就引入了计算图来推导正向传播和反向传播，其过程如下图所示：<br><img src="/2018/02/19/浅层神经网络/logisticregressiongradients" alt="logisticregressiongradients"></p><p>由于多了一个隐藏层，神经网络的计算图要比逻辑回归的复杂一些，如下图所示。对于单个训练样本，正向过程很容易，反向过程可以根据梯度计算方法逐一推导。</p><script type="math/tex; mode=display">dz^{[2]}=a^{[2]}-y</script><script type="math/tex; mode=display">dW^{[2]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial W^{[2]}}=dz^{[2]}a^{[1]T}</script><script type="math/tex; mode=display">db^{[2]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial b^{[2]}}=dz^{[2]}\cdot 1=dz^{[2]}</script><script type="math/tex; mode=display">dz^{[1]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial a^{[1]}}\cdot \frac{\partial a^{[1]}}{\partial z^{[1]}}=W^{[2]T}dz^{[2]}\ast g^{[1]'}(z^{[1]})</script><script type="math/tex; mode=display">dW^{[1]}=dz^{[1]}\cdot \frac{\partial z^{[1]}}{\partial W^{[1]}}=dz^{[1]}x^T</script><script type="math/tex; mode=display">db^{[1]}=dz^{[1]}\cdot \frac{\partial z^{[1]}}{\partial b^{[1]}}=dz^{[1]}\cdot 1=dz^{[1]}</script><p><img src="/2018/02/19/浅层神经网络/neuralnetworkgradients" alt="neuralnetworkgradients"><br>浅层神经网络（包含一个隐藏层），m个训练样本的正向传播过程和反向传播过程分别包含了6个表达式，其向量化矩阵形式如下图所示：<br><img src="/2018/02/19/浅层神经网络/summary" alt="summary"></p><h3 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h3><p>神经网络模型中的参数权重W是不能全部初始化为零的，接下来我们分析一下原因。</p><p>举个简单的例子，一个浅层神经网络包含两个输入，隐藏层包含两个神经元。如果权重$W^{[1]}$和$W^{[2]}$都初始化为零，即：</p><script type="math/tex; mode=display">W^{[1]}= \left[ \begin{matrix} 0 & 0 \\ 0 & 0 \end{matrix} \right]</script><script type="math/tex; mode=display">W^{[2]}= \left[ \begin{matrix} 0 & 0 \end{matrix} \right]</script><p>这样使得隐藏层第一个神经元的输出等于第二个神经元的输出，即$a_1^{[1]}=a_2^{[1]}$。经过推导得到$dz_1^{[1]}=dz_2^{[1]}$，以及$W_1^{[1]}=dW_2^{[1]}$。因此，这样的结果是隐藏层两个神经元对应的权重行向量$W_1^{[1]}$和$W_2^{[1]}$每次迭代更新都会得到完全相同的结果，$W_1^{[1]}$始终等于$W_2^{[1]}$，完全对称。这样隐藏层设置多个神经元就没有任何意义了。值得一提的是，参数b可以全部初始化为零，并不会影响神经网络训练效果。<br><img src="/2018/02/19/浅层神经网络/initialization" alt="initialization"></p><p>我们把这种权重W全部初始化为零带来的问题称为symmetry breaking problem。解决方法也很简单，就是将W进行随机初始化（b可初始化为零）。python里可以使用如下语句进行W和b的初始化：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W_1 = np.random.randn((2,2))*0.01</span><br><span class="line">b_1 = np.zero((2,1))</span><br><span class="line">W_2 = np.random.randn((1,2))*0.01</span><br><span class="line">b_2 = 0</span><br></pre></td></tr></table></figure></p><p>这里我们将$W_1^{[1]}$和$W_2^{[1]}$乘以0.01的目的是尽量使得权重W初始化比较小的值。之所以让W比较小，是因为如果使用sigmoid函数或者tanh函数作为激活函数的话，W比较小，得到的|z|也比较小（靠近零点），而零点区域的梯度比较大，这样能大大提高梯度下降算法的更新速度，尽快找到全局最优解。如果W较大，得到的|z|也比较大，附近曲线平缓，梯度较小，训练过程会慢很多。</p><p>当然，如果激活函数是ReLU或者Leaky ReLU函数，则不需要考虑这个问题。但是，如果输出层是sigmoid函数，则对应的权重W最好初始化到比较小的值。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了浅层神经网络。首先，我们简单概述了神经网络的结构：包括输入层，隐藏层和输出层。然后，我们以计算图的方式推导了神经网络的正向输出，并以向量化的形式归纳出来。接着，介绍了不同的激活函数并做了比较，实际应用中根据不同需要选择合适的激活函数。激活函数必须是非线性的，不然神经网络模型起不了任何作用。然后，我们重点介绍了神经网络的反向传播过程以及各个参数的导数推导，并以矩阵形式表示出来。最后，介绍了权重随机初始化的重要性，必须对权重W进行随机初始化操作。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Neural-Network-Overview&quot;&gt;&lt;a href=&quot;#Neural-Network-Overview&quot; class=&quot;headerlink&quot; title=&quot;Neural Network Overview&quot;&gt;&lt;/a&gt;Neural Network Ov
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>神经网络基础之Python与向量化</title>
    <link href="http://andeper.cn/2018/02/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E4%B9%8BPython%E4%B8%8E%E5%90%91%E9%87%8F%E5%8C%96/"/>
    <id>http://andeper.cn/2018/02/19/神经网络基础之Python与向量化/</id>
    <published>2018-02-18T19:31:26.000Z</published>
    <updated>2018-02-19T12:27:16.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h3><p>深度学习算法中，数据量很大，在程序中尽量减少使用loop循环语句，而可以使用向量运算来提高程序运行速度。</p><p>向量化(Vectorization)就是利用矩阵运算的思想，大大提高运算速度。例如下面所示在Python中使用向量化要比使用循环计算速度快得多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">a = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">b = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line"></span><br><span class="line">tic = time.time()</span><br><span class="line">c = np.dot(a,b)</span><br><span class="line">toc = time.time()</span><br><span class="line"></span><br><span class="line">print(c)</span><br><span class="line">print(<span class="string">"Vectorized version:"</span> + str(<span class="number">1000</span>*(toc-tic)) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line">c = <span class="number">0</span></span><br><span class="line">tic = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000000</span>):</span><br><span class="line">    c += a[i]*b[i]</span><br><span class="line">toc = time.time()</span><br><span class="line"></span><br><span class="line">print(c)</span><br><span class="line">print(<span class="string">"for loop:"</span> + str(<span class="number">1000</span>*(toc-tic)) + <span class="string">"ms"</span>)</span><br></pre></td></tr></table></figure><p>输出结果类似于：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">250286.989866</span><br><span class="line">Vectorized version:1.5027523040771484ms</span><br><span class="line">250286.989866</span><br><span class="line">For loop:474.29513931274414ms</span><br></pre></td></tr></table></figure></p><p>从程序运行结果上来看，该例子使用for循环运行时间是使用向量运算运行时间的约300倍。因此，深度学习算法中，使用向量化矩阵运算的效率要高得多。</p><p>为了加快深度学习神经网络运算速度，可以使用比CPU运算能力更强大的GPU。事实上，GPU和CPU都有并行指令(parallelization instructions),称为Single Instruction Multiple Data(SIMD)。SIMD是单指令多数据流，能够复制多个操作数，并把它们打包在大型寄存器的一组指令集。SIMD能够大大提高程序运行速度，例如python的numpy库中的内建函数(build-in function) 就是使用了SIMD指令。相比而言，GPU的SIMD要比CPU更强大一些。</p><h3 id="More-Vectorization-Examples"><a href="#More-Vectorization-Examples" class="headerlink" title="More Vectorization Examples"></a>More Vectorization Examples</h3><p>尽量避免使用for循环而使用向量化矩阵运算。在python的numpy库中，我们通常使用np.dot()函数来进行矩阵运算。</p><p>我们将向量化的思想使用在逻辑回归算法，尽可能减少for循环，而只使用矩阵运算。值得注意的是，算法最顶层的迭代训练的for循环是不能替换的。而每次迭代过程对J，dw，b的计算是可以直接使用矩阵运算。</p><h3 id="Vectorizing-Logistic-Regression"><a href="#Vectorizing-Logistic-Regression" class="headerlink" title="Vectorizing Logistic Regression"></a>Vectorizing Logistic Regression</h3><p>整个训练样本构成的输入矩阵X的维度是$(n_X,1)$,b是一个常数值，而整个训练忘本构成的输出矩阵Y的维度是(1,m)。利用向量化的思想，所有m个样本的线性输出Z可以用矩阵表示：</p><script type="math/tex; mode=display">Z=w^TX+b</script><p>在python的numpy库中可以表示为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b</span><br><span class="line">A = sigmoid(Z)</span><br></pre></td></tr></table></figure></p><p>其中，w，T表示w的转置</p><p>这样，我们就能够使用向量化矩阵运算代替for循环，对所有m个样本同时运算，大大提高了运算速度。</p><h3 id="Vectorizing-Logistic-Regression’s-Gradient-Output"><a href="#Vectorizing-Logistic-Regression’s-Gradient-Output" class="headerlink" title="Vectorizing Logistic Regression’s Gradient Output"></a>Vectorizing Logistic Regression’s Gradient Output</h3><p>再来看逻辑回归中的梯度下降算法如何转化为向量化的矩阵形式。对于所有m个样本，db可表示为：</p><script type="math/tex; mode=display">dZ=A-Y</script><p>db可表示为：</p><script type="math/tex; mode=display">db=\frac{1}{m}\sum\limits_{i=1}^mdz^{(i)}</script><p>对应的程序为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db=1/m*np.sum(dZ)</span><br></pre></td></tr></table></figure></p><p>dw课表示为：</p><script type="math/tex; mode=display">dw=\frac{1}{m}X\cdot dZ^{T}</script><p>对应的程序为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dw=1/m*np.dot(X,dZ,T)</span><br></pre></td></tr></table></figure></p><p>这样，我们把整个逻辑回归中的for循环尽可能用矩阵运算代替，对于单次迭代，梯度下降算法流程如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Z=np.dot(w.T,X)+b</span><br><span class="line">A= sigmoid(Z)</span><br><span class="line">dZ= A-Y</span><br><span class="line">dw = 1/m*np.dot(X,dZ.T)</span><br><span class="line">db = 1/m*np.sum(dZ)</span><br><span class="line"></span><br><span class="line">w = w - alpha*dw</span><br><span class="line">b = b - alpha*db</span><br></pre></td></tr></table></figure></p><p>其中，alpha是学习因子，决定w和b的更新速度。上述代码只是单次训练更新而言的，外层还需要一个for循环，代表迭代次数。</p><h3 id="Broadcasting-in-Python"><a href="#Broadcasting-in-Python" class="headerlink" title="Broadcasting in Python"></a>Broadcasting in Python</h3><p>下面介绍使用python的另一种技巧：广播(Broadcasting).python中的广播机制可以由下面四条表示：<br><strong>·让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分都通过在前面加1补齐</strong><br><strong>·输出数组的shape是输入数组shape的各轴上的最大值</strong><br><strong>·如果输入数组的某个轴和输出数组的对应轴的长度相同或者其长度为1时，这个数组能够用来计算，否则出错</strong><br><strong>·当输入数组的某个轴的长度为1时，沿着此轴运算时都用此轴上的第一组值</strong><br>简而言之，就是用python中可以对不同维度的矩阵进行四则混合运算，但至少保证有一个维度是相同的。下面给出几个广播的例子，具体细节可参阅python的相关手册，这里就不赘述了。</p><script type="math/tex; mode=display">Broadcasting examples</script><script type="math/tex; mode=display">\begin{bmatrix} 1\\ 2\\ 3\\ 4 \end{bmatrix} + 100=\begin{bmatrix} 101\\ 102\\ 103\\ 104 \end{bmatrix}</script><script type="math/tex; mode=display">\begin{bmatrix} 1&2&3\\ 4&5&6 \end{bmatrix} + \begin{bmatrix} 100&200&300\end{bmatrix} = \begin{bmatrix} 101&202&303\\ 104&205&306 \end{bmatrix}</script><script type="math/tex; mode=display">\begin{bmatrix}1&2&3\\4&5&6\end{bmatrix} + \begin{bmatrix} 100\\200\end{bmatrix} = \begin{bmatrix} 101&102&103\\ 204&205&206 \end{bmatrix}</script><p>在python程序中为了保证矩阵运算正确，可以使用reshape()函数来对矩阵设定所需的维度。这是一个很好且有用的习惯。</p><h3 id="A-note-on-python-numpy-vectors"><a href="#A-note-on-python-numpy-vectors" class="headerlink" title="A note on python/numpy vectors"></a>A note on python/numpy vectors</h3><p>总结一些python的小技巧，避免不必要的code bug<br>python中，如果我们用下列语句来定义一个变量：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">5</span>)</span><br></pre></td></tr></table></figure></p><p>这条语句生成的a维度是(5, )。它既不是行向量也不是列向量，我们把a叫做rank 1 array。这种定义会带来一些问题。例如我们对a进行转置，还会得到a本身。所以，如果我们要定义(5,1)的列向量，最好使以下标准语句，避免使用rank 1 array。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">b = np.random.randn(<span class="number">1</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure></p><p>除此之外，我们还可以使用assert语句对向量或者数组的维度进行判断，例如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span>(a.shape == (<span class="number">5</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><p>assert语句会对内嵌语句进行判断，即判断a的维度是不是(5,1)的。如果不是，则程序在此处停止。使用assert语句也是一种很好的习惯，能够帮我们及时检查、发现语句是否正确。<br>另外，还可以使用reshape函数对数组设定所需的维度：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.shape((<span class="number">5</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><h3 id="Quick-tour-of-Jupyter-iPython-Notebooks"><a href="#Quick-tour-of-Jupyter-iPython-Notebooks" class="headerlink" title="Quick tour of Jupyter/iPython Notebooks"></a>Quick tour of Jupyter/iPython Notebooks</h3><p>Jupyter notebook是一个交互笔记本，支持运行40中编程语言，本课程所有的编程练习题都将在Jupyter notebook上进行，使用语言是python。</p><h3 id="Explanation-of-logistic-regression-cost-function-optional"><a href="#Explanation-of-logistic-regression-cost-function-optional" class="headerlink" title="Explanation of logistic regression cost function(optional)"></a>Explanation of logistic regression cost function(optional)</h3><p>接下来简要介绍逻辑回归的Cost function是怎么来的<br>首先，预测输出$\hat{y}$的表达式可以写成:</p><script type="math/tex; mode=display">\hat{y}=\sigma(w^Tx+b)</script><p>其中，$\sigma(z)=\frac{1}{1+exp(-z)}$。$\hat{y}$可以看成是预测输出为正类(+1)的概率：</p><script type="math/tex; mode=display">\hat{y}=P(y=1|X)</script><p>那么，当y=1时：</p><script type="math/tex; mode=display">p(y|x)= \hat{y}</script><p>当y= 0时：</p><script type="math/tex; mode=display">p(y|x)=\hat{y}^y(1-\hat{y})^{(1-y)}</script><p>我们把上面两个式子整合到一个式子中，得到：</p><script type="math/tex; mode=display">P(y|x)=\hat{y}^y(1-\hat{y})^{(1-y)}</script><p>由于log函数的单调性，可以对上式P(y|x)进行log处理</p><script type="math/tex; mode=display">logP(y|x)=log\hat{y}^y(1-\hat{y})^{(1-y)}=ylog\hat{y}+(1-y)log(1-\hat{y})</script><p>我们希望上述概率P(y|x)越大越好，对上式加上负号，则转化成额单个样本的Loss function，越小越好，也iu得到了我们之前介绍的逻辑回归的Loss function形式</p><script type="math/tex; mode=display">L=-(ylog\hat{y}+(1-y)log(1-\hat{y}))</script><p>如果对于所有m个训练样本，假设样本之间是独立同分布的(iid),我们希望总的概率越大越好：</p><script type="math/tex; mode=display">max\ \prod_{i=1}^m\ P(y^{(i)}|x^{(i)})</script><p>同样引入log函数，加上负号，将上式转化为Cost function：</p><script type="math/tex; mode=display">J(w,b)=-\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})=-\frac 1m\sum_{i=1}^my^{(i)}\ log\ \hat y^{(i)}+(1-y^{(i)})log(1-\hat y^{(i)})</script><p>上式中，$\frac{1}{m}$表示对所有m个样本的Cost function求平均，是缩放因子。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课我们主要介绍了神经网络基础————python和向量话。在深度学习程序中，使用向量化和矩阵运算的方法能够大大提高运行速度，节省时间。以逻辑回归威力，我们将算法流程包括梯度下降转化为向量化的形式，同时，我们也介绍了python的相关编程方法和技巧。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Vectorization&quot;&gt;&lt;a href=&quot;#Vectorization&quot; class=&quot;headerlink&quot; title=&quot;Vectorization&quot;&gt;&lt;/a&gt;Vectorization&lt;/h3&gt;&lt;p&gt;深度学习算法中，数据量很大，在程序中尽量减少使用lo
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>神经网络基础之逻辑回归</title>
    <link href="http://andeper.cn/2018/01/26/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://andeper.cn/2018/01/26/神经网络基础之逻辑回归/</id>
    <published>2018-01-26T09:11:36.000Z</published>
    <updated>2018-02-19T10:58:16.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>附：从本章开始常用到数学符号和公式，这里贴出在md中写公式的方法，<a href="http://blog.csdn.net/zryxh1/article/details/53161011" target="_blank" rel="noopener">md语法|LaTex数学公式</a>。遇到不知道的符号可以去在线编辑器选择查看代码 <a href="http://www.codecogs.com/latex/eqneditor.php" target="_blank" rel="noopener">Latex在线编辑器</a></strong></p><h3 id="Binary-Classification"><a href="#Binary-Classification" class="headerlink" title="Binary Classification"></a>Binary Classification</h3><p>逻辑回归模型一般是用来解决二分类(Binary Classification)问题。二分类就是输出y只有{0，1}两个离散值(也有{1,-1}的情况)。<br>我们以一个图像识别问题为例，判断图片中是否有猫的存在，0代表no cat，1代表cat，如下图所示。<br><img src="/2018/01/26/神经网络基础之逻辑回归/binaryclassification.png" alt="binaryclassification"><br>主要通过这个例子简要介绍神经网络模型中的一些标准化的、有效率的处理方法和notations。<br><img src="/2018/01/26/神经网络基础之逻辑回归/binaryclassification2.png" alt="binaryclassification2"><br>如上图所示，这是一个典型的二分类问题。一般来说，彩色图片包含RGB三个通道。例如该cat的图片尺寸为(64,64,3)。在神经网络模型中，我们首先要将图片输入x转化为一维的特征向量(feature vector)。方法是每个通道一行一行取，再连接起来。由于$64\times 64\times 3= 12288$，转化后的输入特征向量维度为(12288,1)。此特征向量x是列向量，维度一般记为$n_x$。<br>如果训练样本共有m张图片，那么整个训练样本X组成了矩阵，维度是$(n_x,1)$。注意，这里的矩阵X的行$n_x$代表了每个样本$x^{(i)}$特征个数，列m代表了样本个数。这里Andrew解释了X的维度之所以是$(n_x,m)$而不是$(m,n_x)$是为了之后矩阵运算的方便。而所有的训练样本的输出Y也组成了一维的行向量，写成矩阵的形式后，它的维度就是$(1,m)$。</p><h3 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h3><p>如何使用逻辑回归来解决二分类问题。逻辑回归中，预测值$\hat{h}=P(y=1|x)$表示为1的概率，取值范围在[0,1]之间。这是其与二分类模型不同的地方。使用线性模型，引入参数w和b。权重w的维度是$(n_x,1)$,b是一个常数项。这样，逻辑回归的线性预测输出可以写成：</p><script type="math/tex; mode=display">\hat{h}=w^{T}x+b</script><p>值得注意的是，很多其他机器学习资料中，可能吧常数b当作$w_0$处理，并引入$x_0=1$,这样从维度上来看，x和w都会增加一维，但在本课程中，为了简化计算和便于理解，Andrew建议还是用上式这种形式将w和b分开比较好。<br>上式的线性输出区间为整个实数范围，而逻辑回归要求输出范围在[0,1]之间，所以还需要对上式的线性函数输出进行处理，方法是引入Sigmoid函数，让输出限定在[0,1]之间。这样，逻辑回归的预测输出就可以完整写成：</p><script type="math/tex; mode=display">\hat{y}=Sigmoid(w^{T}x+b)=\sigma(w^{T}+b)</script><p>Sigmoid函数是一种非线性的S型函数，输出被限定在[0,1]之间，通常被用在神经网络中当作激活函数(Activation function)使用。Sigmoid函数的表达式和曲线如下：</p><script type="math/tex; mode=display">Sigmoid(z)=\frac{1}{1+e^{-z}}</script><p><img src="/2018/01/26/神经网络基础之逻辑回归/Sigmoid" alt="sigmoid"><br>从Sigmoid函数可以看出，当z值很大时，函数值趋向于1；当z值很小时，函数值趋向于0.且当z=0时，函数值为0.5。Sigmoid函数的一阶导数可以用其自身表示：</p><script type="math/tex; mode=display">\sigma(z)=\sigma(z)(1-\sigma(z))</script><p>这样，通过Sigmoid函数，就能够将逻辑回归的输出限定在[0,1]之间了。</p><h3 id="Logistic-Regression-Cost-Function"><a href="#Logistic-Regression-Cost-Function" class="headerlink" title="Logistic Regression Cost Function"></a>Logistic Regression Cost Function</h3><p>逻辑回归中，w和b都是未知参数，需要反复训练和优化得到。因此，我们需要定义一个cost function，包含了参数w和b。通过优化cost function ，当cost function取值最小时，得到对应的w和b</p><p>对于m个训练样本，我们通常使用上标来表示对应的样本，例如$(x^{(i)},y^{(i)})$来表示第i个样本。</p><p>如何定义所有m个样本的cost function呢，先从单个样本出发，我们希望该样本的预测值$\hat{y}$与真实值越相似越好。我们把单个样本的cost function用Loss function来表示，根据以往经验，如果使用平方误差(squared error)来衡量，如下所示：</p><script type="math/tex; mode=display">L(\hat{y},y) = \frac{1}{2}(\hat{y}-y)^{2}</script><p>但是对于逻辑回归，我们一般不用平方误差来衡量，因为这种Loss function一般是non-convex(非凸性)的。non-convex函数在使用梯度下降法时，容易得到局部最小值(local minmum),即局部最优化。而我们的目标是计算得到全局最优化(Global optimization).因此，我们一般选择的Loss function应该是convex的。</p><p>Loss function的原则和目的是衡量预测输出$\hat(y)$与真实样本输出y的接近程度。平方误差其实也可以，只是它是non-convex的，不利于使用梯度下降法来进行全局优化。因此，我们可以构建另外一种Loss function，且是convex的，如下所示：</p><script type="math/tex; mode=display">L(\hat{y},y)=-(ylog\hat{y}+(1-y)log(1-\hat{y}))</script><p>这个Loss function。它是衡量误差大小的，Loss function越小越好。</p><p>当y=1时，$L(\hat{y},y)=-log\hat{y}$,如果$\hat{y}$越接近1，$L(\hat{y},y)\approx0$,表示预测效果越好；如果$\hat{y}$越接近0，$L(\hat{y},y)\approx+\infty$,表示预测效果越差。这正是我们希望Loss function所实现的功能。</p><p>当y=0时，$L(\hat{y},y)=-log(1-\hat{y})$.如果$\hat{y}$越接近于0，$L(\hat{y},y)\approx0$,表示预测效果越好；如果$\hat{y}$越接近于1，$L(\hat{y},y)\approx+\infty$,表示预测效果越差。这也正是我们希望Loss function所实现的功能。</p><p>因此，这个Loss function能够很好地反映预测输出$\hat{y}$与真实样本输出y的接近程度，越接近的话，其Loss function的值越小。而且这个函数是convex的。上面我们只是简要分析为什么要使用这个Loss function后面的课程将详细推导这个Loss function是如何得到的</p><p>Loss function是针对单个样本的，对于m个样本，我们定义Cost function。Cost function是m的样本的Loss function的平均值，反映了m个样本的预测输出$\hat{y}$与真实样本输出y的平均接近程度。Cost function可表示为：</p><script type="math/tex; mode=display">J(w,b)=\frac{1}{m}\sum\limits_{k=1}^m[y^{(i)}log\hat{y}^{(i)}+(1-y^{(i)})log(1-\hat{y}^{(i)})]</script><p>Cost function已经推导出来了，Cost function是关于待求系数w和b的函数。我们的目标是迭代计算出最佳的w和b值，最小化Cost function，让Cost function尽可能接近于零。</p><p>其实逻辑回归问题可以看成是一个简单的神经网络，只包含一个神经元。这也是我们先介绍逻辑回归的原因。</p><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>我们已经掌握Cost function的表达式，接下来将使用梯度下降(Gradient Descent)算法计算出合适的w和b值，从而最小化m个训练样本的Cost function，即J(w,b)。</p><p>由于J(w,b)是convex function，梯度下降算法是先随机选择一组参数w和b值，然后每次迭代的过程中分别沿着w和b的梯度(偏导数)的反方向前进一小步，不断修正w和b。每次迭代更新w和b后，都能让J(w,b)更接近全局最小值。梯度下降的过程如下图所示<br><img src="/2018/01/26/神经网络基础之逻辑回归/GradientDescent" alt="GradientDescent"><br>梯度下降算法每次迭代更新，w和b的修正表达式为</p><script type="math/tex; mode=display">w:=w-\alpha\frac{\partial J(w,b)}{\partial w}</script><script type="math/tex; mode=display">b:=b-\alpha\frac{\partial J(w,b)}{\partial b}</script><p>上式中，$\alpha$是学习因子(learning rate),表示梯度下降的步进长度。$\alpha$越大，w和b每次更新的“步伐”更大一些；$\alpha$越小，w和b每次更新的“步伐”更小一些。在程序代码中，我们通常使用dw来表示$frac{\partial J(w,b)}{\partial w}$，用db来表示$\frac{\partial J(w,b)}{\partial b}$。微积分里，$\frac{df}{dx}$表示对单一变量求导数，$\frac{\partial f}{\partial x}$表示对多个变量中的某个变量求偏导数。</p><p>梯度下降算法能够保证每次迭代w和b都能向着J(w,b)全局最小化的方向进行，其数学原理主要是运用泰勒一阶展开来证明的</p><h3 id="Derivatives"><a href="#Derivatives" class="headerlink" title="Derivatives"></a>Derivatives</h3><p>这一部分内容主要是Andrew对微积分、求导数进行介绍。梯度或者导数一定程度上可以看成斜率，这里不再赘述。</p><h3 id="More-Derivatives-Examples"><a href="#More-Derivatives-Examples" class="headerlink" title="More Derivatives Examples"></a>More Derivatives Examples</h3><p>Andrew给出更复杂求导数的例子。</p><h3 id="Computation-graph"><a href="#Computation-graph" class="headerlink" title="Computation graph"></a>Computation graph</h3><p>整个神经网络的训练过程实际上包含了两个过程：正向传播(Forward Propagation)和反向传播(Back Propagation)。正向传播是从输入到输出，由神经网络计算的到预测输出的过程，反向传播是从输出到输入，对参数w和b计算梯度的过程。下面，我们用计算图(Computation graph)的形式来理解这两个过程。</p><p>举个简单的例子，假如Cost function为J(a,b,c)=3(a+bc),包含啊a,b,c三个变量。我们用u表示bc，v表示a+u,则J=3v。它的计算图可以写成如下图所示：<br><img src="/2018/01/26/神经网络基础之逻辑回归/Computationgraph" alt="Computationgraph"><br>令a=5,b=3,c=2,则u=bc=6，v=a+u=11，J=3v=33。计算图中，这种从左到右，从输入到输出的过程就对应着神经网络或者逻辑回归中输入与权重经过运算计算得到Cost function的正向过程。</p><h3 id="Derivatives-with-a-Computation-Graph"><a href="#Derivatives-with-a-Computation-Graph" class="headerlink" title="Derivatives with a Computation Graph"></a>Derivatives with a Computation Graph</h3><p>上一部分介绍的是计算图的正向传播(Forward Propagation),下面我们来介绍其反向传播(Back Propagation),即计算输出对输入的偏导数。</p><p>还是上个计算图的例子，输入参数有3个，分别是a，b，c。</p><p>首先计算J对参数a的偏导数。从计算图上来看，从右到左，J是v的函数，v是a的函数，则利用求导技巧，可以得到：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial a}=\frac{\partial J}{\partial v}\cdot\frac{\partial v}{\partial a}=3\cdot1=3</script><p>根据这种思想，然后计算J对参数b的偏导数。从计算图上来看，从右到左，J是v的函数，v是u的函数，u是b的函数。可以推导：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial b}=\frac{\partial J}{\partial v}\cdot\frac{\partial v}{\partial u}\cdot\frac{\partial u}{\partial b}=3\cdot 1\cdot c=3\cdot 1\cdot 2=6</script><p>最后计算J对参数c的偏导数。仍从计算图上来看，从右到左，J是v的函数，v是u的函数，u是c的函数。可以推导：</p><script type="math/tex; mode=display">\frac{\partial J}{\partial c}=\frac{\partial J}{\partial v}\cdot\frac{\partial v}{\partial u}\cdot\frac{\partial u}{\partial c}=3\cdot 1\cdot b=3\cdot 1\cdot 3=9</script><p>为了同一格式，在程序代码中，我们使用da，db，dc来表示J对参数a，b，c的偏导数。<br><img src="/2018/01/26/神经网络基础之逻辑回归/Computationgraph2" alt="Computationgraph2"></p><h3 id="Logistic-Regression-Gradient-Descent"><a href="#Logistic-Regression-Gradient-Descent" class="headerlink" title="Logistic Regression Gradient Descent"></a>Logistic Regression Gradient Descent</h3><p>现在，我们将对逻辑回归进行梯度计算。对单个样本而言，逻辑回归Loss function表达式如下：</p><script type="math/tex; mode=display">z=w^{T}x+b</script><script type="math/tex; mode=display">\hat{y}=a=\sigma(z)</script><script type="math/tex; mode=display">L(a,y)=-(ylog(a)+(1-y)log(1-a))</script><p>首先，该逻辑回归的正向传播过程非常简单。根据上述公式，例如输入样本x有两个特征(x1,x2),相应的权重w维度也是2，即(w1,w2)。则$z=w_1x_1+w_2x_2+b$,最后的Loss function如下所示：<br><img src="/2018/01/26/神经网络基础之逻辑回归/Logisticregression" alt="Logisticregression"><br>然后，计算该逻辑回归的反向传播过程，即由Loss function计算参数w和b的偏导数。推导过程如下：</p><script type="math/tex; mode=display">da=\frac{\partial L}{\partial a}= -\frac{y}{a}+\frac{1-y}{1-a}</script><script type="math/tex; mode=display">dz=\frac{\partial L}{\partial z}=\frac{\partial L}{\partial a}\cdot\frac{\partial a}{\partial z}= (-\frac{y}{a}+\frac{1-y}{1-a})\cdot a(1-a)= a-y</script><p>知道了dz之后，就可以直接对$W_1,W_2$和b进行求导了。</p><script type="math/tex; mode=display">dw_1=\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial z}\cdot\frac{\partial z}{\partial w_1}=x_1\cdot dz=x_1(a-y)</script><script type="math/tex; mode=display">dw_2=\frac{\partial L}{\partial w_2}=\frac{\partial L}{\partial z}\cdot\frac{\partial z}{\partial w_2}=x_2\cdot dz=x_2(a-y)</script><script type="math/tex; mode=display">db=\frac{\partial L}{\partial b}=\frac{\partial L}{\partial z}\cdot\frac{\partial z}{\partial b}=1\cdot dz=a-y</script><p>则梯度下降算法可以表示为：</p><script type="math/tex; mode=display">w_1:=w_1-\alpha dw_1</script><script type="math/tex; mode=display">w_2:=w_2-\alpha dw_2</script><script type="math/tex; mode=display">b:=b-\alpha db</script><p><img src="/2018/01/26/神经网络基础之逻辑回归/Logisticregression2" alt="Logisticregression2"></p><h3 id="Gradient-Descent-on-m-examples"><a href="#Gradient-Descent-on-m-examples" class="headerlink" title="Gradient Descent on m examples"></a>Gradient Descent on m examples</h3><p>上一部分讲的是对单个样本求偏导和梯度下降。如果有m个样本，其Cost function 表达式如下：</p><script type="math/tex; mode=display">z^{(i)}=w^{T}x^{(i)}+b</script><script type="math/tex; mode=display">\hat{y}^{(i)}=a^{(i)}=\sigma(z^{(i)})</script><script type="math/tex; mode=display">J(w,b)=\frac{1}{m}\sum\limits_{i=1}^mL(\hat{y}^{(i)},y^{(i)})=-\frac{1}{m}\sum\limits_{i=1}^m[y^{(i)}log\hat{y}^{(i)}+(1-y^{(i)})log(1-\hat{y}^{(i)})]</script><p>Cost function关于w和b的偏导数可以写成和平均的形式</p><script type="math/tex; mode=display">dw_1=\frac{1}{m}\sum\limits_{i=1}^mx_1^{(i)}(a^{(i)}-y^{(i)})</script><script type="math/tex; mode=display">dw_2=\frac{1}{m}\sum\limits_{i=1}^mx_2^{(i)}(a^{(i)}-y^{(i)})</script><script type="math/tex; mode=display">db=\frac{1}{m}\sum\limits_{i=1}^m(a^{(i)}-y^{(i)})</script><p>这次，每次迭代中w和b的梯度有m个训练样本计算平均值得到。其算法流程图如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">J=0;dw1=0; dw2=0;db=0;</span><br><span class="line">for i = 1 to m</span><br><span class="line">    z(i) = wx(i)+b;</span><br><span class="line">    a(i) = sigmoid(z(i));</span><br><span class="line">    J += -[y(i)log(a(i))+(1-y(i)）log(1-a(i));</span><br><span class="line">    dz(i) = a(i)-y(i);</span><br><span class="line">    dw1 += x1(i)dz(i);</span><br><span class="line">    dw2 += x2(i)dz(i);</span><br><span class="line">    db += dz(i);</span><br><span class="line">J /= m;</span><br><span class="line">dw1 /= m;</span><br><span class="line">dw2 /= m;</span><br><span class="line">db /= m;</span><br></pre></td></tr></table></figure></p><p>这样经过n次迭代后，整个梯度下降算法就完成了。</p><p>在上述的梯度下降算法中，我们是利用for循环对每个样本进行dw1，dw2和db的累加计算最后再求平均数的。在深度学习中，样本数量m通常很大，使用for循环会让神经网络程序运行得很慢。所以，我们应该尽量避免使用for循环操作，而使用矩阵运算，能够大大提高程序运行速度。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了神经网络的基础——逻辑回归。首先，我们介绍了二分类问题，以图片为例，将多维输入x转化为feature vector，输出y只有{0，1}两个离散值。介绍了逻辑回归及其对应的Cost function 形式，然后介绍了梯度下降算法，并使用计算图的方式来讲述神经网络的正向传播和反向传播两个过程，总结出最优化参数w和b的算法流程。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;附：从本章开始常用到数学符号和公式，这里贴出在md中写公式的方法，&lt;a href=&quot;http://blog.csdn.net/zryxh1/article/details/53161011&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;md
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>深度学习概论</title>
    <link href="http://andeper.cn/2018/01/17/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%AE%BA/"/>
    <id>http://andeper.cn/2018/01/17/深度学习概论/</id>
    <published>2018-01-17T08:22:22.000Z</published>
    <updated>2018-02-18T18:08:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>由于毕业设计选择了深度学习相关的选题，我便开始学习深度学习相关的课程，使用的课程资源是吴恩达的《神经网络与深度学习》,<a href="http://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank" rel="noopener">课程链接</a>,最近的博客基本会以这个课程的笔记为主。</p><h3 id="What-is-a-neural-network"><a href="#What-is-a-neural-network" class="headerlink" title="What is a neural network?"></a>What is a neural network?</h3><p>什么是神经网络，下面通过一个简单的例子引入神经网络模型的概念。</p><p>加入我们要建立房价的预测模型，一共有六个房子。我们已知输入x即每个房子的面积，还知道其对应的输出y即每个房子的价格。根据这些输入输出，我们要简历一个函数模型，来预测房价。根据这些输入输出，我们要建立一个函数模型，来预测房价：y=f(x)。首先，我们将已知的六间房子的价格和面积关系绘制在二维平面上，如下图所示：<br><img src="/2018/01/17/深度学习概论/housepriceprediction.png" alt="housepriceprediction"><br>一般地，我们会用一条之间来拟合图中这些离散点，即建立房价与面积的线性模型。但是从实际考虑，我们知道价格永远不会是负数。所以，我们对该之间做一点点修正，让它编程折线的形状，当面积小于某个值时，价格始终为零，如下图蓝色折线所示，这就是我们建立的房价预测模型<br><img src="/2018/01/17/深度学习概论/housepriceprediction2.png" alt="housepriceprediction2"><br>其实这个简单的模型(蓝色折线)就可以看成是一个神经网络，而且几乎是一个最简单的神经网络，我们把房价预测用一个最简单的神经网络模型来表示，如下图所示<br><img src="/2018/01/17/深度学习概论/networkneuron.png" alt="networkneuron"><br>该神经网络的输入x是房屋面积，输入y是房屋价格，中间包含了一个神经元(neuron)，房价预测函数(蓝色折线)。该神经元的功能就是实现函数f(x)的功能。</p><p>上图神经元的预测函数在神经网络应用中比较常见，我们把这个函数称为ReLU函数，即线性整流函数，如下图所示<br><img src="/2018/01/17/深度学习概论/ReLU.png" alt="ReLU"></p><p>上面讲的只是由单个神经元(输入x仅仅是房屋面积一个因素)组成的神经网络，而通常一个大型的神经网络往往由许多个神经元组成。</p><p>现在，我们把上面房价预测例子变得更复杂，而不仅仅使用房屋面积(size)一个判断因素。例如，除了考虑房屋面积之外，我们还考虑卧室数目(bedrooms)。这两点实际上与家庭成员的个数(family size)有关。还有房屋的邮政编码(zip code)，代表着该房屋位置的交通便利性，是否需要步行还是开车？即决定了可步行性(walkability)。另外，还有可能邮政编码和地区财富水平(wealth)共同影响了房屋所在地区的学校质量(school quality)。如下图所示，该神经网络共有三个神经元，分别代表了family size，walkability和school quality。每个神经元都包含了一个ReLU函数(或者其他非线性函数)。那么，根据这个模型，我们可以根据房屋面积和卧室个数来估计family size，根据邮政编码来估计walkability，根据邮政编码和财富水平来估计school quality。最后，由family size，walkability和school quality灯这些人们比较关心的因素来预测最终的房屋价格。<br><img src="/2018/01/17/深度学习概论/housepriceprediction3.png" alt="housepriceprediction3"><br>所以，在这个例子中，x是size，bedrooms，zip code，wealth这四个输入；y是房屋的预测价格。这个神经网络模型包含的神经元个数更多一些，相对之前的单个神经元的模型更加复杂。那么，在建立一个表现良好的神经网络模型之后，在给定输入x时，就能得到比较好的输出y，即房屋的预测价格。</p><p>实际上，上面这个例子神经网络模型结构如下所示，分别是size，bedrooms，zip code和wealth。在给定这四个输入后，神经网络所做的就是输出房屋的预测价格y。图中，三个神经元所在的位置称之称之为中间层或者隐藏层(x所在的称之为输入层，y所在的称之为输出层)，每个神经元与所有的输入x都有关联。<br><img src="/2018/01/17/深度学习概论/housepriceprediction4.png" alt="housepriceprediction4"><br>这就是基本的神经网络模型结构。在训练的过程中，只要有足够的输入x和输出y，就能训练出较好的神经网络模型，该模型在此类房价预测问题中，能够得到比较准确的结果。</p><h3 id="Supervised-Learning-with-Neural-Network"><a href="#Supervised-Learning-with-Neural-Network" class="headerlink" title="Supervised Learning with Neural Network"></a>Supervised Learning with Neural Network</h3><p>目前为止，由神经网络模型创造的价值基本上都是基于监督式学习(Supervised Learning)的，监督式学习与非监督式学习本质区别就是是否已知训练样本的输出y。在实际应用中，机器学习解决的大部分问题都属于监督式学习，神经网络模型也大都属于监督式学习。下面举几个监督式学习在神经网络中应用的例子。<br>第一个例子是房屋价格预测。根据训练样本的输入x和输出y，训练神经网络模型，预测房价。<br>第二个例子是线上广告，输入x是广告和用户个人信息，输入y是用户是否对广告进行点击。神经网络模型经过训练，能够根据广告类型和用户信息对用户的点击行为进行预测，从而向用户提供自己可能感兴趣的广告。<br>第三个例子是电脑视觉(computer vision)。输入x是图片像素值，输出是图片所属的不同类别。<br>第四个例子是语音识别(speech recognition).可以将一段语音信号辨识为相应的文字信息。<br>第五个例子是智能翻译，通过神经网络输入英文，然后直接输出中文。<br>第六个例子是自动驾驶。通过输入一张图片或者汽车雷达信息，神经网络通过训练来告诉你相应的路况并做出相应的决策。<br>如下图所示<br><img src="/2018/01/17/深度学习概论/监督式学习.png" alt="监督式学习"></p><p>对于不同的问题和应用场合，应该使用不同类型的神经网络模型。上面介绍的几个例子中，<br>对于一般的监督式学习(房价预测和线上广告问题)，我们只要使用标准的神经网络模型就可以了。图像识别处理问题，我们则要使用卷积神经网络(Convolution Neural Network)即CNN。而对于处理类似语音这样的序列信号时，则要使用循环神经网络(Recurrent Neural Network),即RNN。还有其他例如自动驾驶这样的复杂问题则需要更加复杂的混合神经网络模型。<br>下面给出Standard NN，Convolutional NN和Recurrent NN的神经网络结构图。<br><img src="/2018/01/17/深度学习概论/CNNRNN.png" alt="CNNRNN"><br>CNN一般处理图像问题，RNN一般处理语音信号。</p><p>数据类型一般分为两种：结构化数据(Structured Data)和非结构化数据(Unstructured Data)。<br><img src="/2018/01/17/深度学习概论/结构化数据与非结构化数据.png" alt="结构化数据与非结构化数据"></p><h3 id="Why-is-Deep-Learning-taking-off"><a href="#Why-is-Deep-Learning-taking-off" class="headerlink" title="Why is Deep Learning taking off?"></a>Why is Deep Learning taking off?</h3><p>深度学习和神经网络背后的技术思想已经出现了数十年了，那么为什么直到现在才开始发挥作用呢？<br>深度学习为什么强大？下面我们用一张图来说明。如下图所示，横坐标x表示数据量(Amount of data),纵坐标y表示机器学习模型的性能表现(Performance)。<br><img src="/2018/01/17/深度学习概论/Performance.png" alt="Performance"><br>上面共有4条曲线。最底下那条红色曲线代表了传统机器学习算法的表现，例如是SVM，logistic regression，decision tree等，当数据量比较小的时候，传统学习模型的表现是比较好的，但是当数据量很大的时候表现很一般。黄色曲线代表了规模较小的神经网络模型(Small NN)。它在数据量大的时候性能优于传统机器学习。蓝色曲线代表了规模中等的神经网络模型(Media NN),它在数据量更大的时候表现比Small NN更好。绿色曲线代表了更大规模的神经网络(Large NN),即深度学习模型。从图中可以看出，在数据量很大的时候，它的表现仍然是最好的，而且基本上保持了较快的上升趋势。<br>深度学习强大的原因归结为3个因素：<br><strong>Data</strong><br><strong>Computation</strong><br><strong>Algorithms</strong><br>数据量的几何级数增加<br>GPU出现，计算机运算能力大大提升。<br>举个算法改进的例子：之前神经网络神经元的激活函数是Sigmoid函数，后来改成了ReLU函数。更改的原因是对于Sigmoid函数，在远离Sigmoid函数，后来改成了ReLU函数。之所以这样更改的原因是对于Sigmoid函数，在远离零点的位置，函数曲线非常平缓，其梯度始终为0，所以造成神经网络模型学习速度变得很慢。然而，ReLU函数在x大于零的区域，其梯度始终为1，尽管x在小于零的区域梯度为0，但是在实际应用中采用ReLU函数确实比Sigmoid函数快很多。<br>构建一个深度学习的流程是首先开始产生Idea，然后把Idea转化为Code，最后进行Experiment。根据结果修改Idea，继续这种循环，直到最终训练得到表现不错的深度学习网络模型。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于毕业设计选择了深度学习相关的选题，我便开始学习深度学习相关的课程，使用的课程资源是吴恩达的《神经网络与深度学习》,&lt;a href=&quot;http://mooc.study.163.com/smartSpec/detail/1001319001.htm&quot; target=&quot;_b
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>自顶向下之TCP</title>
    <link href="http://andeper.cn/2017/04/02/%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B%E4%B9%8BTCP/"/>
    <id>http://andeper.cn/2017/04/02/自顶向下之TCP/</id>
    <published>2017-04-02T13:22:00.000Z</published>
    <updated>2018-01-17T07:33:52.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="TCP服务"><a href="#TCP服务" class="headerlink" title="TCP服务"></a>TCP服务</h3><h4 id="进程到进程的通信"><a href="#进程到进程的通信" class="headerlink" title="进程到进程的通信"></a>进程到进程的通信</h4><p>TCP通过使用端口号来提供进程到进程间通信</p><h4 id="流传递服务"><a href="#流传递服务" class="headerlink" title="流传递服务"></a>流传递服务</h4><p>TCP是一个面向流的协议,TCP允许发送进程以字节流的形式传递数据，并且接受进程也以字节流的形式接受数据</p><h4 id="全双工通信"><a href="#全双工通信" class="headerlink" title="全双工通信"></a>全双工通信</h4><p>数据可以在同一事件双向流动，每一个方向TCP都有发送和接受缓冲区，他们能双向发送和接受段</p><h4 id="多路复用和多路分解"><a href="#多路复用和多路分解" class="headerlink" title="多路复用和多路分解"></a>多路复用和多路分解</h4><p>TCP在发送端执行多路复用，在接收端执行多路分解</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;TCP服务&quot;&gt;&lt;a href=&quot;#TCP服务&quot; class=&quot;headerlink&quot; title=&quot;TCP服务&quot;&gt;&lt;/a&gt;TCP服务&lt;/h3&gt;&lt;h4 id=&quot;进程到进程的通信&quot;&gt;&lt;a href=&quot;#进程到进程的通信&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>图解TCP/IP之TCP与UDP</title>
    <link href="http://andeper.cn/2017/03/19/%E5%9B%BE%E8%A7%A3TCPIP-TCP%E4%B8%8EUDP/"/>
    <id>http://andeper.cn/2017/03/19/图解TCPIP-TCP与UDP/</id>
    <published>2017-03-19T14:06:00.000Z</published>
    <updated>2017-03-20T06:55:12.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="TCP与UDP"><a href="#TCP与UDP" class="headerlink" title="TCP与UDP"></a>TCP与UDP</h3><h4 id="传输层的作用"><a href="#传输层的作用" class="headerlink" title="传输层的作用"></a>传输层的作用</h4><h5 id="传输层定义"><a href="#传输层定义" class="headerlink" title="传输层定义"></a>传输层定义</h5><p>IP首部有一个协议字段，用来标识网络层的上一层所采用的是哪一种传输层协议</p><h4 id=""><a href="#" class="headerlink" title="#"></a>#</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;TCP与UDP&quot;&gt;&lt;a href=&quot;#TCP与UDP&quot; class=&quot;headerlink&quot; title=&quot;TCP与UDP&quot;&gt;&lt;/a&gt;TCP与UDP&lt;/h3&gt;&lt;h4 id=&quot;传输层的作用&quot;&gt;&lt;a href=&quot;#传输层的作用&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
      <category term="读书笔记" scheme="http://andeper.cn/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>图解TCP/IP-IP协议相关技术</title>
    <link href="http://andeper.cn/2017/03/19/%E5%9B%BE%E8%A7%A3TCPIP-IP%E5%8D%8F%E8%AE%AE%E7%9B%B8%E5%85%B3%E6%8A%80%E6%9C%AF/"/>
    <id>http://andeper.cn/2017/03/19/图解TCPIP-IP协议相关技术/</id>
    <published>2017-03-18T16:28:00.000Z</published>
    <updated>2017-03-20T06:54:56.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="IP协议相关技术"><a href="#IP协议相关技术" class="headerlink" title="IP协议相关技术"></a>IP协议相关技术</h3><h4 id="仅凭IP无法完成通信"><a href="#仅凭IP无法完成通信" class="headerlink" title="仅凭IP无法完成通信"></a>仅凭IP无法完成通信</h4><h4 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h4><h5 id="IP地址不便记忆"><a href="#IP地址不便记忆" class="headerlink" title="IP地址不便记忆"></a>IP地址不便记忆</h5><h5 id="DNS的产生"><a href="#DNS的产生" class="headerlink" title="DNS的产生"></a>DNS的产生</h5><p>当用户输入域名时，DNS会自动检索那个注册了主机名和IP地址 的数据库，并迅速定位对应的IP地址</p><h5 id="域名的构成"><a href="#域名的构成" class="headerlink" title="域名的构成"></a>域名的构成</h5><p>域名的分成结构<br><strong>域名服务器</strong><br>域名服务器值管理域名的主机和相应软件，它可以管理所在分层的域的相关信息<br>其所管理的分层叫做ZONE<br>各个域的分层上都设有各自的域名服务器<br>各层域名服务器都了解以下分层所有域名服务器的IP地址，因此他们从根域名服务器开始呈树状结构相互连接<br>由于所有域名服务器都了解根域名服务器的IP地址，所以从根开始按照顺序追踪，可以访问世界上所有域名服务器的地址<br><strong>根据DNS协议，根域名服务器可以由13个IP地址表示，并且从A到M开始，然后由于IP任播可以为多个结点设置同一个IP地址，为了提高容灾能力和负载均衡能力</strong></p><h5 id="DNS查询"><a href="#DNS查询" class="headerlink" title="DNS查询"></a>DNS查询</h5><p>计算机pepper要访问www.ietf.org</p><ol><li>解析器为了调查IP地址，向域名服务器进行查询处理</li><li>接受到这个查询请求的域名服务器首先在自己的数据库进行查找，如果有则返回，没有则向上一级根域名服务器进行查询<h5 id="DNS如同互联网中的分布式数据库"><a href="#DNS如同互联网中的分布式数据库" class="headerlink" title="DNS如同互联网中的分布式数据库"></a>DNS如同互联网中的分布式数据库</h5><h4 id="ARP"><a href="#ARP" class="headerlink" title="ARP"></a>ARP</h4><h5 id="ARP概要"><a href="#ARP概要" class="headerlink" title="ARP概要"></a>ARP概要</h5>ARP是一种解决地址问题的协议，用来定位下一个应该接受数据分包的网络设备对应的MAC地址<h5 id="ARP的工作机制"><a href="#ARP的工作机制" class="headerlink" title="ARP的工作机制"></a>ARP的工作机制</h5>假定主机A要向主机B发送IP包，主机A的IP地址为172.20.1.1，主机B的IP地址为172.20.1.2<br>主机A为了获得主机B的MAC地址，起初要通过广播发送一个ARP请求，这个包包含了主机的IP地址<br>广播的包可以被同意链路上所有主机或路由器接受，所有主机都会解析这个包，只有当这个包中IP地址和自己的IP地址一致，那么这个结点就将自己的MAC地址塞进ARP响应返回给主机A<h5 id="IP地址和MAC地址缺一不可？"><a href="#IP地址和MAC地址缺一不可？" class="headerlink" title="IP地址和MAC地址缺一不可？"></a>IP地址和MAC地址缺一不可？</h5>数据链路上只要知道接收端的MAC地址不久知道数据是准备发给主机B的吗，那么还需要知道它的IP地址吗？<br>主机A想发送数据报给主机B时必须得经过路由器C，即时知道B的MAC地址没有用，路由器C会隔断来两个网络，此时主机A要先得到C的MAC地址<h5 id="RARP"><a href="#RARP" class="headerlink" title="RARP"></a>RARP</h5>从MAC地址定位到IP地址的一种协议，将打印机等小型嵌入式设备连接到网络会用到<h5 id="代理ARP"><a href="#代理ARP" class="headerlink" title="代理ARP"></a>代理ARP</h5><h4 id="ICMP"><a href="#ICMP" class="headerlink" title="ICMP"></a>ICMP</h4><h5 id="辅助IP的ICMP"><a href="#辅助IP的ICMP" class="headerlink" title="辅助IP的ICMP"></a>辅助IP的ICMP</h5>主要功能：</li><li>确认IP包是否成功送达目标地址</li><li>通知在发送过程中IP包被废弃的具体原因</li><li>改善网络设置<br>ICMP只负责其中IP相关的设置<br>ICMP这种通知消息会使用IP进行发送<br>ICMP的消息大致分为两类：</li><li>通知出错原因的错误消息</li><li>用于诊断的查询消息<h5 id="主要的ICMP消息"><a href="#主要的ICMP消息" class="headerlink" title="主要的ICMP消息"></a>主要的ICMP消息</h5><strong>ICMP目标不可达消息</strong><br><strong>ICMP重定向消息</strong><br><strong>ICMP超时消息</strong><br><strong>ICMP回送消息</strong><h5 id="其他ICMP消息"><a href="#其他ICMP消息" class="headerlink" title="其他ICMP消息"></a>其他ICMP消息</h5><strong>ICMP原点抑制消息</strong><br><strong>ICMP路由探索消息</strong><br><strong>ICMP地址掩码消息</strong><h5 id="ICMPv6"><a href="#ICMPv6" class="headerlink" title="ICMPv6"></a>ICMPv6</h5><strong>ICMPv6的作用</strong><br><strong>邻居探索</strong><h4 id="NAT"><a href="#NAT" class="headerlink" title="NAT"></a>NAT</h4><h5 id="NAT定义"><a href="#NAT定义" class="headerlink" title="NAT定义"></a>NAT定义</h5>Network Address Translator是用于本地网络的私有协议，在连接互联网时转而使用全局IP地址的技术，除转换IP地址外，还出现了可以转换TCP，UDP端口号的NAPT技术，由此可以实现用一个全局IP地址与多个主机通信<h5 id="NAT的工作机制"><a href="#NAT的工作机制" class="headerlink" title="NAT的工作机制"></a>NAT的工作机制</h5><h5 id="NAT-PT"><a href="#NAT-PT" class="headerlink" title="NAT-PT"></a>NAT-PT</h5><h5 id="NAT的潜在问题"><a href="#NAT的潜在问题" class="headerlink" title="NAT的潜在问题"></a>NAT的潜在问题</h5><h5 id="解决NAT的潜在问题与NAT穿越"><a href="#解决NAT的潜在问题与NAT穿越" class="headerlink" title="解决NAT的潜在问题与NAT穿越"></a>解决NAT的潜在问题与NAT穿越</h5><h4 id="IP隧道"><a href="#IP隧道" class="headerlink" title="IP隧道"></a>IP隧道</h4><img src="http://omjy3y3o5.bkt.clouddn.com/543a8ec0cfe6ab2940a9881cee2e0685.png" alt="ip"><br>在如图所示的网络环境中，网络A、B使用IPv6，如果处于中间位置的网络C支持使用IPv4的话，网络A与网络B之间将无法直接进行通信，这时候就需要采用IP隧道的功能<br>IP隧道可以降那些从网络A发过来的IPv6的包统和为一个数据，再为之追加一个IPv6的首部以后转发给网络C<br>一般情况下，紧接着IP首部的是TCP或UDP的首部，然而现在的应用中IP首部后面还是IP首部，这种网络层首部后面继续追加网络层首部的通信方法叫做IP隧道<h4 id="其他IP相关技术"><a href="#其他IP相关技术" class="headerlink" title="其他IP相关技术"></a>其他IP相关技术</h4><h5 id="IP多播相关技术"><a href="#IP多播相关技术" class="headerlink" title="IP多播相关技术"></a>IP多播相关技术</h5><h5 id="IP任播"><a href="#IP任播" class="headerlink" title="IP任播"></a>IP任播</h5><h5 id="通信质量控制"><a href="#通信质量控制" class="headerlink" title="通信质量控制"></a>通信质量控制</h5><h5 id="显式拥塞通知"><a href="#显式拥塞通知" class="headerlink" title="显式拥塞通知"></a>显式拥塞通知</h5><h5 id="Mobile-IP"><a href="#Mobile-IP" class="headerlink" title="Mobile IP"></a>Mobile IP</h5></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;IP协议相关技术&quot;&gt;&lt;a href=&quot;#IP协议相关技术&quot; class=&quot;headerlink&quot; title=&quot;IP协议相关技术&quot;&gt;&lt;/a&gt;IP协议相关技术&lt;/h3&gt;&lt;h4 id=&quot;仅凭IP无法完成通信&quot;&gt;&lt;a href=&quot;#仅凭IP无法完成通信&quot; class=&quot;
      
    
    </summary>
    
    
      <category term="读书笔记" scheme="http://andeper.cn/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Android开发艺术探索-理解Window和WindowManager</title>
    <link href="http://andeper.cn/2017/03/18/Android%E5%BC%80%E5%8F%91%E8%89%BA%E6%9C%AF%E6%8E%A2%E7%B4%A2-%E7%90%86%E8%A7%A3Window%E5%92%8CWindowManager/"/>
    <id>http://andeper.cn/2017/03/18/Android开发艺术探索-理解Window和WindowManager/</id>
    <published>2017-03-18T08:53:00.000Z</published>
    <updated>2017-03-20T06:55:38.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="第八章：理解Window和WindowManager"><a href="#第八章：理解Window和WindowManager" class="headerlink" title="第八章：理解Window和WindowManager"></a>第八章：理解Window和WindowManager</h3><p>Window表示一个窗口的概念。Window是一个抽象类，它的具体实现是PhoneWindow，创建一个Window在WindowManager中完成，但Window的具体实现是在WindowManagerService中，WindowManager和WindowManagerService的交互是一个IPC过程。Andorid的所有视图都是通过Window来呈现的</p><h4 id="Window和WindowManager"><a href="#Window和WindowManager" class="headerlink" title="Window和WindowManager"></a>Window和WindowManager</h4><p>下面代码通过WindowManager添加一个Window<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Button mFloatingButton = <span class="keyword">new</span> Button(<span class="keyword">this</span>);</span><br><span class="line">mFloatingButton.setText(<span class="string">"button"</span>);</span><br><span class="line">WindowManager.LayoutParams mLayoutParams = <span class="keyword">new</span> WindowManager.LayoutParams(WindowManager.LayoutParams.WRAP_CONTENT, WindowManager.LayoutParams.WRAP_CONTENT,<span class="number">0</span>,<span class="number">0</span>, PixelFormat.TRANSPARENT);</span><br><span class="line">mLayoutParams.flags = WindowManager.LayoutParams.FLAG_NOT_FOCUSABLE| WindowManager.LayoutParams.FLAG_NOT_TOUCH_MODAL| WindowManager.LayoutParams.FLAG_SHOW_WHEN_LOCKED;</span><br><span class="line">mLayoutParams.gravity = Gravity.LEFT|Gravity.TOP;</span><br><span class="line">mLayoutParams.x = <span class="number">100</span>;</span><br><span class="line">mLayoutParams.y = <span class="number">300</span>;</span><br><span class="line">WindowManager windowManager = getWindowManager();</span><br><span class="line">windowManager.addView(mFloatingButton,mLayoutParams);</span><br></pre></td></tr></table></figure></p><p>上述代码将Button添加到屏幕坐标为（100，300）的位置上，WindowManager.LayoutParams中的flag和type参数比较重要<br>Flags参数代表Window的属性<br>FLAG_NOT_FOCUSABLE<br>表示Window不需要获取焦点，也不需要接收各种输入事件，此标记会同时启用FLAG_NOT_TOUCH_MODAL。最终事件会直接传递给下层的具有焦点的Window<br>FLAG_NOT_TOUCH_MODAL<br>再次模式下，系统会将当前Window区域以外的单击事件传递给底层的Window，当前Window区域以内的单击事件则自己处理，一般需要开启，否则其他Window将无法收到单击事件<br>FLAG_SHOW_WHEN_LOCKED<br>开启此模式可以让Window显示在锁屏的界面上</p><p>Type参数表示Window的类型，Window有三种类型，分别是应用Window、子Window和系统Window<br>Window是分层的，每个WIndow都有对应的z-ordered，在三类Window中，应用Window的层级范围是1-99，子Window的范围哦是1000-1999，系统Window的层级范围是2000-2999<br>WindowManager只有三个功能。定义在ViewManager中<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ViewManager</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addView</span><span class="params">(View view, ViewGroup.LayoutParams params)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">updateViewLayout</span><span class="params">(View view, ViewGroup.LayoutParams params)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">removeView</span><span class="params">(View view)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>WindowManager操作Window更像是在操作Window中的View</p><h4 id="Window的内部机制"><a href="#Window的内部机制" class="headerlink" title="Window的内部机制"></a>Window的内部机制</h4><p>每个Window都对应着一个View和一个ViewRootImpl，Window和View通过ViewRootImpl来建立联系，Window是一个抽象的概念，View才是Window存在的实体。</p><h5 id="Window的添加过程"><a href="#Window的添加过程" class="headerlink" title="Window的添加过程"></a>Window的添加过程</h5><p>WindowManager的实现类WindowManagerImpl没有实现Window的三大操作，而是交给WindowManagerGlobal来处理，addView方法主要分为以下几步</p><h6 id="检查参数是否合法，如果是子Window还需要调整布局参数"><a href="#检查参数是否合法，如果是子Window还需要调整布局参数" class="headerlink" title="检查参数是否合法，如果是子Window还需要调整布局参数"></a>检查参数是否合法，如果是子Window还需要调整布局参数</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (view == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"view must not be null"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (display == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"display must not be null"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (!(params <span class="keyword">instanceof</span> WindowManager.LayoutParams)) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Params must be WindowManager.LayoutParams"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> WindowManager.LayoutParams wparams = (WindowManager.LayoutParams) params;</span><br><span class="line">        <span class="keyword">if</span> (parentWindow != <span class="keyword">null</span>) &#123;</span><br><span class="line">            parentWindow.adjustLayoutParamsForSubWindow(wparams);</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h6 id="创建ViewRootImpl并将View添加到列表中"><a href="#创建ViewRootImpl并将View添加到列表中" class="headerlink" title="创建ViewRootImpl并将View添加到列表中"></a>创建ViewRootImpl并将View添加到列表中</h6><p>WindowManagerGlobal内部下面几个列表比较重要<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> ArrayList&lt;View&gt; mViews = <span class="keyword">new</span> ArrayList&lt;View&gt;();</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> ArrayList&lt;ViewRootImpl&gt; mRoots = <span class="keyword">new</span> ArrayList&lt;ViewRootImpl&gt;();</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> ArrayList&lt;WindowManager.LayoutParams&gt; mParams =</span><br><span class="line">            <span class="keyword">new</span> ArrayList&lt;WindowManager.LayoutParams&gt;();</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> ArraySet&lt;View&gt; mDyingViews = <span class="keyword">new</span> ArraySet&lt;View&gt;();</span><br></pre></td></tr></table></figure></p><p>mViews存储所有Window对应的View<br>mRoots存储的是所有Window对应的ViewRootImpl<br>mParams存储所有对应的布局参数<br>mDyingViews则存储那些正在被删除的View对象。<br>addView通过如下方式将Window的一系列对象添加到列表中<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root = <span class="keyword">new</span> ViewRootImpl(view.getContext(), display);</span><br><span class="line">view.setLayoutParams(wparams);</span><br><span class="line">mViews.add(view);</span><br><span class="line">mRoots.add(root);</span><br><span class="line">mParams.add(wparams);</span><br></pre></td></tr></table></figure></p><h6 id="通过ViewRootImpl来更新界面并完成Window的添加过程"><a href="#通过ViewRootImpl来更新界面并完成Window的添加过程" class="headerlink" title="通过ViewRootImpl来更新界面并完成Window的添加过程"></a>通过ViewRootImpl来更新界面并完成Window的添加过程</h6><p>由ViewRootImpl的setView方法来完成.<br>View的绘制过程由ViewRootImpl的setView完成的<br>在setView内部会通过requestLayout来完成异步刷新请求的<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">requestLayout</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!mHandlingLayoutInLayoutRequest) &#123;</span><br><span class="line">            checkThread();</span><br><span class="line">            mLayoutRequested = <span class="keyword">true</span>;</span><br><span class="line">            scheduleTraversals();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p>scheduleTraversals是View绘制的入口，接着会通过WindowSession最终来完成Window的添加过程<br>mWindowSession的类型是IWindowSession，它是一个Binder对象，真正的实现类是Session，也就是Window的添加过程是一次IPC调用<br>在Session内部会通过WindowManagerService来实现Window的添加</p><h5 id="Window的删除过程"><a href="#Window的删除过程" class="headerlink" title="Window的删除过程"></a>Window的删除过程</h5><p>真正删除View 的逻辑在dispatchDetachedFromWindow，重要做四件事情</p><ol><li>垃圾回收相关工作，比如清除数据和消息，移除回调</li><li>通过Session的remove方法删除View，这是一个IPC过程，最终会调用WindowManagerService的removeWindow方法</li><li>调用View的dispatchDetachedFromWindow调用回调，让用户在回调方法里做资源回收工作</li><li>调用WindowManagerGlobal的doRemoveView方法刷新数据，包括mRoots、mParams、以及mDyingViews</li></ol><h5 id="Window的更新过程"><a href="#Window的更新过程" class="headerlink" title="Window的更新过程"></a>Window的更新过程</h5><p>调用WindowManagerGlobal的updateViewLayout方法，首先更新View的LayoutParams并替换老的LayoutParams，接着再更新ViewRootImpl中的LayoutParams</p><h4 id="Window的创建过程"><a href="#Window的创建过程" class="headerlink" title="Window的创建过程"></a>Window的创建过程</h4><p>View是Android中的师徒的呈现方式，View必须附着在Window这个抽象的概念上面</p><h5 id="Activity的Window创建过程"><a href="#Activity的Window创建过程" class="headerlink" title="Activity的Window创建过程"></a>Activity的Window创建过程</h5><p>PhoneWindow的setContentView方法大致遵循如下几个步骤<br>1.如果没有DecorView，那么创建它<br>DecorView算是一个FrameLayout，它包含标题栏和内容栏，DecorView的创建过程由installDecor方法来完成，再方法内部会通过generateDecor方法来直接创建DecorView，这个时候DecorView还是一个空白的FrameLayout。<br>2.将View添加到DecorView的mContentParent中<br>3.回调Activity的onContentChanged方法通过Activity视图已经发生改变</p><h5 id="Dialog的Window创建过程"><a href="#Dialog的Window创建过程" class="headerlink" title="Dialog的Window创建过程"></a>Dialog的Window创建过程</h5><p>1.创建Window<br>2.初始化DecorView并将Dialog的视图添加到DecorView中<br>3.将DecorView添加到Window中并显示</p><h5 id="Toast的Window创建过程"><a href="#Toast的Window创建过程" class="headerlink" title="Toast的Window创建过程"></a>Toast的Window创建过程</h5><p>再Toast内部有两类IPC过程，第一类是Toast访问NotificationManagerService，第二类是NotificationManagerService回调Toast 的TN接口</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;第八章：理解Window和WindowManager&quot;&gt;&lt;a href=&quot;#第八章：理解Window和WindowManager&quot; class=&quot;headerlink&quot; title=&quot;第八章：理解Window和WindowManager&quot;&gt;&lt;/a&gt;第八章：理解Wi
      
    
    </summary>
    
    
      <category term="读书笔记" scheme="http://andeper.cn/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Android开发艺术探索-消息机制</title>
    <link href="http://andeper.cn/2017/03/15/Android%E5%BC%80%E5%8F%91%E8%89%BA%E6%9C%AF%E6%8E%A2%E7%B4%A2-%E6%B6%88%E6%81%AF%E6%9C%BA%E5%88%B6/"/>
    <id>http://andeper.cn/2017/03/15/Android开发艺术探索-消息机制/</id>
    <published>2017-03-15T02:51:00.000Z</published>
    <updated>2017-03-20T06:55:56.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="消息机制概述"><a href="#消息机制概述" class="headerlink" title="消息机制概述"></a>消息机制概述</h3><p>Android访问UI只能在主线程中进行，如果在子线程访问ui就会抛出异常，这个验证工作是在ViewRootImpl中的checkThread中进行，</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">checkThread</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (mThread != Thread.currentThread()) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> CalledFromWrongThreadException(</span><br><span class="line">                    <span class="string">"Only the original thread that created a view hierarchy can touch its views."</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>系统为什么不允许在子线程访问UI呢？<br>因为Android的ui线程不是线程安全的<br>那么系统为什么不对UI控件的访问加上锁机制呢？<br>加上锁机制会让UI访问的逻辑变的复杂<br>锁机制会降低UI访问的效率</p><p>Handler工作原理<br>Handler创建时会采用当前线程的Looper来构建内部的消息循环系统，如果当前线程没有Looper，那么就会报错<br>Handler创建完毕后，内部的Looper以及MessageQueue就可以和Handler一起协同工作了，然后通过Handler的post方法将一个Runnable投递到Handler内部的Looper中去处理，也可以通过Handler的send方法发送一个消息，这个消息同样会在Looper中去处理。<br>当Hnadler的sent方法被调用时，它会调用MessageQueue的enqueneMessage方法把这个消息放在消息队列中，然后Looper发现有新消息到来时，就会处理这个消息，最终消息中的Runnable或者Handler的handleMessage方法就会被调用。Looper是运行在创建Handler所在的线程中的，这样以来Handler中的业务逻辑就被切换到创建Handler所在的线程中去执行了</p><h3 id="消息机制分析"><a href="#消息机制分析" class="headerlink" title="消息机制分析"></a>消息机制分析</h3><h4 id="ThreadLocal的工作原理"><a href="#ThreadLocal的工作原理" class="headerlink" title="ThreadLocal的工作原理"></a>ThreadLocal的工作原理</h4><p>ThreadLocal是一个线程内部的数据存储类<br>当某些数据是以线程为作用域并且不同线程具有不同的数据副本的时候，就可以考虑使用ThreadLocal。<br>比如对于Handler，Looper的作用域就是线程且不同线程有不同的Looper<br>ThreadLocal的另一个使用场景就是复杂逻辑下的对象传递<br>具体使用如下代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> ThreadLocal&lt;Boolean&gt; mBooleanThreadLocal = <span class="keyword">new</span> ThreadLocal&lt;&gt;();</span><br><span class="line">mBooleanThreadLocal.set(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="string">"thread 1"</span>)&#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">super</span>.run();</span><br><span class="line">                mBooleanThreadLocal.set(<span class="keyword">false</span>);</span><br><span class="line">                Log.e(<span class="string">"tag"</span>,mBooleanThreadLocal.get().toString());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;.start();</span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="string">"thread 2"</span>)&#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">super</span>.run();</span><br><span class="line">                Log.e(<span class="string">"tag"</span>,mBooleanThreadLocal.get().toString());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;.start();</span><br></pre></td></tr></table></figure><p><img src="http://omjy3y3o5.bkt.clouddn.com/f51a5a81197601e2d818ab7fcf2fe0d1.png" alt="result"><br>不同线程访问的是同一个ThreadLocal对象，但他们通过ThreadLocal获取到的值是不一样的<br>不同线程访问同一个ThreadLocal的get方法，ThreadLocal内部会从各自的线程取出一个数组，然后再从数组中根据当前ThreadLocal的索引去查找出对应的value值</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">set</span><span class="params">(T value)</span> </span>&#123;</span><br><span class="line">        Thread t = Thread.currentThread();</span><br><span class="line">        ThreadLocalMap map = getMap(t);</span><br><span class="line">        <span class="keyword">if</span> (map != <span class="keyword">null</span>)</span><br><span class="line">            map.set(<span class="keyword">this</span>, value);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            createMap(t, value);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>这里源码和书上讲的不一样，数据其实是存在ThreadLocalMap中<br>住上的结论是他们操作的对象都是当前线程的localValues对象的table数组</p><h4 id="消息队列的工作原理"><a href="#消息队列的工作原理" class="headerlink" title="消息队列的工作原理"></a>消息队列的工作原理</h4><p>消息队列值得是MessageQueue，<br>MessageQueue包含两个操作：插入和读取<br>插入和读取方法分别为enqueneMessage和next</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">enqueueMessage</span><span class="params">(Message msg, <span class="keyword">long</span> when)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (msg.target == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Message must have a target."</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (msg.isInUse()) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalStateException(msg + <span class="string">" This message is already in use."</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (mQuitting) &#123;</span><br><span class="line">                IllegalStateException e = <span class="keyword">new</span> IllegalStateException(</span><br><span class="line">                        msg.target + <span class="string">" sending message to a Handler on a dead thread"</span>);</span><br><span class="line">                Log.w(TAG, e.getMessage(), e);</span><br><span class="line">                msg.recycle();</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            msg.markInUse();</span><br><span class="line">            msg.when = when;</span><br><span class="line">            Message p = mMessages;</span><br><span class="line">            <span class="keyword">boolean</span> needWake;</span><br><span class="line">            <span class="keyword">if</span> (p == <span class="keyword">null</span> || when == <span class="number">0</span> || when &lt; p.when) &#123;</span><br><span class="line">                <span class="comment">// New head, wake up the event queue if blocked.</span></span><br><span class="line">                msg.next = p;</span><br><span class="line">                mMessages = msg;</span><br><span class="line">                needWake = mBlocked;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// Inserted within the middle of the queue.  Usually we don't have to wake</span></span><br><span class="line">                <span class="comment">// up the event queue unless there is a barrier at the head of the queue</span></span><br><span class="line">                <span class="comment">// and the message is the earliest asynchronous message in the queue.</span></span><br><span class="line">                needWake = mBlocked &amp;&amp; p.target == <span class="keyword">null</span> &amp;&amp; msg.isAsynchronous();</span><br><span class="line">                Message prev;</span><br><span class="line">                <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">                    prev = p;</span><br><span class="line">                    p = p.next;</span><br><span class="line">                    <span class="keyword">if</span> (p == <span class="keyword">null</span> || when &lt; p.when) &#123;</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span> (needWake &amp;&amp; p.isAsynchronous()) &#123;</span><br><span class="line">                        needWake = <span class="keyword">false</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                msg.next = p; <span class="comment">// invariant: p == prev.next</span></span><br><span class="line">                prev.next = msg;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// We can assume mPtr != 0 because mQuitting is false.</span></span><br><span class="line">            <span class="keyword">if</span> (needWake) &#123;</span><br><span class="line">                nativeWake(mPtr);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>主要操作其实是单链表的插入操作</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Message <span class="title">next</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// Return here if the message loop has already quit and been disposed.</span></span><br><span class="line">        <span class="comment">// This can happen if the application tries to restart a looper after quit</span></span><br><span class="line">        <span class="comment">// which is not supported.</span></span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">long</span> ptr = mPtr;</span><br><span class="line">        <span class="keyword">if</span> (ptr == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> pendingIdleHandlerCount = -<span class="number">1</span>; <span class="comment">// -1 only during first iteration</span></span><br><span class="line">        <span class="keyword">int</span> nextPollTimeoutMillis = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nextPollTimeoutMillis != <span class="number">0</span>) &#123;</span><br><span class="line">                Binder.flushPendingCommands();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            nativePollOnce(ptr, nextPollTimeoutMillis);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">                <span class="comment">// Try to retrieve the next message.  Return if found.</span></span><br><span class="line">                <span class="keyword">final</span> <span class="keyword">long</span> now = SystemClock.uptimeMillis();</span><br><span class="line">                Message prevMsg = <span class="keyword">null</span>;</span><br><span class="line">                Message msg = mMessages;</span><br><span class="line">                <span class="keyword">if</span> (msg != <span class="keyword">null</span> &amp;&amp; msg.target == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="comment">// Stalled by a barrier.  Find the next asynchronous message in the queue.</span></span><br><span class="line">                    <span class="keyword">do</span> &#123;</span><br><span class="line">                        prevMsg = msg;</span><br><span class="line">                        msg = msg.next;</span><br><span class="line">                    &#125; <span class="keyword">while</span> (msg != <span class="keyword">null</span> &amp;&amp; !msg.isAsynchronous());</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (msg != <span class="keyword">null</span>) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (now &lt; msg.when) &#123;</span><br><span class="line">                        <span class="comment">// Next message is not ready.  Set a timeout to wake up when it is ready.</span></span><br><span class="line">                        nextPollTimeoutMillis = (<span class="keyword">int</span>) Math.min(msg.when - now, Integer.MAX_VALUE);</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        <span class="comment">// Got a message.</span></span><br><span class="line">                        mBlocked = <span class="keyword">false</span>;</span><br><span class="line">                        <span class="keyword">if</span> (prevMsg != <span class="keyword">null</span>) &#123;</span><br><span class="line">                            prevMsg.next = msg.next;</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            mMessages = msg.next;</span><br><span class="line">                        &#125;</span><br><span class="line">                        msg.next = <span class="keyword">null</span>;</span><br><span class="line">                        <span class="keyword">if</span> (DEBUG) Log.v(TAG, <span class="string">"Returning message: "</span> + msg);</span><br><span class="line">                        msg.markInUse();</span><br><span class="line">                        <span class="keyword">return</span> msg;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="comment">// No more messages.</span></span><br><span class="line">                    nextPollTimeoutMillis = -<span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Process the quit message now that all pending messages have been handled.</span></span><br><span class="line">                <span class="keyword">if</span> (mQuitting) &#123;</span><br><span class="line">                    dispose();</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// If first time idle, then get the number of idlers to run.</span></span><br><span class="line">                <span class="comment">// Idle handles only run if the queue is empty or if the first message</span></span><br><span class="line">                <span class="comment">// in the queue (possibly a barrier) is due to be handled in the future.</span></span><br><span class="line">                <span class="keyword">if</span> (pendingIdleHandlerCount &lt; <span class="number">0</span></span><br><span class="line">                        &amp;&amp; (mMessages == <span class="keyword">null</span> || now &lt; mMessages.when)) &#123;</span><br><span class="line">                    pendingIdleHandlerCount = mIdleHandlers.size();</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (pendingIdleHandlerCount &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="comment">// No idle handlers to run.  Loop and wait some more.</span></span><br><span class="line">                    mBlocked = <span class="keyword">true</span>;</span><br><span class="line">                    <span class="keyword">continue</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (mPendingIdleHandlers == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    mPendingIdleHandlers = <span class="keyword">new</span> IdleHandler[Math.max(pendingIdleHandlerCount, <span class="number">4</span>)];</span><br><span class="line">                &#125;</span><br><span class="line">                mPendingIdleHandlers = mIdleHandlers.toArray(mPendingIdleHandlers);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Run the idle handlers.</span></span><br><span class="line">            <span class="comment">// We only ever reach this code block during the first iteration.</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; pendingIdleHandlerCount; i++) &#123;</span><br><span class="line">                <span class="keyword">final</span> IdleHandler idler = mPendingIdleHandlers[i];</span><br><span class="line">                mPendingIdleHandlers[i] = <span class="keyword">null</span>; <span class="comment">// release the reference to the handler</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">boolean</span> keep = <span class="keyword">false</span>;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    keep = idler.queueIdle();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">                    Log.wtf(TAG, <span class="string">"IdleHandler threw exception"</span>, t);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (!keep) &#123;</span><br><span class="line">                    <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">                        mIdleHandlers.remove(idler);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Reset the idle handler count to 0 so we do not run them again.</span></span><br><span class="line">            pendingIdleHandlerCount = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// While calling an idle handler, a new message could have been delivered</span></span><br><span class="line">            <span class="comment">// so go back and look again for a pending message without waiting.</span></span><br><span class="line">            nextPollTimeoutMillis = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>next是一个无限循环的方法，如果消息队列没有消息，那么next方法会一直阻塞在这里，有新消息到来时，next方法返回这条消息并将其从单链表移除</p><h4 id="Looper的工作原理"><a href="#Looper的工作原理" class="headerlink" title="Looper的工作原理"></a>Looper的工作原理</h4><p>Handler的工作需要Looper，没有Looper的线程会报错，通过Looper.prepared()即可为当前线程创建一个Looper，接着通过Looper.Loop()来开启消息循环<br>loop方法如下</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">loop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Looper me = myLooper();</span><br><span class="line">        <span class="keyword">if</span> (me == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> RuntimeException(<span class="string">"No Looper; Looper.prepare() wasn't called on this thread."</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">final</span> MessageQueue queue = me.mQueue;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Make sure the identity of this thread is that of the local process,</span></span><br><span class="line">        <span class="comment">// and keep track of what that identity token actually is.</span></span><br><span class="line">        Binder.clearCallingIdentity();</span><br><span class="line">        <span class="keyword">final</span> <span class="keyword">long</span> ident = Binder.clearCallingIdentity();</span><br><span class="line">        <span class="comment">//不进入死循环，不断取对象</span></span><br><span class="line">        <span class="keyword">for</span> (;;) &#123;</span><br><span class="line">            Message msg = queue.next(); <span class="comment">// might block</span></span><br><span class="line">            <span class="keyword">if</span> (msg == <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">// No message indicates that the message queue is quitting.</span></span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// This must be in a local variable, in case a UI event sets the logger</span></span><br><span class="line">            <span class="keyword">final</span> Printer logging = me.mLogging;</span><br><span class="line">            <span class="keyword">if</span> (logging != <span class="keyword">null</span>) &#123;</span><br><span class="line">                logging.println(<span class="string">"&gt;&gt;&gt;&gt;&gt; Dispatching to "</span> + msg.target + <span class="string">" "</span> +</span><br><span class="line">                        msg.callback + <span class="string">": "</span> + msg.what);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">final</span> <span class="keyword">long</span> traceTag = me.mTraceTag;</span><br><span class="line">            <span class="keyword">if</span> (traceTag != <span class="number">0</span>) &#123;</span><br><span class="line">                Trace.traceBegin(traceTag, msg.target.getTraceName(msg));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                msg.target.dispatchMessage(msg);</span><br><span class="line">            &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (traceTag != <span class="number">0</span>) &#123;</span><br><span class="line">                    Trace.traceEnd(traceTag);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (logging != <span class="keyword">null</span>) &#123;</span><br><span class="line">                logging.println(<span class="string">"&lt;&lt;&lt;&lt;&lt; Finished to "</span> + msg.target + <span class="string">" "</span> + msg.callback);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Make sure that during the course of dispatching the</span></span><br><span class="line">            <span class="comment">// identity of the thread wasn't corrupted.</span></span><br><span class="line">            <span class="keyword">final</span> <span class="keyword">long</span> newIdent = Binder.clearCallingIdentity();</span><br><span class="line">            <span class="keyword">if</span> (ident != newIdent) &#123;</span><br><span class="line">                Log.wtf(TAG, <span class="string">"Thread identity changed from 0x"</span></span><br><span class="line">                        + Long.toHexString(ident) + <span class="string">" to 0x"</span></span><br><span class="line">                        + Long.toHexString(newIdent) + <span class="string">" while dispatching to "</span></span><br><span class="line">                        + msg.target.getClass().getName() + <span class="string">" "</span></span><br><span class="line">                        + msg.callback + <span class="string">" what="</span> + msg.what);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            msg.recycleUnchecked();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h4 id="Handler的工作原理"><a href="#Handler的工作原理" class="headerlink" title="Handler的工作原理"></a>Handler的工作原理</h4><p>Handler的工作主要包含消息的发送和接受过程</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">sendMessage</span><span class="params">(Message msg)</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sendMessageDelayed(msg, <span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">boolean</span> <span class="title">sendMessageDelayed</span><span class="params">(Message msg, <span class="keyword">long</span> delayMillis)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (delayMillis &lt; <span class="number">0</span>) &#123;</span><br><span class="line">            delayMillis = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> sendMessageAtTime(msg, SystemClock.uptimeMillis() + delayMillis);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">sendMessageAtTime</span><span class="params">(Message msg, <span class="keyword">long</span> uptimeMillis)</span> </span>&#123;</span><br><span class="line">       MessageQueue queue = mQueue;</span><br><span class="line">       <span class="keyword">if</span> (queue == <span class="keyword">null</span>) &#123;</span><br><span class="line">           RuntimeException e = <span class="keyword">new</span> RuntimeException(</span><br><span class="line">                   <span class="keyword">this</span> + <span class="string">" sendMessageAtTime() called with no mQueue"</span>);</span><br><span class="line">           Log.w(<span class="string">"Looper"</span>, e.getMessage(), e);</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> enqueueMessage(queue, msg, uptimeMillis);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>Hadnler发送消息只是向消息队列插入了一条消息，最终消息由Looper交由Handler处理，即Handler的dispatchMessage会被调用<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">dispatchMessage</span><span class="params">(Message msg)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (msg.callback != <span class="keyword">null</span>) &#123;</span><br><span class="line">            handleCallback(msg);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (mCallback != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">if</span> (mCallback.handleMessage(msg)) &#123;</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            handleMessage(msg);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p><p>消息处理流程图如下：<br><img src="http://omjy3y3o5.bkt.clouddn.com/7d2884b24e9ebc2cae44415c4de6e17a.png" alt="流程图"></p><h3 id="主线程的消息循环"><a href="#主线程的消息循环" class="headerlink" title="主线程的消息循环"></a>主线程的消息循环</h3><p>主线程就是ActivityThread，入口方法为main，在main方法中会通过Looper.prepareMainLooper()来创建主线程的Looper以及MessageQueue<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Looper.prepareMainLooper();</span><br><span class="line">ActivityThread thread = <span class="keyword">new</span> ActivityThread();</span><br><span class="line">thread.attach(<span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (sMainThreadHandler == <span class="keyword">null</span>) &#123;</span><br><span class="line">    sMainThreadHandler = thread.getHandler();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (<span class="keyword">false</span>) &#123;</span><br><span class="line">    Looper.myLooper().setMessageLogging(<span class="keyword">new</span></span><br><span class="line">            LogPrinter(Log.DEBUG, <span class="string">"ActivityThread"</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// End of event ActivityThreadMain.</span></span><br><span class="line">Trace.traceEnd(Trace.TRACE_TAG_ACTIVITY_MANAGER);</span><br><span class="line">Looper.loop();</span><br></pre></td></tr></table></figure></p><p>总结<br>当我们调用handler.sendMessage(msg)方法发送一个Message时，实际上这个Message是发送到与当前线程绑定的一个MessageQueue中，然后与当前线程绑定的Looper将会不断的从MessageQueue中取出新的Message，调用msg.target.dispathMessage(msg)方法将消息分发到与Message绑定的handler.handleMessage()方法中。<br>一个Thread对应多个Handler 一个Thread对应一个Looper和MessageQueue，Handler与Thread共享Looper和MessageQueue。 Message只是消息的载体，将会被发送到与线程绑定的唯一的MessageQueue中，并且被与线程绑定的唯一的Looper分发，被与其自身绑定的Handler消费。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;消息机制概述&quot;&gt;&lt;a href=&quot;#消息机制概述&quot; class=&quot;headerlink&quot; title=&quot;消息机制概述&quot;&gt;&lt;/a&gt;消息机制概述&lt;/h3&gt;&lt;p&gt;Android访问UI只能在主线程中进行，如果在子线程访问ui就会抛出异常，这个验证工作是在ViewRoot
      
    
    </summary>
    
    
      <category term="读书笔记" scheme="http://andeper.cn/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>基础面试题</title>
    <link href="http://andeper.cn/2017/03/14/%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    <id>http://andeper.cn/2017/03/14/基础面试题/</id>
    <published>2017-03-14T12:30:00.000Z</published>
    <updated>2017-03-20T06:52:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>1.TCP三次握手、四次挥手<br>2.为什么建立连接是三次，而断开连接要四次？<br>3.HTTP POST 和 GET的区别和联系。<br>4.进程和线程的区别。<br>5.操作系统内存管理。<br>6.进程间如何通信<br>7.操作系统中有几种状态</p><h5 id="8-从下往上说一下OSI七个分层？"><a href="#8-从下往上说一下OSI七个分层？" class="headerlink" title="8.从下往上说一下OSI七个分层？"></a>8.从下往上说一下OSI七个分层？</h5><p>物理层、数据链路层、网络层、传输层、会话层、表示层、应用层。</p><h5 id="TCP、UDP属于哪个层？"><a href="#TCP、UDP属于哪个层？" class="headerlink" title="TCP、UDP属于哪个层？"></a>TCP、UDP属于哪个层？</h5><p>传输   </p><h5 id="FTP在哪个层？"><a href="#FTP在哪个层？" class="headerlink" title="FTP在哪个层？"></a>FTP在哪个层？</h5><p>应用<br>OSI分层 （7层）：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层。<br>TCP/IP分层（4层）：网络接口层、 网际层、运输层、 应用层。<br>五层协议（5层）：物理层、数据链路层、网络层、运输层、 应用层。<br>每一层的协议如下：<br>物理层：RJ45、CLOCK、IEEE802.3    （中继器，集线器，网关）<br>数据链路：PPP、FR、HDLC、VLAN、MAC  （网桥，交换机）<br>网络层：IP、ICMP、ARP、RARP、OSPF、IPX、RIP、IGRP、 （路由器）<br>传输层：TCP、UDP、SPX<br>会话层：NFS、SQL、NETBIOS、RPC<br>表示层：JPEG、MPEG、ASII<br>应用层：FTP、DNS、Telnet、SMTP、HTTP、WWW、NFS</p><h5 id="每一层的作用："><a href="#每一层的作用：" class="headerlink" title="每一层的作用："></a>每一层的作用：</h5><p>物理层：通过媒介传输比特,确定机械及电气规范（比特Bit）<br>数据链路层：将比特组装成帧和点到点的传递（帧Frame）<br>网络层：负责数据包从源到宿的传递和网际互连（包PackeT）<br>传输层：提供端到端的可靠报文传递和错误恢复（段Segment）<br>会话层：建立、管理和终止会话（会话协议数据单元SPDU）<br>表示层：对数据进行翻译、加密和压缩（表示协议数据单元PPDU）<br>应用层：允许访问OSI环境的手段（应用协议数据单元APDU）</p><h5 id="TCP和UDP的区别？"><a href="#TCP和UDP的区别？" class="headerlink" title="TCP和UDP的区别？"></a>TCP和UDP的区别？</h5><p>TCP提供面向连接的、可靠的数据流传输，而UDP提供的是非面向连接的、不可靠的数据流传输。（TCP发出去还会问候核实一下以确保安全;UDP发出去就不管了 ）<br>TCP传输单位称为TCP报文段，UDP传输单位称为用户数据报。<br>TCP注重数据安全性，UDP数据传输快，因为不需要连接等待，少了许多操作，但是其安全性却一般。</p><h5 id="TCP对应的协议"><a href="#TCP对应的协议" class="headerlink" title="TCP对应的协议:"></a>TCP对应的协议:</h5><p>（1）FTP：定义了文件传输协议，使用21端口。<br>（2）Telnet：一种用于远程登陆的端口，使用23端口，用户可以以自己的身份远程连接到计算机上，可提供基于DOS模式下的通信服务。<br>（3）SMTP：邮件传送协议，用于发送邮件。服务器开放的是25号端口。<br>（4）POP3：它是和SMTP对应，POP3用于接收邮件。POP3协议所用的是110端口。<br>（5）HTTP：是从Web服务器传输超文本到本地浏览器的传送协议。</p><h5 id="TDP对应的协议："><a href="#TDP对应的协议：" class="headerlink" title="TDP对应的协议："></a>TDP对应的协议：</h5><p>（1） DNS：用于域名解析服务，将域名地址转换为IP地址。DNS用的是53号端口。<br>（2） SNMP：简单网络管理协议，使用161号端口，是用来管理网络设备的。由于网络设备很多，无连接的服务就体现出其优势。<br>（3） TFTP(Trival File Transfer Protocal)，简单文件传输协议，该协议在端口69上使用UDP服务。</p><p>作者：微尘2017<br>链接：<a href="https://www.nowcoder.com/discuss/21845?type=2&amp;order=0&amp;pos=26&amp;page=1" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/21845?type=2&amp;order=0&amp;pos=26&amp;page=1</a><br>来源：牛客网</p><p>1.线程通信方式，以及它们的区别</p><pre><code>2.I/O多路复用3.内存管理，我回答的是jvm内存分配、垃圾回收4.TCP连接、断开状态转换，具体细节5.海量数据下，如何做随机抽样？蓄水池抽样（这里感觉面试官题目定义的不清楚）6.海量文本下，字符串匹配，如何利用索引加快查找？（这题真不会，后来查了一下，可能跟lucene有关）7.50亿URL，统计频率在前100位的URL</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1.TCP三次握手、四次挥手&lt;br&gt;2.为什么建立连接是三次，而断开连接要四次？&lt;br&gt;3.HTTP POST 和 GET的区别和联系。&lt;br&gt;4.进程和线程的区别。&lt;br&gt;5.操作系统内存管理。&lt;br&gt;6.进程间如何通信&lt;br&gt;7.操作系统中有几种状态&lt;/p&gt;
&lt;h5 i
      
    
    </summary>
    
    
      <category term="面试题" scheme="http://andeper.cn/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
    
  </entry>
  
</feed>
