<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="Andeper的个人博客" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！Semi-supervised Clustering for Short Text via Deep Representation Learning基于深度表示学习的短文本半监督聚类 AbstractIn this work, we propose a semi-supervised method for short text clus">
<meta property="og:type" content="article">
<meta property="og:title" content="文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning">
<meta property="og:url" content="http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/index.html">
<meta property="og:site_name" content="Andeper的个人博客">
<meta property="og:description" content="声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！Semi-supervised Clustering for Short Text via Deep Representation Learning基于深度表示学习的短文本半监督聚类 AbstractIn this work, we propose a semi-supervised method for short text clus">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/TIM图片20190401153334.png">
<meta property="og:image" content="http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/]DFHXJ5RX%7DG[A242WKCXNT6.png">
<meta property="og:image" content="http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/图2.png">
<meta property="og:updated_time" content="2019-04-01T12:19:09.975Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning">
<meta name="twitter:description" content="声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！Semi-supervised Clustering for Short Text via Deep Representation Learning基于深度表示学习的短文本半监督聚类 AbstractIn this work, we propose a semi-supervised method for short text clus">
<meta name="twitter:image" content="http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/TIM图片20190401153334.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/"/>





  <title> 文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning | Andeper的个人博客 </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
<!-- hexo-inject:begin --><!-- hexo-inject:end --><script>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '',
      xfbml      : true,
      version    : 'v2.6'
    });
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "//connect.facebook.net/zh_Hans/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>




	<div id="vk_api_transport"></div>
	<script type="text/javascript">
		window.vkAsyncInit = function() {
			VK.init({
				apiId: 
			});

			
				VK.Widgets.Like("vk_like", {type: "mini", height: 20});
			

			
				VK.Widgets.Comments("vk_comments", {limit: 10, attach: "*"});
			
		};
		setTimeout(function() {
			var el = document.createElement("script");
			el.type = "text/javascript";
			el.src = "//vk.com/js/api/openapi.js";
			el.async = true;
			document.getElementById("vk_api_transport").appendChild(el);
		}, 0);
	</script>





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?8aabc26c969f399d0abe524a29699f13";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Andeper的个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-01T15:40:37+08:00">
                2019-04-01
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong><br>Semi-supervised Clustering for Short Text via Deep Representation Learning<br>基于深度表示学习的短文本半监督聚类</p>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>In this work, we propose a semi-supervised method for short text clustering, where we represent texts as distributed vectors with neural networks, and use a small amount of labeled data to specify our intention for clustering. We design a novel objective to combine the representation learning process and the kmeans clustering process together, and optimize the objective with both labeled data and unlabeled data iteratively until convergence through three steps: (1) assign each short text to its nearest centroid based on its representation from the current neural networks; (2) reestimate the cluster centroids based on cluster assignments from step (1); (3) update neural networks according to the objective by keeping centroids and cluster assignments fixed.Experimental results on four datasets show that our method works significantly better than several other text clustering methods.</p>
<p>在这项工作中，我们提出了一种用于短文本聚类的半监督方法，其中我们将文本表示为具有神经网络的分布式向量，并使用少量标记数据来指定我们的聚类意图。 我们设计了一个新的目标，将表示学习过程和kmeans聚类过程结合在一起，迭代地用标记数据和未标记数据优化目标，直到通过三个步骤收敛：（1）基于其分配每个短文本到最近的质心 来自当前神经网络的表示; （2）根据步骤（1）中的聚类分配重新估计聚类质心; （3）通过保持质心和聚类分配固定，根据目标更新神经网络。对四个数据集的实验结果表明，我们的方法比其他几种文本聚类方法效果明显更好。</p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h3><p>Text clustering is a fundamental problem in text mining and information retrieval. Its task is to group similar texts together such that texts within a cluster are more similar to texts in other clusters. Usually, a text is represented as a bag-of-words or term frequency-inverse document frequency (TFIDF) vector, and then the k-means algorithm (MacQueen, 1967) is performed to partition a set of texts into homogeneous groups.<br>However, when dealing with short texts, the characteristics of short text and clustering task raise several issues for the conventional unsupervised clustering algorithms. First, the number of uniqe words in each short text is small, as a result, the lexcical sparsity issue usually leads to poor clustering quality (Dhillon and Guan, 2003). Second, for a specific short text clustering task, we have prior knowledge or paticular intenstions before clustering, while fully unsupervised approaches may learn some classes the other way around. Take the sentences in Table 1 for example, those sentences can be clustered into different partitions based on different intentions: apple{a, b, c} and orange {d, e, f} with a fruit type intension, or what-question {a, d}, when-question {b, e},and yes/no-question cluster {c, f} with a question type intension.<br>文本聚类是文本挖掘和信息检索中的基本问题。 其任务是将类似的文本组合在一起，使得群集内的文本更类似于其他群集中的文本。 通常，文本被表示为词袋或术语频率 - 逆文档频率（TFIDF）向量，然后执行k均值算法（MacQueen，1967）以将一组文本划分为同类组。<br>然而，在处理短文本时，短文本和聚类任务的特征为传统的无监督聚类算法提出了若干问题。首先，每个短文本中的单词数量很少，因此，词汇稀疏性问题通常会导致较差的聚类质量（Dhillon和Guan，2003）。其次，对于特定的短文本聚类任务，我们在聚类之前具有先验知识或特定强度，而完全无监督的方法可以反过来学习一些类。 以表1中的句子为例，这些句子可以根据不同的意图聚类到不同的分区：苹果{a，b，c}和橘子{d，e，f}，具有水果类型内涵，或者什么问题{a，d}，什么时候问题{b，e}，是否问题{c，f}，带有问题类型含义。<br><img src="/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/TIM图片20190401153334.png" alt="table1"><br>To address the lexical sparity issue, one direction is to enrich text representations by extracting features and relations from Wikipedia (Banerjee et al.,2007) or an ontology (Fodeh et al., 2011). But this approach requires the annotated knowlege, which is also language dependent. So the other direction, which directly encode texts into distributed vectors with neural networks (Hinton and Salakhutdinov,2006; Xu et al., 2015), becomes more interesing. To tackle the second problem, semi-supervised approaches (e.g. (Bilenko et al., 2004; Davidson and Basu, 2007; Bair, 2013)) have gained significant popularity in the past decades. Our question is can we have a unified model to integrate netural networks into the semi-supervied framework?<br>为了解决词汇间性问题，一个方向是通过从维基百科(Banerjee et al.,2007)或本体论(Fodeh et al., 2011)中提取特征和关系来丰富文本表示。但是这种方法需要带注释的知识，这也是语言依赖的。因此，使用神经网络将文本直接编码为分布式向量的另一个方向（Hinton和Salakhutdinov，2006; Xu等，2015）变得更加有意义。为了解决第二个问题，半监督方法 (e.g. (Bilenko et al., 2004; Davidson and Basu, 2007; Bair, 2013))在过去几十年中获得了极大的欢迎。我们的问题是，我们可以有一个统一的模型将神经网络整合到半监督框架中吗？</p>
<p>In this paper, we propose a unified framework for the short text clustering task. We employ a deep neural network model to represent short sentences, and integrate it into a semi-supervised algorithm. Concretely, we extend the objective in the classical unsupervised k-means algorithm by adding a penalty term from labeled data. Thus, the new objective covers three key groups of parameters: centroids of clusters, the cluster assignment for each text, and the parameters within deep neural networks. In the training procedure, we start from random initialization of centroids and neural networks, and then optimize the objective iteratively through three steps until converge:<br>(1) assign each short text to its nearest centroid based on its representation from the current neural networks;<br>(2) re-estimate cluster centroids based on cluster assignments from step (1);<br>(3) update neural networks according to the objective by keeping centroids and cluster assignments fixed.<br>Experimental results on four different datasets show that our method achieves significant improvements over several other text clustering methods In following parts, we first describe our neural network models for text representaion (Section 2).Then we introduce our semi-supervised clustering method and the learning algorithm (Section 3). Finally, we evaluate our method on four different datasets (Section 4).<br>在本文中，我们提出了一个用于短文本聚类任务的统一框架。我们采用深度神经网络模型来表示短句，并将其整合到半监督算法中。具体地说，我们通过从标记数据中添加惩罚项来扩展经典无监督k均值算法的目标。因此，新目标涵盖三个关键参数组：聚类的质心，每个文本的聚类分配以及深度神经网络中的参数。在训练过程中，我们从质心和神经网络的随机初始化开始，然后通过三个步骤迭代地优化目标直到收敛：<br>（1）根据当前神经网络的表示，将每个短文本分配到最近的质心;<br>（2）根据步骤（1）中的聚类分配重新估计聚类质心;<br>（3）通过保持质心和簇分配固定，根据目标更新神经网络。<br>四个不同数据集的实验结果表明，我们的方法比其他几种文本聚类方法有了显着的改进。在下面的部分中，我们首先描述了用于文本表示的神经网络模型（第2节）。然后我们介绍了我们的半监督聚类方法和学习。算法（第3节）。最后，我们在四个不同的数据集上评估我们的方法（第4节）。</p>
<h3 id="2-Representation-Learning-for-Short-Texts"><a href="#2-Representation-Learning-for-Short-Texts" class="headerlink" title="2 Representation Learning for Short Texts"></a>2 Representation Learning for Short Texts</h3><p>We represent each word with a dense vector w, so that a short text s is first represented as a matrix $S = [w_1, …, w_{|s|}]$, which is a concatenation of all vectors of w in s, |s| is the length of s. Then we design two different types of neural networks to ingest the word vector sequence S: the convolutional neural networks (CNN) and the long short-term memory(LSTM). More formally, we define the presentation function as $x = f(s)$, where x is the represent vector of the text s. We test two encoding functions (CNN and LSTM) in our experiments.<br>我们用密集向量w表示每个单词，因此短文本s首先表示为矩阵$S = [w_1, …, w_{|s|}]$，它是s中所有w的向量的连接，|s|是s的长度。然后我们设计了两种不同类型的神经网络来摄取单词矢量序列S：卷积神经网络（CNN）和长短期记忆（LSTM）。更正式地，我们将表示函数定义为$x = f(s)$，其中x是文本s的表示向量。我们在实验中测试了两种编码函数（CNN和LSTM）。<br><img src="/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/]DFHXJ5RX}G[A242WKCXNT6.png" alt="figure1"><br>Inspired from Kim (2014), our CNN model views the sequence of word vectors as a matrix, and applies two sequential operations: convolution and maxpooling. Then, a fully connected layer is employed to convert the final representation vector into a fixed size. Figure 1 gives the diagram of the CNN model. In the convolution operation, we define a list of filters ${w_o}$, where the shape of each filter is d × h, dis the dimension of word vectors and h is the window size. Each filter is applied to a patch (a window size h of vectors) of S, and generates a feature. We apply this filter to all possible patches in S, and produce a series of features. The number of features depends on the shape of the filter wo and the length of the input short text. To deal with variable feature size, we perform a max-pooling operation over all the features to select the maximum value. Therefore, after the two operations, each filter generates only one feature. We define several filters by varying the window size and the initial values. Thus, a vector of features is captured after the max-pooling operation, and the feature dimension is equal to the number of filters.<br>受Kim（2014）的启发，我们的CNN模型将单词向量序列视为矩阵，并应用两个连续运算：卷积和最大化。然后，使用完全连接的层将最终表示矢量转换为固定大小。图1给出了CNN模型的示意图。在卷积运算中，我们定义了一个过滤器列表${w_o}$，其中每个过滤器的形状是d×h，dis是单词向量的维度，h是窗口大小。每个滤波器应用于S的贴片（矢量的窗口大小h），并生成特征。我们将此过滤器应用于S中的所有可能的补丁，并生成一系列功能。特征的数量取决于滤波器的形状和输入短文本的长度。为了处理可变特征尺寸，我们对所有特征执行最大池操作以选择最大值。因此，在两次操作之后，每个过滤器仅生成一个特征。我们通过改变窗口大小和初始值来定义几个过滤器。因此，在最大池操作之后捕获特征向量，并且特征维度等于过滤器的数量。<br><img src="/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/图2.png" alt="tu2"><br>Figure 2 gives the diagram of our LSTM model. We implement the standard LSTM block described in Graves (2012). Each word vector is fed into the LSTM model sequentially, and the mean of the hidden states over the entire sentence is taken as the final representation vector.<br>图2给出了我们的LSTM模型图。我们实现了Graves（2012）中描述的标准LSTM块。每个单词矢量被顺序地馈送到LSTM模型中，并且整个句子上的隐藏状态的平均值被作为最终表示矢量。</p>
<h3 id="3-Semi-supervised-Clustering-for-Short-Texts"><a href="#3-Semi-supervised-Clustering-for-Short-Texts" class="headerlink" title="3 Semi-supervised Clustering for Short Texts"></a>3 Semi-supervised Clustering for Short Texts</h3><h4 id="3-1-Revisiting-K-means-Clustering"><a href="#3-1-Revisiting-K-means-Clustering" class="headerlink" title="3.1 Revisiting K-means Clustering"></a>3.1 Revisiting K-means Clustering</h4><p>Given a set of texts $\{s_1,s_2,\cdots,s_N\}$, we represent them as a set of data points $\{x_1,x_2,\cdots,x_N\}$, where $x_i$ can be a bag-of-words or TF-IDF vector in traditional approaches, or a dense vector in Section 2.The task of text clustering is to partition the data set into some numberK of clusters, such that the sum of the squared distance of each data point to its closest cluster centroid is minimized. For each data point $x_n$, we define a set of binary variables $r_{nk}\in\{0,1\}$,where $k\in\{1,\cdots,K\}$ describing which of the K clusters $x_n$ is assigned to. So that if $x_n$ is assigned to cluster k, then $r_{nk} = 1$, and $r_{nj} = 0$ for$j\neq k$.Let’s define $u_k$ as the centroid of the k-th cluster.We can then formulate the objective function as<script type="math/tex">J_{unsup} = \sum_{n=1}^N\sum_{k=1}^K r_{nk}\left \|x_n-u_k\right \|^2</script>Our goal is the find the values of $\{r_{nk}\}$ and $\{u_k\}$ so as to minimize Junsup.<br>给出一组文本$\{s_1,s_2,\cdots,s_N\}$，我们将它们表示为一组数据点$\{x_1,x_2,\cdots,x_N\}$，其中$x_i$可以是传统方法中的词袋或TF-IDF向量，或者是第2节中的密集向量。文本聚类的任务是将数据集划分为某些数量的簇，使得总和 每个数据点到其最近的聚类质心的平方距离最小化。 对于每个数据点$x_n$，我们定义一组二进制变量$r_{nk}\in\{0,1\}$，其中$k\in\{1,\cdots,K\}$描述$x_n$分配到哪个群集。 因此，如果将$x_n$分配给簇k，则$r_{nk} = 1$，并且对于$j\neq k,r_{nj} = 0$。让我们将$u_k$定义为第k个簇的质心。然后我们可以将目标函数表示为<script type="math/tex">J_{unsup} = \sum_{n=1}^N\sum_{k=1}^K r_{nk}\left \|x_n-u_k\right \|^2</script>我们的目标是找到$\{r_{nk}\}$和$\{u_k\}$的值，以便最小化$J_{unsup}$。</p>
<p>The k-means algorithm optimizes $J_{semi}$ through the gradient descent approach, and results in an iterative procedure (Bishop, 2006). Each iteration involves two steps: E-step and M-step. In the Estep, the algorithm minimizes $J_{semi}$ with respect to $\{r_{nk}\}$ by keeping $\{u_k\}$ fixed. $J_{semi}$ is a linear function for $\{r_{nk}\}$, so we can optimize for each data point separately by simply assigning the n-th data point to the closest cluster centroid. In the M-step,the algorithm minimizes $J_{semi}$ with respect to $\{u_k\}$ by keeping $\{r_{nk}\}$ fixed. $J_{semi}$ is a quadratic function of $\{u_k\}$, and it can be minimized by setting its derivative with respect to $\{u_k\}$ to zero.<script type="math/tex">\frac{\partial J_{unsup}}{\partial \mu_k} = 2\sum_{n=1}^Nr_{nk}(x_n-\mu_k)=0(2)</script><br>Then, we can easily solve $\{u_k\}$ as<script type="math/tex">\mu_k=\frac{\sum_{n=1}^Nr_{nk}x_n}{\sum_{n=1}^Nr_{nk}}(3)</script><br>In other words, $\{u_k\}$ is equal to the mean of all the data points assigned to cluster k.<br>k-means算法通过梯度下降方法优化$J_{semi}$，并产生迭代过程（Bishop，2006）。 每次迭代都涉及两个步骤：E步和M步。 在E-step中，算法通过保持$\{u_k\}$固定来最小化$J_{semi}$相对于$\{r_{nk}\}$。 $J_{semi}$是$\{r_{nk}\}$的线性函数，因此我们可以通过简单地将第n个数据点分配给最近的聚类质心来分别优化每个数据点。 在M步骤中，算法通过保持$\{r_{nk}\}$固定来最小化$J_{semi}$相对于$\{u_k\}$。 $J_{semi}$是$\{u_k\}$的二次函数，可以通过将其导数相对于$\{u_k\}$设置为零来最小化。<script type="math/tex">\frac{\partial J_{unsup}}{\partial \mu_k} = 2\sum_{n=1}^Nr_{nk}(x_n-\mu_k)=0(2)</script><br>我们很容易得到<script type="math/tex">\mu_k=\frac{\sum_{n=1}^Nr_{nk}x_n}{\sum_{n=1}^Nr_{nk}}(3)</script><br>换句话说，$\{u_k\}$等于分配给簇k的所有数据点的平均值。</p>
<h4 id="3-2-Semi-supervised-K-means-with-Neural-Networks"><a href="#3-2-Semi-supervised-K-means-with-Neural-Networks" class="headerlink" title="3.2 Semi-supervised K-means with Neural Networks"></a>3.2 Semi-supervised K-means with Neural Networks</h4><p>The classical k-means algorithm only uses unlabeled data, and solves the clustering problem under the unsupervised learning framework. As already mentioned, the clustering results may not be consistent to our intention. In order to acquire useful clustering results, some supervised information should be introduced into the learning procedure. To this end,we employ a small amount of labeled data to guide the clustering process.<br>Following Section 2, we represent each text s as a dense vector x via neural networks $f(s)$. Instead of training the text representation model separately,we integrate the training process into the k-means algorithm, so that both the labeled data and the unlabeled data can be used for representation learning and text clustering. Let us denote the labeled data set as $\{(s_1,y_1),(s_2,y_2),\cdots,(s_L,y_L)\}$, and the unlabeled data set as $\{s_{L+1},s_{L+2},\cdots,s_N\}$, where $y_i$ is the given label for $s_i$. We then define the objective function as:<script type="math/tex">J_{semi} =\alpha \sum_{n=1}^N\sum_{k=1}^Kr_{nk}\left \|  f(s_n)-\mu_k\right \|^2+(1-\alpha)\sum_{n=1}^L\{\left \|  f(s_n)-\mu_{g_n}\right \|^2+\sum_{j\neq g_n}[l+\sum_{n=1}^L\left \|  f(s_n)-\mu_{g_n}\right \|^2-\sum_{n=1}^L\left \|  f(s_n)-\mu_{j}\right \|^2]_+\}</script><br>经典的k-means算法仅使用未标记的数据，并解决了无监督学习框架下的聚类问题。 如前所述，聚类结果可能与我们的意图不一致。为了获得有用的聚类结果，应该在学习过程中引入一些监督信息。为此，我们使用少量标记数据来指导群集过程。<br>在第2节之后，我们通过神经网络$f(s)$将每个文本s表示为密集向量x。我们不是单独训练文本表示模型，而是将训练过程集成到k-means算法中，以便标记数据和未标记数据都可以用于表示学习和文本聚类。 让我们将标记数据集表示为$\{(s_1,y_1),(s_2,y_2),\cdots,(s_L,y_L)\}$，并将未标记数据集表示为$\{s_{L+1},s_{L+2},\cdots,s_N\}$，其中$y_i$是$s_i$的给定标签。 然后我们将目标函数定义为：</p>
<script type="math/tex; mode=display">J_{semi} =\alpha \sum_{n=1}^N\sum_{k=1}^Kr_{nk}\left \|  f(s_n)-\mu_k\right \|^2+(1-\alpha)\sum_{n=1}^L\{\left \|  f(s_n)-\mu_{g_n}\right \|^2+\sum_{j\neq g_n}[l+\sum_{n=1}^L\left \|  f(s_n)-\mu_{g_n}\right \|^2-\sum_{n=1}^L\left \|  f(s_n)-\mu_{j}\right \|^2]_+\}</script><p>The objective function contains two terms. The first term is adapted from the unsupervised k-means algorithm in Eq. (1), and the second term is defined to encourage labeled data being clustered in correlation with the given labels. $\alpha\in[0,1]$ is used to tune the importance of unlabeled data. The second term contains two parts. The first part penalizes large distance between each labeled instance and its correct cluster centroid, where $g_n=G(y_n)$ is the cluster ID mapped from the given label yn, and the mapping function $G(\cdot)$ is implemented with the Hungarian algorithm (Munkres, 1957). The second part is denoted as a hinge loss with a margin l, where $[x]_+ = max(x,0)$. This part incurs some loss if the distance to the correct centroid is not shorter (by the margin l) than distances to any of incorrect cluster centroids.<br>目标函数包含两个部分。第一项改编自方程式中的无监督k均值算法。(1)，并且第二项被定义为鼓励标记数据与给定标签相关联地聚类。$\alpha\in[0,1]$用于调整未标记数据的重要性。第二项包含两部分。第一部分惩罚每个标记实例与其正确的聚类质心之间的大距离，其中$g_n=G(y_n)$是从给定标签$y_n$映射的聚类ID，映射函数$G(\cdot)$用匈牙利算法(Munkres, 1957)实现。第二部分表示为具有边界l的铰链损耗，其中$[x]_+ = max(x,0)$。如果到正确质心的距离不短（通过边缘l）而不是到任何不正确的聚类质心的距离，则该部分会引起一些损失。</p>
<p>There are three groups of parameters in $J_{semi}$: the cluster assignment of each text $\{r_{nk}\}$, the cluster centroids $\{\mu_k\}$, and the parameters within the neural network model $f(\cdot)$. Our goal is the find the values of $\{r_{nk}\}$, $\{\mu_k\}$ and parameters in $f(\cdot)$, so as to minimize $J_{semi}$. Inspired from the k-means algorithm,we design an algorithm to successively minimize $J_{semi}$ with respect to $\{r_{nk}\}$, $\{\mu_k\}$, and parameters in $f(\cdot)$. Table 2 gives the corresponding pseudocode.First, we initialize the cluster centroids $\{\mu_k\}$ with the k-means++ strategy (Arthur and Vassilvitskii,2007), and randomly initialize all the parameters in the neural network model. Then, the algorithm iteratively goes through three steps (assign cluster, estimate centroid, and update parameter) until $J_{semi}$ converges.<br>$J_{semi}$中有三组参数：每个文本的集群分配$\{r_{nk}\}$，集群质心$\{\mu_k\}$，以及神经网络模型$f(\cdot)$中的参数。我们的目标是在$f(\cdot)$中找到$\{r_{nk}\}$，$\{\mu_k\}$和参数的值，以便最小化$J_{semi}$。受k-means算法的启发，我们设计了一种算法，相对于$\{r_{nk}\}$，$\{\mu_k\}$和$f(\cdot)$中的参数，连续地最小化$J_{semi}$。表2给出了相应的伪代码。首先，我们用k-means ++策略初始化聚类中心$\{\mu_k\}$（Arthur和Vassilvitskii，2007），并随机初始化神经网络模型中的所有参数。然后，算法迭代地经历三个步骤（分配簇，估计质心和更新参数）直到$J_{semi}$收敛。</p>
<p>The assign cluster step minimizes $J_{semi}$ with respect to $\{r_{nk}\}$ by keeping $f(\cdot)$ and $\{\mu_k\}$ fixed. Its goal is to assign a cluster ID for each data point. We can see that the second term in Eq. (4) has no relation with $\{r_{nk}\}$. Thus, we only need to minimize the first term by assigning each text to its nearest cluster centroid, which is identical to the E-step in the k-means algorithm. In this step, we also calculate the mappings between the given labels $\{y_i\}$ and the cluster IDs (with the Hungarian algorithm) based on cluster assignments of all labeled data.<br>分配簇步骤通过保持$f(\cdot)$和$\{\mu_k\}$固定来最小化$J_{semi}$相对于$\{r_{nk}\}$。其目标是为每个数据点分配一个集群ID。我们可以看到方程式(4)中的第二项与$\{r_{nk}\}$无关。因此，我们只需要通过将每个文本分配到其最近的聚类质心来最小化第一项，这与k均值算法中的E步骤相同。在此步骤中，我们还基于所有标记数据的集群分配来计算给定标签$\{y_i\}$与集群ID（使用匈牙利算法）之间的映射。</p>
<p>The estimate centroid step minimizes $J_{semi}$ with respect to $\{\mu_k\}$ by keeping $\{r_{nk}\}$ and $f(\cdot)$ fixed,which corresponds to the M-step in the k-means algorithm. It aims to estimate the cluster centroids$\{\mu_k\}$ based on the cluster assignments $\{r_{nk}\}$ from the assign cluster step. The second term in Eq.(4) makes each labeled instance involved in the estimating process of cluster centroids. By solving $\partial J_{semi}/\partial \mu_k = 0$, we get<script type="math/tex">u_k =\frac{\sum_{n=1}^N\alpha r_{nk}f(s_n)+\sum_{n=1}^L w_{nk}f(s_n)}{\sum_{n=1}^N\alpha r_{nk}+\sum_{n=1}^L w_{nk}} (5)</script><br>${I}’_{nk} = \delta(k,g_n)$<br>${I}’’_{nkj} = \delta(k,j)\cdot \delta’_{nj}$<br>${I}’’’_{nkj} = (1-\delta(k,j))\cdot \delta’_{nj}$<br>$\delta’_{nj} = \delta(l+\left|f(s_n)-\mu_{g_n}\right|^2-\left|f(s_n)-\mu_{j}\right|^2&gt;0)(6)$<br>where $\delta(x_1, x_2)=1$ if $x_1$ is equal to $x_2$, otherwise $\delta(x_1, x_2)=0$, and $\delta(x)=1$ if x is true, otherwise $\delta(x)=0$. The first term in the numerator of Eq. (5) is the contributions from all data points, and $\alpha r_{nk}$ is the weight of $s_n$ for $\mu_k$. The second term is acquired from labeled data, and $w_{nk}$ is the weight of a labeled instance $s_n$ for $\mu_k$.<br>通过保持$\{r_{nk}\}$和$f(\cdot)$固定，估计质心步骤使$J_{semi}$相对于$\{\mu_k\}$最小化，这对应于k均值算法中的M步。它旨在根据分配集群步骤中的集群分配$\{r_{nk}\}$估计集群质心$\{\mu_k\}$。方程(4)中的第二项使得每个标记实例参与聚类质心的估计过程。通过求解$\partial J_{semi}/\partial \mu_k = 0$，我们得到了<script type="math/tex">u_k =\frac{\sum_{n=1}^N\alpha r_{nk}f(s_n)+\sum_{n=1}^L w_{nk}f(s_n)}{\sum_{n=1}^N\alpha r_{nk}+\sum_{n=1}^L w_{nk}}</script><br>${I}’_{nk} = \delta(k,g_n)$<br>${I}’’_{nkj} = \delta(k,j)\cdot \delta’_{nj}$<br>${I}’’’_{nkj} = (1-\delta(k,j))\cdot \delta’_{nj}$<br>$\delta’_{nj} = \delta(l+\left|f(s_n)-\mu_{g_n}\right|^2-\left|f(s_n)-\mu_{j}\right|^2&gt;0)$<br>其中$ \delta(x_1,x_2)= 1 $如果$ x_1 $等于$ x_2 $，否则$ \delta(x_1,x_2)= 0 $，如果x为真，$ \delta(x)= 1 $ ，否则$ \delta(x)= 0 $。 方程式(5)分子中的第一项是所有数据点的贡献，$ \alpha r_{nk} $是$ \mu_k $的$ s_n $的权重。 第二个术语是从标记数据中获取的，$ w_{nk} $是$ \mu_k $的标记实例$s_n$的权重。</p>
<p>The update parameter step minimizes $J_{semi}$ with respect to $f(\cdot)$ by keeping $\{r_{nk}\}$ and $\{\mu_k\}$ fixed,which has no counterpart in the k-means algorithm.The main goal is to update parameters for the text representation model. We take $J_{semi}$ as the loss function, and train neural networks with the Adam algorithm (Kingma and Ba, 2014).<br>更新参数步骤通过保持$\{r_{nk}\}$和$\{\mu_k\}$固定来最小化$J_{semi}$相对于$f(\cdot)$，其在k均值算法中没有对应物。主要目标是更新文本表示模型的参数。 我们将$J_{semi}$作为损失函数，并使用Adam算法训练神经网络（Kingma和Ba，2014）</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      

      
        
          <div class="social-like">
            
              <div class="vk_like">
                <span id="vk_like"></span>
              </div>
            

            
          </div>
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/20/文献摘要之Convolutional-Neural-Networks-for-Sentence-Classification/" rel="next" title="文献摘要之Convolutional Neural Networks for Sentence Classification">
                <i class="fa fa-chevron-left"></i> 文献摘要之Convolutional Neural Networks for Sentence Classification
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/"
           data-title="文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning" data-url="http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/update/avatar.jpg"
               alt="Andeper" />
          <p class="site-author-name" itemprop="name">Andeper</p>
           
              <p class="site-description motion-element" itemprop="description">欢迎来到我的技术博客</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">33</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/tags/index.html">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Introduction"><span class="nav-number">2.</span> <span class="nav-text">1 Introduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Representation-Learning-for-Short-Texts"><span class="nav-number">3.</span> <span class="nav-text">2 Representation Learning for Short Texts</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Semi-supervised-Clustering-for-Short-Texts"><span class="nav-number">4.</span> <span class="nav-text">3 Semi-supervised Clustering for Short Texts</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Revisiting-K-means-Clustering"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 Revisiting K-means Clustering</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Semi-supervised-K-means-with-Neural-Networks"><span class="nav-number">4.2.</span> <span class="nav-text">3.2 Semi-supervised K-means with Neural Networks</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Andeper</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"andeper"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  













  
  

  
  


  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



  

</body>
</html>
