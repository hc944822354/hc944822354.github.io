<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="Andeper的个人博客" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！Semi-supervised Clustering for Short Text via Deep Representation Learning基于深度表示学习的短文本半监督聚类 AbstractIn this work, we propose a semi-supervised method for short text clus">
<meta property="og:type" content="article">
<meta property="og:title" content="文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning">
<meta property="og:url" content="http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/index.html">
<meta property="og:site_name" content="Andeper的个人博客">
<meta property="og:description" content="声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！Semi-supervised Clustering for Short Text via Deep Representation Learning基于深度表示学习的短文本半监督聚类 AbstractIn this work, we propose a semi-supervised method for short text clus">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/TIM图片20190401153334.png">
<meta property="og:image" content="http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/]DFHXJ5RX%7DG[A242WKCXNT6.png">
<meta property="og:image" content="http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/图2.png">
<meta property="og:updated_time" content="2019-04-16T11:30:35.251Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning">
<meta name="twitter:description" content="声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！Semi-supervised Clustering for Short Text via Deep Representation Learning基于深度表示学习的短文本半监督聚类 AbstractIn this work, we propose a semi-supervised method for short text clus">
<meta name="twitter:image" content="http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/TIM图片20190401153334.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/"/>





  <title> 文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning | Andeper的个人博客 </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
<!-- hexo-inject:begin --><!-- hexo-inject:end --><script>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '',
      xfbml      : true,
      version    : 'v2.6'
    });
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "//connect.facebook.net/zh_Hans/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>




	<div id="vk_api_transport"></div>
	<script type="text/javascript">
		window.vkAsyncInit = function() {
			VK.init({
				apiId: 
			});

			
				VK.Widgets.Like("vk_like", {type: "mini", height: 20});
			

			
				VK.Widgets.Comments("vk_comments", {limit: 10, attach: "*"});
			
		};
		setTimeout(function() {
			var el = document.createElement("script");
			el.type = "text/javascript";
			el.src = "//vk.com/js/api/openapi.js";
			el.async = true;
			document.getElementById("vk_api_transport").appendChild(el);
		}, 0);
	</script>





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?8aabc26c969f399d0abe524a29699f13";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Andeper的个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-01T15:40:37+08:00">
                2019-04-01
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong><br>Semi-supervised Clustering for Short Text via Deep Representation Learning<br>基于深度表示学习的短文本半监督聚类</p>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>In this work, we propose a semi-supervised method for short text clustering, where we represent texts as distributed vectors with neural networks, and use a small amount of labeled data to specify our intention for clustering. We design a novel objective to combine the representation learning process and the kmeans clustering process together, and optimize the objective with both labeled data and unlabeled data iteratively until convergence through three steps: (1) assign each short text to its nearest centroid based on its representation from the current neural networks; (2) reestimate the cluster centroids based on cluster assignments from step (1); (3) update neural networks according to the objective by keeping centroids and cluster assignments fixed.Experimental results on four datasets show that our method works significantly better than several other text clustering methods.</p>
<p>在这项工作中，我们提出了一种用于短文本聚类的半监督方法，其中我们将文本表示为具有神经网络的分布式向量，并使用少量标记数据来指定我们的聚类意图。 我们设计了一个新的目标，将表示学习过程和kmeans聚类过程结合在一起，迭代地用标记数据和未标记数据优化目标，直到通过三个步骤收敛：（1）基于其分配每个短文本到最近的质心 来自当前神经网络的表示; （2）根据步骤（1）中的聚类分配重新估计聚类质心; （3）通过保持质心和聚类分配固定，根据目标更新神经网络。对四个数据集的实验结果表明，我们的方法比其他几种文本聚类方法效果明显更好。</p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h3><p>Text clustering is a fundamental problem in text mining and information retrieval. Its task is to group similar texts together such that texts within a cluster are more similar to texts in other clusters. Usually, a text is represented as a bag-of-words or term frequency-inverse document frequency (TFIDF) vector, and then the k-means algorithm (MacQueen, 1967) is performed to partition a set of texts into homogeneous groups.<br>However, when dealing with short texts, the characteristics of short text and clustering task raise several issues for the conventional unsupervised clustering algorithms. First, the number of uniqe words in each short text is small, as a result, the lexcical sparsity issue usually leads to poor clustering quality (Dhillon and Guan, 2003). Second, for a specific short text clustering task, we have prior knowledge or paticular intenstions before clustering, while fully unsupervised approaches may learn some classes the other way around. Take the sentences in Table 1 for example, those sentences can be clustered into different partitions based on different intentions: apple{a, b, c} and orange {d, e, f} with a fruit type intension, or what-question {a, d}, when-question {b, e},and yes/no-question cluster {c, f} with a question type intension.<br>文本聚类是文本挖掘和信息检索中的基本问题。 其任务是将类似的文本组合在一起，使得群集内的文本更类似于其他群集中的文本。 通常，文本被表示为词袋或术语频率 - 逆文档频率（TFIDF）向量，然后执行k均值算法（MacQueen，1967）以将一组文本划分为同类组。<br>然而，在处理短文本时，短文本和聚类任务的特征为传统的无监督聚类算法提出了若干问题。首先，每个短文本中的单词数量很少，因此，词汇稀疏性问题通常会导致较差的聚类质量（Dhillon和Guan，2003）。其次，对于特定的短文本聚类任务，我们在聚类之前具有先验知识或特定强度，而完全无监督的方法可以反过来学习一些类。 以表1中的句子为例，这些句子可以根据不同的意图聚类到不同的分区：苹果{a，b，c}和橘子{d，e，f}，具有水果类型内涵，或者什么问题{a，d}，什么时候问题{b，e}，是否问题{c，f}，带有问题类型含义。<br><img src="/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/TIM图片20190401153334.png" alt="table1"><br>To address the lexical sparity issue, one direction is to enrich text representations by extracting features and relations from Wikipedia (Banerjee et al.,2007) or an ontology (Fodeh et al., 2011). But this approach requires the annotated knowlege, which is also language dependent. So the other direction, which directly encode texts into distributed vectors with neural networks (Hinton and Salakhutdinov,2006; Xu et al., 2015), becomes more interesing. To tackle the second problem, semi-supervised approaches (e.g. (Bilenko et al., 2004; Davidson and Basu, 2007; Bair, 2013)) have gained significant popularity in the past decades. Our question is can we have a unified model to integrate netural networks into the semi-supervied framework?<br>为了解决词汇间性问题，一个方向是通过从维基百科(Banerjee et al.,2007)或本体论(Fodeh et al., 2011)中提取特征和关系来丰富文本表示。但是这种方法需要带注释的知识，这也是语言依赖的。因此，使用神经网络将文本直接编码为分布式向量的另一个方向（Hinton和Salakhutdinov，2006; Xu等，2015）变得更加有意义。为了解决第二个问题，半监督方法 (e.g. (Bilenko et al., 2004; Davidson and Basu, 2007; Bair, 2013))在过去几十年中获得了极大的欢迎。我们的问题是，我们可以有一个统一的模型将神经网络整合到半监督框架中吗？</p>
<p>In this paper, we propose a unified framework for the short text clustering task. We employ a deep neural network model to represent short sentences, and integrate it into a semi-supervised algorithm. Concretely, we extend the objective in the classical unsupervised k-means algorithm by adding a penalty term from labeled data. Thus, the new objective covers three key groups of parameters: centroids of clusters, the cluster assignment for each text, and the parameters within deep neural networks. In the training procedure, we start from random initialization of centroids and neural networks, and then optimize the objective iteratively through three steps until converge:<br>(1) assign each short text to its nearest centroid based on its representation from the current neural networks;<br>(2) re-estimate cluster centroids based on cluster assignments from step (1);<br>(3) update neural networks according to the objective by keeping centroids and cluster assignments fixed.<br>Experimental results on four different datasets show that our method achieves significant improvements over several other text clustering methods In following parts, we first describe our neural network models for text representaion (Section 2).Then we introduce our semi-supervised clustering method and the learning algorithm (Section 3). Finally, we evaluate our method on four different datasets (Section 4).<br>在本文中，我们提出了一个用于短文本聚类任务的统一框架。我们采用深度神经网络模型来表示短句，并将其整合到半监督算法中。具体地说，我们通过从标记数据中添加惩罚项来扩展经典无监督k均值算法的目标。因此，新目标涵盖三个关键参数组：聚类的质心，每个文本的聚类分配以及深度神经网络中的参数。在训练过程中，我们从质心和神经网络的随机初始化开始，然后通过三个步骤迭代地优化目标直到收敛：<br>（1）根据当前神经网络的表示，将每个短文本分配到最近的质心;<br>（2）根据步骤（1）中的聚类分配重新估计聚类质心;<br>（3）通过保持质心和簇分配固定，根据目标更新神经网络。<br>四个不同数据集的实验结果表明，我们的方法比其他几种文本聚类方法有了显着的改进。在下面的部分中，我们首先描述了用于文本表示的神经网络模型（第2节）。然后我们介绍了我们的半监督聚类方法和学习。算法（第3节）。最后，我们在四个不同的数据集上评估我们的方法（第4节）。</p>
<h3 id="2-Representation-Learning-for-Short-Texts"><a href="#2-Representation-Learning-for-Short-Texts" class="headerlink" title="2 Representation Learning for Short Texts"></a>2 Representation Learning for Short Texts</h3><p>We represent each word with a dense vector w, so that a short text s is first represented as a matrix $S = [w_1, …, w_{|s|}]$, which is a concatenation of all vectors of w in s, |s| is the length of s. Then we design two different types of neural networks to ingest the word vector sequence S: the convolutional neural networks (CNN) and the long short-term memory(LSTM). More formally, we define the presentation function as $x = f(s)$, where x is the represent vector of the text s. We test two encoding functions (CNN and LSTM) in our experiments.<br>我们用密集向量w表示每个单词，因此短文本s首先表示为矩阵$S = [w_1, …, w_{|s|}]$，它是s中所有w的向量的连接，|s|是s的长度。然后我们设计了两种不同类型的神经网络来摄取单词矢量序列S：卷积神经网络（CNN）和长短期记忆（LSTM）。更正式地，我们将表示函数定义为$x = f(s)$，其中x是文本s的表示向量。我们在实验中测试了两种编码函数（CNN和LSTM）。<br><img src="/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/]DFHXJ5RX}G[A242WKCXNT6.png" alt="figure1"><br>Inspired from Kim (2014), our CNN model views the sequence of word vectors as a matrix, and applies two sequential operations: convolution and maxpooling. Then, a fully connected layer is employed to convert the final representation vector into a fixed size. Figure 1 gives the diagram of the CNN model. In the convolution operation, we define a list of filters ${w_o}$, where the shape of each filter is d × h, dis the dimension of word vectors and h is the window size. Each filter is applied to a patch (a window size h of vectors) of S, and generates a feature. We apply this filter to all possible patches in S, and produce a series of features. The number of features depends on the shape of the filter wo and the length of the input short text. To deal with variable feature size, we perform a max-pooling operation over all the features to select the maximum value. Therefore, after the two operations, each filter generates only one feature. We define several filters by varying the window size and the initial values. Thus, a vector of features is captured after the max-pooling operation, and the feature dimension is equal to the number of filters.<br>受Kim（2014）的启发，我们的CNN模型将单词向量序列视为矩阵，并应用两个连续运算：卷积和最大化。然后，使用完全连接的层将最终表示矢量转换为固定大小。图1给出了CNN模型的示意图。在卷积运算中，我们定义了一个过滤器列表${w_o}$，其中每个过滤器的形状是d×h，dis是单词向量的维度，h是窗口大小。每个滤波器应用于S的贴片（矢量的窗口大小h），并生成特征。我们将此过滤器应用于S中的所有可能的补丁，并生成一系列功能。特征的数量取决于滤波器的形状和输入短文本的长度。为了处理可变特征尺寸，我们对所有特征执行最大池操作以选择最大值。因此，在两次操作之后，每个过滤器仅生成一个特征。我们通过改变窗口大小和初始值来定义几个过滤器。因此，在最大池操作之后捕获特征向量，并且特征维度等于过滤器的数量。<br><img src="/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/图2.png" alt="tu2"><br>Figure 2 gives the diagram of our LSTM model. We implement the standard LSTM block described in Graves (2012). Each word vector is fed into the LSTM model sequentially, and the mean of the hidden states over the entire sentence is taken as the final representation vector.<br>图2给出了我们的LSTM模型图。我们实现了Graves（2012）中描述的标准LSTM块。每个单词矢量被顺序地馈送到LSTM模型中，并且整个句子上的隐藏状态的平均值被作为最终表示矢量。</p>
<h3 id="3-Semi-supervised-Clustering-for-Short-Texts"><a href="#3-Semi-supervised-Clustering-for-Short-Texts" class="headerlink" title="3 Semi-supervised Clustering for Short Texts"></a>3 Semi-supervised Clustering for Short Texts</h3><h4 id="3-1-Revisiting-K-means-Clustering"><a href="#3-1-Revisiting-K-means-Clustering" class="headerlink" title="3.1 Revisiting K-means Clustering"></a>3.1 Revisiting K-means Clustering</h4><p>Given a set of texts $\{s_1,s_2,\cdots,s_N\}$, we represent them as a set of data points $\{x_1,x_2,\cdots,x_N\}$, where $x_i$ can be a bag-of-words or TF-IDF vector in traditional approaches, or a dense vector in Section 2.The task of text clustering is to partition the data set into some numberK of clusters, such that the sum of the squared distance of each data point to its closest cluster centroid is minimized. For each data point $x_n$, we define a set of binary variables $r_{nk}\in\{0,1\}$,where $k\in\{1,\cdots,K\}$ describing which of the K clusters $x_n$ is assigned to. So that if $x_n$ is assigned to cluster k, then $r_{nk} = 1$, and $r_{nj} = 0$ for$j\neq k$.Let’s define $u_k$ as the centroid of the k-th cluster.We can then formulate the objective function as<script type="math/tex">J_{unsup} = \sum_{n=1}^N\sum_{k=1}^K r_{nk}\left \|x_n-u_k\right \|^2</script>Our goal is the find the values of $\{r_{nk}\}$ and $\{u_k\}$ so as to minimize Junsup.<br>给出一组文本$\{s_1,s_2,\cdots,s_N\}$，我们将它们表示为一组数据点$\{x_1,x_2,\cdots,x_N\}$，其中$x_i$可以是传统方法中的词袋或TF-IDF向量，或者是第2节中的密集向量。文本聚类的任务是将数据集划分为某些数量的簇，使得总和 每个数据点到其最近的聚类质心的平方距离最小化。 对于每个数据点$x_n$，我们定义一组二进制变量$r_{nk}\in\{0,1\}$，其中$k\in\{1,\cdots,K\}$描述$x_n$分配到哪个群集。 因此，如果将$x_n$分配给簇k，则$r_{nk} = 1$，并且对于$j\neq k,r_{nj} = 0$。让我们将$u_k$定义为第k个簇的质心。然后我们可以将目标函数表示为<script type="math/tex">J_{unsup} = \sum_{n=1}^N\sum_{k=1}^K r_{nk}\left \|x_n-u_k\right \|^2</script>我们的目标是找到$\{r_{nk}\}$和$\{u_k\}$的值，以便最小化$J_{unsup}$。</p>
<p>The k-means algorithm optimizes $J_{semi}$ through the gradient descent approach, and results in an iterative procedure (Bishop, 2006). Each iteration involves two steps: E-step and M-step. In the Estep, the algorithm minimizes $J_{semi}$ with respect to $\{r_{nk}\}$ by keeping $\{u_k\}$ fixed. $J_{semi}$ is a linear function for $\{r_{nk}\}$, so we can optimize for each data point separately by simply assigning the n-th data point to the closest cluster centroid. In the M-step,the algorithm minimizes $J_{semi}$ with respect to $\{u_k\}$ by keeping $\{r_{nk}\}$ fixed. $J_{semi}$ is a quadratic function of $\{u_k\}$, and it can be minimized by setting its derivative with respect to $\{u_k\}$ to zero.<script type="math/tex">\frac{\partial J_{unsup}}{\partial \mu_k} = 2\sum_{n=1}^Nr_{nk}(x_n-\mu_k)=0(2)</script><br>Then, we can easily solve $\{u_k\}$ as<script type="math/tex">\mu_k=\frac{\sum_{n=1}^Nr_{nk}x_n}{\sum_{n=1}^Nr_{nk}}(3)</script><br>In other words, $\{u_k\}$ is equal to the mean of all the data points assigned to cluster k.<br>k-means算法通过梯度下降方法优化$J_{semi}$，并产生迭代过程（Bishop，2006）。 每次迭代都涉及两个步骤：E步和M步。 在E-step中，算法通过保持$\{u_k\}$固定来最小化$J_{semi}$相对于$\{r_{nk}\}$。 $J_{semi}$是$\{r_{nk}\}$的线性函数，因此我们可以通过简单地将第n个数据点分配给最近的聚类质心来分别优化每个数据点。 在M步骤中，算法通过保持$\{r_{nk}\}$固定来最小化$J_{semi}$相对于$\{u_k\}$。 $J_{semi}$是$\{u_k\}$的二次函数，可以通过将其导数相对于$\{u_k\}$设置为零来最小化。<script type="math/tex">\frac{\partial J_{unsup}}{\partial \mu_k} = 2\sum_{n=1}^Nr_{nk}(x_n-\mu_k)=0(2)</script><br>我们很容易得到<script type="math/tex">\mu_k=\frac{\sum_{n=1}^Nr_{nk}x_n}{\sum_{n=1}^Nr_{nk}}(3)</script><br>换句话说，$\{u_k\}$等于分配给簇k的所有数据点的平均值。</p>
<h4 id="3-2-Semi-supervised-K-means-with-Neural-Networks"><a href="#3-2-Semi-supervised-K-means-with-Neural-Networks" class="headerlink" title="3.2 Semi-supervised K-means with Neural Networks"></a>3.2 Semi-supervised K-means with Neural Networks</h4><p>The classical k-means algorithm only uses unlabeled data, and solves the clustering problem under the unsupervised learning framework. As already mentioned, the clustering results may not be consistent to our intention. In order to acquire useful clustering results, some supervised information should be introduced into the learning procedure. To this end,we employ a small amount of labeled data to guide the clustering process.<br>Following Section 2, we represent each text s as a dense vector x via neural networks $f(s)$. Instead of training the text representation model separately,we integrate the training process into the k-means algorithm, so that both the labeled data and the unlabeled data can be used for representation learning and text clustering. Let us denote the labeled data set as $\{(s_1,y_1),(s_2,y_2),\cdots,(s_L,y_L)\}$, and the unlabeled data set as $\{s_{L+1},s_{L+2},\cdots,s_N\}$, where $y_i$ is the given label for $s_i$. We then define the objective function as:<script type="math/tex">J_{semi} =\alpha \sum_{n=1}^N\sum_{k=1}^Kr_{nk}\left \|  f(s_n)-\mu_k\right \|^2+(1-\alpha)\sum_{n=1}^L\{\left \|  f(s_n)-\mu_{g_n}\right \|^2+\sum_{j\neq g_n}[l+\sum_{n=1}^L\left \|  f(s_n)-\mu_{g_n}\right \|^2-\sum_{n=1}^L\left \|  f(s_n)-\mu_{j}\right \|^2]_+\}</script><br>经典的k-means算法仅使用未标记的数据，并解决了无监督学习框架下的聚类问题。 如前所述，聚类结果可能与我们的意图不一致。为了获得有用的聚类结果，应该在学习过程中引入一些监督信息。为此，我们使用少量标记数据来指导群集过程。<br>在第2节之后，我们通过神经网络$f(s)$将每个文本s表示为密集向量x。我们不是单独训练文本表示模型，而是将训练过程集成到k-means算法中，以便标记数据和未标记数据都可以用于表示学习和文本聚类。 让我们将标记数据集表示为$\{(s_1,y_1),(s_2,y_2),\cdots,(s_L,y_L)\}$，并将未标记数据集表示为$\{s_{L+1},s_{L+2},\cdots,s_N\}$，其中$y_i$是$s_i$的给定标签。 然后我们将目标函数定义为：</p>
<script type="math/tex; mode=display">J_{semi} =\alpha \sum_{n=1}^N\sum_{k=1}^Kr_{nk}\left \|  f(s_n)-\mu_k\right \|^2+(1-\alpha)\sum_{n=1}^L\{\left \|  f(s_n)-\mu_{g_n}\right \|^2+\sum_{j\neq g_n}[l+\sum_{n=1}^L\left \|  f(s_n)-\mu_{g_n}\right \|^2-\sum_{n=1}^L\left \|  f(s_n)-\mu_{j}\right \|^2]_+\}</script><p>The objective function contains two terms. The first term is adapted from the unsupervised k-means algorithm in Eq. (1), and the second term is defined to encourage labeled data being clustered in correlation with the given labels. $\alpha\in[0,1]$ is used to tune the importance of unlabeled data. The second term contains two parts. The first part penalizes large distance between each labeled instance and its correct cluster centroid, where $g_n=G(y_n)$ is the cluster ID mapped from the given label yn, and the mapping function $G(\cdot)$ is implemented with the Hungarian algorithm (Munkres, 1957). The second part is denoted as a hinge loss with a margin l, where $[x]_+ = max(x,0)$. This part incurs some loss if the distance to the correct centroid is not shorter (by the margin l) than distances to any of incorrect cluster centroids.<br>目标函数包含两个部分。第一项改编自方程式中的无监督k均值算法。(1)，并且第二项被定义为鼓励标记数据与给定标签相关联地聚类。$\alpha\in[0,1]$用于调整未标记数据的重要性。第二项包含两部分。第一部分惩罚每个标记实例与其正确的聚类质心之间的大距离，其中$g_n=G(y_n)$是从给定标签$y_n$映射的聚类ID，映射函数$G(\cdot)$用匈牙利算法(Munkres, 1957)实现。第二部分表示为具有边界l的铰链损耗，其中$[x]_+ = max(x,0)$。如果到正确质心的距离不短（通过边缘l）而不是到任何不正确的聚类质心的距离，则该部分会引起一些损失。</p>
<p>There are three groups of parameters in $J_{semi}$: the cluster assignment of each text $\{r_{nk}\}$, the cluster centroids $\{\mu_k\}$, and the parameters within the neural network model $f(\cdot)$. Our goal is the find the values of $\{r_{nk}\}$, $\{\mu_k\}$ and parameters in $f(\cdot)$, so as to minimize $J_{semi}$. Inspired from the k-means algorithm,we design an algorithm to successively minimize $J_{semi}$ with respect to $\{r_{nk}\}$, $\{\mu_k\}$, and parameters in $f(\cdot)$. Table 2 gives the corresponding pseudocode.First, we initialize the cluster centroids $\{\mu_k\}$ with the k-means++ strategy (Arthur and Vassilvitskii,2007), and randomly initialize all the parameters in the neural network model. Then, the algorithm iteratively goes through three steps (assign cluster, estimate centroid, and update parameter) until $J_{semi}$ converges.<br>$J_{semi}$中有三组参数：每个文本的集群分配$\{r_{nk}\}$，集群质心$\{\mu_k\}$，以及神经网络模型$f(\cdot)$中的参数。我们的目标是在$f(\cdot)$中找到$\{r_{nk}\}$，$\{\mu_k\}$和参数的值，以便最小化$J_{semi}$。受k-means算法的启发，我们设计了一种算法，相对于$\{r_{nk}\}$，$\{\mu_k\}$和$f(\cdot)$中的参数，连续地最小化$J_{semi}$。表2给出了相应的伪代码。首先，我们用k-means ++策略初始化聚类中心$\{\mu_k\}$（Arthur和Vassilvitskii，2007），并随机初始化神经网络模型中的所有参数。然后，算法迭代地经历三个步骤（分配簇，估计质心和更新参数）直到$J_{semi}$收敛。</p>
<p>The assign cluster step minimizes $J_{semi}$ with respect to $\{r_{nk}\}$ by keeping $f(\cdot)$ and $\{\mu_k\}$ fixed. Its goal is to assign a cluster ID for each data point. We can see that the second term in Eq. (4) has no relation with $\{r_{nk}\}$. Thus, we only need to minimize the first term by assigning each text to its nearest cluster centroid, which is identical to the E-step in the k-means algorithm. In this step, we also calculate the mappings between the given labels $\{y_i\}$ and the cluster IDs (with the Hungarian algorithm) based on cluster assignments of all labeled data.<br>分配簇步骤通过保持$f(\cdot)$和$\{\mu_k\}$固定来最小化$J_{semi}$相对于$\{r_{nk}\}$。其目标是为每个数据点分配一个集群ID。我们可以看到方程式(4)中的第二项与$\{r_{nk}\}$无关。因此，我们只需要通过将每个文本分配到其最近的聚类质心来最小化第一项，这与k均值算法中的E步骤相同。在此步骤中，我们还基于所有标记数据的集群分配来计算给定标签$\{y_i\}$与集群ID（使用匈牙利算法）之间的映射。</p>
<p>The estimate centroid step minimizes $J_{semi}$ with respect to $\{\mu_k\}$ by keeping $\{r_{nk}\}$ and $f(\cdot)$ fixed,which corresponds to the M-step in the k-means algorithm. It aims to estimate the cluster centroids$\{\mu_k\}$ based on the cluster assignments $\{r_{nk}\}$ from the assign cluster step. The second term in Eq.(4) makes each labeled instance involved in the estimating process of cluster centroids. By solving $\partial J_{semi}/\partial \mu_k = 0$, we get<script type="math/tex">u_k =\frac{\sum_{n=1}^N\alpha r_{nk}f(s_n)+\sum_{n=1}^L w_{nk}f(s_n)}{\sum_{n=1}^N\alpha r_{nk}+\sum_{n=1}^L w_{nk}} (5)</script><br>${I}’_{nk} = \delta(k,g_n)$<br>${I}’’_{nkj} = \delta(k,j)\cdot \delta’_{nj}$<br>${I}’’’_{nkj} = (1-\delta(k,j))\cdot \delta’_{nj}$<br>$\delta’_{nj} = \delta(l+\left|f(s_n)-\mu_{g_n}\right|^2-\left|f(s_n)-\mu_{j}\right|^2&gt;0)(6)$<br>where $\delta(x_1, x_2)=1$ if $x_1$ is equal to $x_2$, otherwise $\delta(x_1, x_2)=0$, and $\delta(x)=1$ if x is true, otherwise $\delta(x)=0$. The first term in the numerator of Eq. (5) is the contributions from all data points, and $\alpha r_{nk}$ is the weight of $s_n$ for $\mu_k$. The second term is acquired from labeled data, and $w_{nk}$ is the weight of a labeled instance $s_n$ for $\mu_k$.<br>通过保持$\{r_{nk}\}$和$f(\cdot)$固定，估计质心步骤使$J_{semi}$相对于$\{\mu_k\}$最小化，这对应于k均值算法中的M步。它旨在根据分配集群步骤中的集群分配$\{r_{nk}\}$估计集群质心$\{\mu_k\}$。方程(4)中的第二项使得每个标记实例参与聚类质心的估计过程。通过求解$\partial J_{semi}/\partial \mu_k = 0$，我们得到了<script type="math/tex">u_k =\frac{\sum_{n=1}^N\alpha r_{nk}f(s_n)+\sum_{n=1}^L w_{nk}f(s_n)}{\sum_{n=1}^N\alpha r_{nk}+\sum_{n=1}^L w_{nk}}</script><br>${I}’_{nk} = \delta(k,g_n)$<br>${I}’’_{nkj} = \delta(k,j)\cdot \delta’_{nj}$<br>${I}’’’_{nkj} = (1-\delta(k,j))\cdot \delta’_{nj}$<br>$\delta’_{nj} = \delta(l+\left|f(s_n)-\mu_{g_n}\right|^2-\left|f(s_n)-\mu_{j}\right|^2&gt;0)$<br>其中$ \delta(x_1,x_2)= 1 $如果$ x_1 $等于$ x_2 $，否则$ \delta(x_1,x_2)= 0 $，如果x为真，$ \delta(x)= 1 $ ，否则$ \delta(x)= 0 $。 方程式(5)分子中的第一项是所有数据点的贡献，$ \alpha r_{nk} $是$ \mu_k $的$ s_n $的权重。 第二个术语是从标记数据中获取的，$ w_{nk} $是$ \mu_k $的标记实例$s_n$的权重。</p>
<p>The update parameter step minimizes $J_{semi}$ with respect to $f(\cdot)$ by keeping $\{r_{nk}\}$ and $\{\mu_k\}$ fixed,which has no counterpart in the k-means algorithm.The main goal is to update parameters for the text representation model. We take $J_{semi}$ as the loss function, and train neural networks with the Adam algorithm (Kingma and Ba, 2014).<br>更新参数步骤通过保持$\{r_{nk}\}$和$\{\mu_k\}$固定来最小化$J_{semi}$相对于$f(\cdot)$，其在k均值算法中没有对应物。主要目标是更新文本表示模型的参数。 我们将$J_{semi}$作为损失函数，并使用Adam算法训练神经网络（Kingma和Ba，2014）</p>
<h3 id="4-Experiment"><a href="#4-Experiment" class="headerlink" title="4 Experiment"></a>4 Experiment</h3><h4 id="4-1-Experimental-Setting"><a href="#4-1-Experimental-Setting" class="headerlink" title="4.1 Experimental Setting"></a>4.1 Experimental Setting</h4><p>We evaluate our method on four short text datasets.(1) question type is the TREC question dataset (Li and Roth, 2002), where all the questions are classified into 6 categories: abbreviation, description,entity, human, location and numeric. (2) ag news dataset contains short texts extracted from the AG’s news corpus, where all the texts are classified into 4 categories: World, Sports, Business, and Sci/Tech(Zhang and LeCun, 2015). (3) dbpedia is the DBpedia ontology dataset, which is constructed by picking 14 non-overlapping classes from DBpedia 2014(Lehmann et al., 2014). (4) yahoo answer is the 10 topics classification dataset extracted from Yahoo! Answers Comprehensive Questions and Answers version 1.0 dataset by Zhang and LeCun(2015). We use all the 5,952 questions for the question type dataset. But the other three datasets contain too many instances (e.g. 1,400,000 instances in yahoo answer). Running clustering experiments on such a large dataset is quite inefficient. Following the same solution in (Xu et al., 2015), we randomly choose 1,000 samples for each classes individually for the other three datasets. Within each dataset, we randomly sample 10% of the instances as labeled data, and evaluate the performance on the remaining 90% instances. Table 3 summarizes the statistics of these datasets.<br>In all experiments, we set the size of word vector dimension as d=300 1, and pre-train the word vectors with the word2vec toolkit (Mikolov et al., 2013) on the English Gigaword (LDC2011T07). The number of clusters is set to be the same number of labels in the dataset. The clustering performance is evaluated with two metrics: Adjusted Mutual Information (AMI) (Vinh et al., 2009) and accuracy (ACC) (Amigo et al., 2009). In order to show the statistical significance, the performance of each experiment is the average of 10 trials.<br>我们在四个短文本数据集上评估我们的方法。（1）问题类型是TREC问题数据集（Li和Roth，2002），其中所有问题分为6类：缩写，描述，实体，人，位置和数字。 （2）ag新闻数据集包含从AG的新闻语料库中提取的短文本，其中所有文本分为4类：世界，体育，商业和科学/技术（Zhang和LeCun，2015）。 （3）dbpedia是DBpedia本体数据集，它是通过从DBpedia 2014中挑选14个非重叠类来构建的（Lehmann等，2014）。 （4）雅虎答案是从Yahoo!中提取的10个主题分类数据集。由Zhang和LeCun（2015）回答综合问题和答案版本1.0数据集。我们对问题类型数据集使用了所有5,952个问题。但其他三个数据集包含太多实例（例如雅虎答案中的1,400,000个实例）。在如此大的数据集上运行聚类实验是非常低效的。按照（Xu et al。，2015）中的相同解决方案，我们为其他三个数据集分别为每个类随机选择1,000个样本。在每个数据集中，我们将10％的实例随机抽样为标记数据，并评估剩余90％实例的性能。表3总结了这些数据集的统计数据。<br>在所有实验中，我们将字向量维度的大小设置为d = 300 1，并使用word2vec工具包（Mikolov等人，2013）对英语千兆字（LDC2011T07）预先训练单词向量。簇的数量设置为数据集中的标签数量。使用两个度量来评估聚类性能：调整的互信息（AMI）（Vinh等人，2009）和准确度（ACC）（Amigo等人，2009）。为了显示统计学显着性，每个实验的性能是10次试验的平均值。</p>
<h4 id="4-2-Model-Properties"><a href="#4-2-Model-Properties" class="headerlink" title="4.2 Model Properties"></a>4.2 Model Properties</h4><p>There are several hyper-parameters in our model,e.g., the output dimension of the text representation models, and the α in Eq. (4). The choice of these hyper-parameters may affect the final performance.In this subsection, we present some experiments to demonstrate the properties of our model, and find a good configuration that we use to evaluate our final model. All the experiments in this subsection were performed on the question type dataset.<br>First, we evaluated the effectiveness of the output dimension in text representation models. We switched the dimension size among {50, 100, 300,500, 1000}, and fixed the other options as: α =0.5, the filter types in the CNN model including{unigram, bigram, trigram} and 500 filters for each type. Figure 3 presents the AMIs from both CNN and LSTM models. We found that 100 is the best output dimension for both CNN and LSTM models.Therefore, we set the output dimension as 100 in the following experiments.<br>Second, we studied the effect of α in Eq. (4),which tunes the importance of unlabeled data. We varied α among {0.00001, 0.0001, 0.001, 0.01, 0.1}, and remain the other options as the last experiment. Figure 4 shows the AMIs from both CNN and LSTM models. We found that the clustering performance is not good when using a very small α<br>By increasing the value of α, we acquired progressive improvements, and reached to the peak point at α=0.01. After that, the performance dropped.Therefore, we choose α=0.01 in the following experiments. This results also indicate that the unlabeled data are useful for the text representation learning process.<br>Third, we tested the influence of the size of labeled data. We tuned the ratio of labeled instances from the whole dataset among [1%, 10%], and kept the other configurations as the previous experiment.The AMIs are shown in Figure 5. We can see that the more labeled data we use, the better performance we get. Therefore, the labeled data are quite useful for the clustering process.<br>Fourth, we checked the effect of the pre-training strategy for our models. We added a softmax layer on top of our CNN and LSTM models, where the size of the output layer is equal to the number of labels in the dataset. We then trained the model through the classification task using all labeled data.After this process, we removed the top layer, and used the remaining parameters to initialize our CNN and LSTM models. The performance for our models with and without pre-training strategy are given in Figure 6. We can see that the pre-training strategy is quite effective for our models. Therefore, we use the pre-training strategy in the following experiments.<br>在我们的模型中有几个超参数，例如，文本表示模型的输出维度，以及方程式中的α。 （4）。这些超参数的选择可能会影响最终的性能。在本小节中，我们提供了一些实验来演示模型的属性，并找到一个好的配置，用于评估我们的最终模型。本小节中的所有实验都是在问题类型数据集上执行的。<br>首先，我们评估了文本表示模型中输出维度的有效性。我们将维度大小切换为{50,100,300,500,1000}，并将其他选项固定为：α= 0.5，CNN模型中的过滤器类型包括{unigram，bigram，trigram}和每种类型的500个过滤器。图3显示了CNN和LSTM模型的AMI。我们发现100是CNN和LSTM模型的最佳输出维度。因此，我们在以下实验中将输出维度设置为100。<br>其次，我们研究了α在方程式中的影响。 （4），调整未标记数据的重要性。我们在{0.00001,0.0001,0.001,0.01,0.1}中改变α，并且作为最后一个实验保留其他选项。图4显示了CNN和LSTM模型的AMI。我们发现当使用非常小的α时，聚类性能不好<br>通过增加α的值，我们获得了渐进的改进，并达到α= 0.01的峰值点。之后，性能下降。因此，我们在以下实验中选择α= 0.01。该结果还表明未标记的数据对于文本表示学习过程是有用的。<br>第三，我们测试了标记数据大小的影响。我们调整了整个数据集中标记实例的比例[1％，10％]，并将其他配置保留为之前的实验。图6显示了AMI。我们可以看到我们使用的标记数据越多，我们得到更好的表现。因此，标记数据对于聚类过程非常有用。<br>第四，我们检查了预训练策略对我们模型的影响。我们在CNN和LSTM模型的顶部添加了softmax图层，其中输出图层的大小等于数据集中的标签数量。然后，我们使用所有标记数据通过分类任务训练模型。在此过程之后，我们移除了顶层，并使用其余参数初始化我们的CNN和LSTM模型。图6给出了有和没有预训练策略的模型的性能。我们可以看到预训练策略对我们的模型非常有效。因此，我们在以下实验中使用预训练策略。</p>
<h4 id="4-3-Comparing-with-other-Models"><a href="#4-3-Comparing-with-other-Models" class="headerlink" title="4.3 Comparing with other Models"></a>4.3 Comparing with other Models</h4><p>In this subsection, we compared our method with some representative systems. We implemented a series of clustering systems. All of these systems are based on the k-means algorithm, but they represent short texts differently:<br>bow represents each text as a bag-of-words vector.<br>tf-idf represents each text as a TF-IDF vector.<br>average-vec represents each text with the average of all word vectors within the text.<br>metric-learn-bow employs the metric learning method proposed by Weinberger et al. (2005), and learns to project a bag-of-words vector into a 300-dimensional vector based on labeled data.<br>metric-learn-idf uses the same metric learning method, and learns to map a TF-IDF vectorinto a 300-dimensional vector based on labeled data.<br>metric-learn-ave-vec also uses the metric learning method, and learns to project an averaged word vector into a 100-dimensional vector based on labeled data.<br>We designed two classifiers (cnn-classifier and lstm-classifier) by adding a softmax layer on top of our CNN and LSTM models. We trained these two classifiers with labeled data, and utilized them to predict labels for unlabeled data. We also built two text representation models (“cnn-represent.” and “lstm-represent.”) by setting parameters of our CNN and LSTM models with the corresponding parameters in cnn-classifier and lstm-classifier. Then, we used them to represent short texts into vectors, and applied the k-means algorithm for clustering.<br>Table 4 summarizes the results of all systems on each dataset, where “semi-cnn” is our semisupervised clustering algorithm with the CNN model, and “semi-lstm” is our semi-supervised clustering algorithm with the LSTM model. We grouped all the systems into three categories: unsupervised (Unsup.), supervised (Sup.), and semi-supervised (Semisup.) 2. We found that the supervised systems worked much better than the unsupervised counterparts, which implies that the small amount of labeled data is necessary for better performance. We also noticed that within the supervised systems, the systems using deep learning (CNN or LSTM) models worked better than the systems using metric learning method, which shows the power of deep learning models for short text modeling. Our “semi-cnn” system got the best performance on almost all the datasets.<br>Figure 7 visualizes clustering results on the question type dataset from four representative systems. In Figure 7(a), clusters severely overlap with each other. When using the CNN sentence representation model, we can clearly identify all clusters in Figure 7(b), but the boundaries between clusters are still obscure. The clustering results from our semisupervised clustering algorithm are given in Figure 7(c) and Figure 7(d). We can see that the boundaries between clusters become much clearer. Therefore, our algorithm is very effective for short text clustering.<br>在本小节中，我们将我们的方法与一些代表性系统进行了比较。我们实施了一系列集群系统。所有这些系统都基于k-means算法，但它们以不同的方式表示短文本：<br>bow将每个文本表示为一个词袋矢量。<br>tf-idf将每个文本表示为TF-IDF向量。<br>average-vec用文本中所有单词向量的平均值表示每个文本。<br>metric-learn-bow采用Weinberger等人提出的度量学习方法。 （2005），并且学习基于标记数据将袋子矢量投影到300维矢量中。<br>metric-learn-idf使用相同的度量学习方法，并且学习基于标记数据将TF-IDF向量映射到300维向量。<br>metric-learn-ave-vec还使用度量学习方法，并学习基于标记数据将平均单词向量投影到100维向量中。<br>我们通过在CNN和LSTM模型之上添加softmax层来设计两个分类器（cnn-classifier和lstm-classifier）。我们使用标记数据训练这两个分类器，并利用它们来预测未标记数据的标签。我们还通过使用cnn-classifier和lstm-classifier中的相应参数设置CNN和LSTM模型的参数，构建了两个文本表示模型（“cnn-represent。”和“lstm-represent。”）。然后，我们使用它们将短文本表示为向量，并应用k-means算法进行聚类。<br>表4总结了每个数据集上所有系统的结果，其中“semi-cnn”是我们使用CNN模型的半监督聚类算法，“semi-lstm”是我们使用LSTM模型的半监督聚类算法。我们将所有系统分为三类：无监督（Unsup。），监督（Sup。）和半监督（Semisup。）2。我们发现监督系统比无监督系统工作得更好，这意味着小标记数据量是提高性能所必需的。我们还注意到，在监督系统中，使用深度学习（CNN或LSTM）模型的系统比使用度量学习方法的系统工作得更好，这显示了深度学习模型用于短文本建模的能力。我们的“半cnn”系统在几乎所有数据集上都获得了最佳性能。<br>图7显示了来自四个代表性系统的问题类型数据集的聚类结果。在图7（a）中，簇严重地彼此重叠。当使用CNN语句表示模型时，我们可以清楚地识别图7（b）中的所有聚类，但聚类之间的边界仍然模糊不清。我们的半监督聚类算法的聚类结果如图7（c）和图7（d）所示。我们可以看到集群之间的界限变得更加清晰。因此，我们的算法对短文本聚类非常有效。</p>
<h3 id="5-Related-Work"><a href="#5-Related-Work" class="headerlink" title="5 Related Work"></a>5 Related Work</h3><p>Existing semi-supervised clustering methods fall into two categories: constraint-based and representation-based. In constraint-based methods (Davidson and Basu, 2007), some labeled information is used to constrain the clustering process. In representation-based methods (Bair, 2013), a representation model is first trained to satisfy the labeled information, and all data points are clustered based on representations from the representation model. Bilenko et al. (2004) proposed to integrate there two methods into a unified framework, which shares the same idea of our proposed method. However, they only employed the metric learning model for representation learning, which is a linear projection. Whereas, our method utilized deep learning models to learn representations in a more flexible non-linear space. Xu et al. (2015) also employed deep learning models for short text clustering. However, their method separated the representation learning process from the clustering process, so it belongs to the representation-based method. Whereas, our method combined the representation learning process and the clustering process together, and utilized both labeled data and unlabeled data for representation learning and clustering.<br>现有的半监督聚类方法分为两类：基于约束和基于表示。在基于约束的方法中（Davidson和Basu，2007），一些标记信息用于约束聚类过程。在基于表示的方法（Bair，2013）中，首先训练表示模型以满足标记信息，并且基于来自表示模型的表示来聚类所有数据点。 Bilenko等。 （2004）提出将两种方法整合到一个统一的框架中，它与我们提出的方法有着相同的想法。然而，他们只使用度量学习模型进行表示学习，这是一种线性投影。然而，我们的方法利用深度学习模型在更灵活的非线性空间中学习表示。徐等人。 （2015）也采用深度学习模型进行短文本聚类。但是，他们的方法将表示学习过程与聚类过程分开，因此它属于基于表示的方法。然而，我们的方法将表示学习过程和聚类过程结合在一起，并利用标记数据和未标记数据进行表示学习和聚类。</p>
<h3 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6 Conclusion"></a>6 Conclusion</h3><p>In this paper, we proposed a semi-supervised clustering algorithm for short texts. We utilized deep learning models to learn representations for short texts, and employed a small amount of labeled data to specify our intention for clustering. We integrated the representation learning process and the clustering process into a unified framework, so that both of the two processes get some benefits from labeled data and unlabeled data. Experimental results on four datasets show that our method is more effective than other competitors.<br>在本文中，我们提出了一种用于短文本的半监督聚类算法。 我们利用深度学习模型来学习短文本的表示，并使用少量标记数据来指定我们的聚类意图。 我们将表示学习过程和聚类过程集成到一个统一的框架中，这样两个过程都可以从标记数据和未标记数据中获益。 四个数据集的实验结果表明，我们的方法比其他竞争对手更有效。</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      

      
        
          <div class="social-like">
            
              <div class="vk_like">
                <span id="vk_like"></span>
              </div>
            

            
          </div>
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/20/文献摘要之Convolutional-Neural-Networks-for-Sentence-Classification/" rel="next" title="文献摘要之Convolutional Neural Networks for Sentence Classification">
                <i class="fa fa-chevron-left"></i> 文献摘要之Convolutional Neural Networks for Sentence Classification
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/03/文献摘要之Short-Text-Clustering-via-Convolutional-Neural-Networks/" rel="prev" title="文献摘要之Short-Text-Clustering-via-Convolutional-Neural-Networks">
                文献摘要之Short-Text-Clustering-via-Convolutional-Neural-Networks <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/"
           data-title="文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning" data-url="http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/update/avatar.jpg"
               alt="Andeper" />
          <p class="site-author-name" itemprop="name">Andeper</p>
           
              <p class="site-description motion-element" itemprop="description">欢迎来到我的技术博客</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">41</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/tags/index.html">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Introduction"><span class="nav-number">2.</span> <span class="nav-text">1 Introduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Representation-Learning-for-Short-Texts"><span class="nav-number">3.</span> <span class="nav-text">2 Representation Learning for Short Texts</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Semi-supervised-Clustering-for-Short-Texts"><span class="nav-number">4.</span> <span class="nav-text">3 Semi-supervised Clustering for Short Texts</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Revisiting-K-means-Clustering"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 Revisiting K-means Clustering</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Semi-supervised-K-means-with-Neural-Networks"><span class="nav-number">4.2.</span> <span class="nav-text">3.2 Semi-supervised K-means with Neural Networks</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Experiment"><span class="nav-number">5.</span> <span class="nav-text">4 Experiment</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-Experimental-Setting"><span class="nav-number">5.1.</span> <span class="nav-text">4.1 Experimental Setting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-Model-Properties"><span class="nav-number">5.2.</span> <span class="nav-text">4.2 Model Properties</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-Comparing-with-other-Models"><span class="nav-number">5.3.</span> <span class="nav-text">4.3 Comparing with other Models</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Related-Work"><span class="nav-number">6.</span> <span class="nav-text">5 Related Work</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-Conclusion"><span class="nav-number">7.</span> <span class="nav-text">6 Conclusion</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Andeper</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"andeper"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  













  
  

  
  


  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



  

</body>
</html>
