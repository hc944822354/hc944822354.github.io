<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="Andeper的个人博客" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="上节课我们主要介绍了浅层神经网络。首先介绍神经网络的基本结构，包括输入层，隐藏层和输出层。然后以简单的2 layer NN为例，详细推导了其正向传播过程和反向传播过程，使用梯度下降的方法优化神经网络参数。同时，我们还介绍了不同的激活函数，比较各自优缺点，讨论了激活函数必须是非线性的原因。最后介绍了神经网络参数随机初始化的必要性，特别是权重W，不同神经元的W不能初始化为同一零值。本节课是对上节课的延">
<meta property="og:type" content="article">
<meta property="og:title" content="深层神经网络">
<meta property="og:url" content="http://andeper.cn/2018/02/21/深层神经网络/index.html">
<meta property="og:site_name" content="Andeper的个人博客">
<meta property="og:description" content="上节课我们主要介绍了浅层神经网络。首先介绍神经网络的基本结构，包括输入层，隐藏层和输出层。然后以简单的2 layer NN为例，详细推导了其正向传播过程和反向传播过程，使用梯度下降的方法优化神经网络参数。同时，我们还介绍了不同的激活函数，比较各自优缺点，讨论了激活函数必须是非线性的原因。最后介绍了神经网络参数随机初始化的必要性，特别是权重W，不同神经元的W不能初始化为同一零值。本节课是对上节课的延">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://andeper.cn/2018/02/21/深层神经网络/deepLNN">
<meta property="og:image" content="http://andeper.cn/2018/02/21/深层神经网络/DNNN">
<meta property="og:image" content="http://andeper.cn/2018/02/21/深层神经网络/intutition">
<meta property="og:image" content="http://andeper.cn/2018/02/21/深层神经网络/Ilayer">
<meta property="og:image" content="http://andeper.cn/2018/02/21/深层神经网络/Ilayer2">
<meta property="og:image" content="http://andeper.cn/2018/02/21/深层神经网络/propagation">
<meta property="og:updated_time" content="2018-02-21T12:39:48.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="深层神经网络">
<meta name="twitter:description" content="上节课我们主要介绍了浅层神经网络。首先介绍神经网络的基本结构，包括输入层，隐藏层和输出层。然后以简单的2 layer NN为例，详细推导了其正向传播过程和反向传播过程，使用梯度下降的方法优化神经网络参数。同时，我们还介绍了不同的激活函数，比较各自优缺点，讨论了激活函数必须是非线性的原因。最后介绍了神经网络参数随机初始化的必要性，特别是权重W，不同神经元的W不能初始化为同一零值。本节课是对上节课的延">
<meta name="twitter:image" content="http://andeper.cn/2018/02/21/深层神经网络/deepLNN">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://andeper.cn/2018/02/21/深层神经网络/"/>





  <title> 深层神经网络 | Andeper的个人博客 </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
<!-- hexo-inject:begin --><!-- hexo-inject:end --><script>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '',
      xfbml      : true,
      version    : 'v2.6'
    });
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "//connect.facebook.net/zh_Hans/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>




	<div id="vk_api_transport"></div>
	<script type="text/javascript">
		window.vkAsyncInit = function() {
			VK.init({
				apiId: 
			});

			
				VK.Widgets.Like("vk_like", {type: "mini", height: 20});
			

			
				VK.Widgets.Comments("vk_comments", {limit: 10, attach: "*"});
			
		};
		setTimeout(function() {
			var el = document.createElement("script");
			el.type = "text/javascript";
			el.src = "//vk.com/js/api/openapi.js";
			el.async = true;
			document.getElementById("vk_api_transport").appendChild(el);
		}, 0);
	</script>





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?8aabc26c969f399d0abe524a29699f13";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Andeper的个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2018/02/21/深层神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                深层神经网络
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-21T18:20:38+08:00">
                2018-02-21
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/02/21/深层神经网络/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/02/21/深层神经网络/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>上节课我们主要介绍了浅层神经网络。首先介绍神经网络的基本结构，包括输入层，隐藏层和输出层。然后以简单的2 layer NN为例，详细推导了其正向传播过程和反向传播过程，使用梯度下降的方法优化神经网络参数。同时，我们还介绍了不同的激活函数，比较各自优缺点，讨论了激活函数必须是非线性的原因。最后介绍了神经网络参数随机初始化的必要性，特别是权重W，不同神经元的W不能初始化为同一零值。本节课是对上节课的延伸和扩展，讨论更深层的神经网络。</p>
<h3 id="Deep-L-layer-neural-network"><a href="#Deep-L-layer-neural-network" class="headerlink" title="Deep L-layer neural network"></a>Deep L-layer neural network</h3><p>深层神经网络其实就是包含更多的隐藏层神经网络。如下图所示，分别列举了逻辑回归、1个隐藏层的神经网络、2个隐藏层的神经网络和5个隐藏层的神经网络它们的模型结构。<br><img src="/2018/02/21/深层神经网络/deepLNN" alt="deepLNN"></p>
<p>命名规则上，一般只参考隐藏层个数和输出层。例如，上图中的逻辑回归又叫1 layer NN，1个隐藏层的神经网络叫做2 layer NN，2个隐藏层的神经网络叫做3 layer NN，以此类推。如果是L-layer NN，则包含了L-1个隐藏层，最后的L层是输出层。</p>
<p>下面以一个4层神经网络为例来介绍关于神经网络的一些标记写法。如下图所示，首先，总层数用L表示，L=4。输入层是第0层，输出层是第L层。$n^{[l]}$表示第l层包含的单元个数，l=0,1,⋯,L。这个模型中，$n^{[0]}=n_x=3$，表示三个输入特征$x_1,x_2,x_3$。$n^{[1]}=5$，$n^{[2]}=5$，$n^{[3]}=3$，$n^{[4]}=n^{[L]}=1$。第l层的激活函数输出用$a^{[l]}$表示，$a^{[l]}=g^{[l]}(z^{[l]})$。$W^{[l]}$表示第l层的权重，用于计算$z^{[l]}$。另外，我们把输入x记为$a^{[0]}$，把输出层$\hat{y}$记为$a^{[L]}$。</p>
<p>注意，$a^{[l]}$和$W^{[l]}$中的上标l都是从1开始的，l=1,⋯,L。<br><img src="/2018/02/21/深层神经网络/DNNN" alt="DNNN"></p>
<h3 id="Forward-Propagation-in-a-Deep-Network"><a href="#Forward-Propagation-in-a-Deep-Network" class="headerlink" title="Forward Propagation in a Deep Network"></a>Forward Propagation in a Deep Network</h3><p>接下来，我们来推导一下深层神经网络的正向传播过程。仍以上面讲过的4层神经网络为例，对于单个样本：</p>
<p>第1层，l=1：</p>
<script type="math/tex; mode=display">z^{[1]}=W^{[1]}x+b^{[1]}=W^{[1]}a^{[0]}+b^{[1]}</script><script type="math/tex; mode=display">a^{[1]}=g^{[1]}(z^{[1]})</script><p>第2层，l=2：</p>
<script type="math/tex; mode=display">z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}</script><script type="math/tex; mode=display">a^{[2]}=g^{[2]}(z^{[2]})</script><p>第3层，l=3：</p>
<script type="math/tex; mode=display">z^{[3]}=W^{[3]}a^{[2]}+b^{[3]}</script><script type="math/tex; mode=display">a^{[3]}=g^{[3]}(z^{[3]})</script><p>第4层，l=4：</p>
<script type="math/tex; mode=display">z^{[4]}=W^{[4]}a^{[3]}+b^{[4]}</script><script type="math/tex; mode=display">a^{[4]}=g^{[4]}(z^{[4]})</script><p>如果有m个训练样本，其向量化矩阵形式为：</p>
<p>第1层，l=1：</p>
<script type="math/tex; mode=display">Z^{[1]}=W^{[1]}X+b^{[1]}=W^{[1]}A^{[0]}+b^{[1]}</script><script type="math/tex; mode=display">A^{[1]}=g^{[1]}(Z^{[1]})</script><p>第2层，l=2：</p>
<script type="math/tex; mode=display">Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}</script><script type="math/tex; mode=display">A^{[2]}=g^{[2]}(Z^{[2]})</script><p>第3层，l=3：</p>
<script type="math/tex; mode=display">Z^{[3]}=W^{[3]}A^{[2]}+b^{[3]}</script><script type="math/tex; mode=display">A^{[3]}=g^{[3]}(Z^{[3]})</script><p>第4层，l=4：</p>
<script type="math/tex; mode=display">Z^{[4]}=W^{[4]}A^{[3]}+b^{[4]}</script><script type="math/tex; mode=display">A^{[4]}=g^{[4]}(Z^{[4]})</script><p>综上所述，对于第l层，其正向传播过程的Z[l]和A[l]可以表示为：</p>
<script type="math/tex; mode=display">Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}</script><script type="math/tex; mode=display">A^{[l]}=g^{[l]}(Z^{[l]})</script><p>其中l=1,⋯,L</p>
<h3 id="Getting-your-matrix-dimensions-right"><a href="#Getting-your-matrix-dimensions-right" class="headerlink" title="Getting your matrix dimensions right"></a>Getting your matrix dimensions right</h3><p>对于单个训练样本，输入x的维度是$(n^{[0]},1)$,神经网络的参数$W^{[l]}$和$b^{[l]}$的维度分别是：</p>
<script type="math/tex; mode=display">W^{[l]}:\ (n^{[l]},n^{[l-1]})</script><script type="math/tex; mode=display">b^{[l]}:\ (n^{[l]},1)</script><p>其中，l=1,⋯,L，$n^{[l]}$和$n^{[l−1]}$分别表示第l层和l−1层的所含单元个数。$n^{[0]}=n_x$，表示输入层特征数目。</p>
<p>顺便提一下，反向传播过程中的$dW^{[l]}$和$db^{[l]}$的维度分别是：</p>
<script type="math/tex; mode=display">dW^{[l]}:\ (n^{[l]},n^{[l-1]})</script><script type="math/tex; mode=display">db^{[l]}:\ (n^{[l]},1)</script><p>注意到，$W^{[l]}$与$dW^{[l]}$维度相同，$b^{[l]}$与$db^{[l]}$维度相同。这很容易理解。</p>
<p>正向传播过程中的$z^{[l]}$和$a^{[l]}$的维度分别是：</p>
<script type="math/tex; mode=display">z^{[l]}:\ (n^{[l]},1)</script><script type="math/tex; mode=display">a^{[l]}:\ (n^{[l]},1)</script><p>$z^{[l]}$和$a^{[l]}$的维度是一样的，且$dz^{[l]}$和$da^{[l]}$的维度均与$z^{[l]}$和$a^{[l]}$的维度一致。</p>
<p>对于m个训练样本，输入矩阵X的维度是$(n^{[0]},m)$。需要注意的是$W^{[l]}$和$b^{[l]}$的维度与只有单个样本是一致的：</p>
<script type="math/tex; mode=display">W^{[l]}:\ (n^{[l]},n^{[l-1]})</script><script type="math/tex; mode=display">b^{[l]}:\ (n^{[l]},1)</script><p>只不过在运算$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$中，$b^{[l]}$会被当成$(n^{[l]},m)$矩阵进行运算，这是因为python的广播性质，且$b^{[l]}$每一列向量都是一样的。$dW^{[l]}$和$db^{[l]}$的维度分别与$W^{[l]}$和$b^{[l]}$的相同。</p>
<p>但是，$Z^{[l]}$和$A^{[l]}$的维度发生了变化：</p>
<script type="math/tex; mode=display">Z^{[l]}:\ (n^{[l]},m)</script><script type="math/tex; mode=display">A^{[l]}:\ (n^{[l]},m)</script><p>$dZ^{[l]}$和$dA^{[l]}$的维度分别与$Z^{[l]}$和$A^{[l]}$的相同。</p>
<h3 id="Why-deep-representations"><a href="#Why-deep-representations" class="headerlink" title="Why deep representations?"></a>Why deep representations?</h3><p>我们都知道神经网络能处理很多问题，而且效果显著。其强大能力主要源自神经网络足够“深”，也就是说网络层数越多，神经网络就更加复杂和深入，学习也更加准确。接下来，我们从几个例子入手，看一下为什么深度网络能够如此强大。</p>
<p>先来看人脸识别的例子，如下图所示。经过训练，神经网络第一层所做的事就是从原始图片中提取出人脸的轮廓与边缘，即边缘检测。这样每个神经元得到的是一些边缘信息。神经网络第二层所做的事情就是将前一层的边缘进行组合，组合成人脸一些局部特征，比如眼睛、鼻子、嘴巴等。再往后面，就将这些局部特征组合起来，融合成人脸的模样。可以看出，随着层数由浅到深，神经网络提取的特征也是从边缘到局部特征到整体，由简单到复杂。可见，如果隐藏层足够多，那么能够提取的特征就越丰富、越复杂，模型的准确率就会越高。</p>
<p>语音识别模型也是这个道理。浅层的神经元能够检测一些简单的音调，然后较深的神经元能够检测出基本的音素，更深的神经元就能够检测出单词信息。如果网络够深，还能对短语、句子进行检测。记住一点，神经网络从左到右，神经元提取的特征从简单到复杂。特征复杂度与神经网络层数成正相关。特征越来越复杂，功能也越来越强大。<br><img src="/2018/02/21/深层神经网络/intutition" alt="intutition"></p>
<p>除了从提取特征复杂度的角度来说明深层网络的优势之外，深层网络还有另外一个优点，就是能够减少神经元个数，从而减少计算量。例如下面这个例子，使用电路理论，计算逻辑输出：</p>
<script type="math/tex; mode=display">y=x_1\oplus x_2\oplus x_3\oplus\cdots\oplus x_n</script><p>其中，$\oplus$表示异或操作。对于这个逻辑运算，如果使用深度网络，深度网络的结构是每层将前一层的两两单元进行异或，最后到一个输出，如下图左边所示。这样，整个深度网络的层数是$log_2(n)$，不包含输入层。总共使用的神经元个数为：</p>
<script type="math/tex; mode=display">1+2+\cdots+2^{log_2(n)-1}=1\cdot\frac{1-2^{log_2(n)}}{1-2}=2^{log_2(n)}-1=n-1</script><p>可见，输入个数是n，这种深层网络所需的神经元个数仅仅是n-1个。</p>
<p>如果不用深层网络，仅仅使用单个隐藏层，那么需要的神经元个数将是指数级别那么大。Andrew指出，由于包含了所有的逻辑位（0和1），则需要$2^{n-1}$ ?个神经元。</p>
<p>比较下来，处理同一逻辑问题，深层网络所需的神经元个数比浅层网络要少很多。这也是深层神经网络的优点之一。</p>
<p>尽管深度学习有着非常显著的优势，Andrew还是建议对实际问题进行建模时，尽量先选择层数少的神经网络模型，这也符合奥卡姆剃刀定律（Occam’s Razor）。对于比较复杂的问题，再使用较深的神经网络模型。</p>
<h3 id="Building-blocks-of-deep-neural-networks"><a href="#Building-blocks-of-deep-neural-networks" class="headerlink" title="Building blocks of deep neural networks"></a>Building blocks of deep neural networks</h3><p>下面用流程块图来解释神经网络正向传播和反向传播过程。如下图所示，对于第l层来说，正向传播过程中：</p>
<p>输入：$a^{[l-1]}$<br>输出：$a^{[l]}$<br>参数：$W^{[l]}$,$b^{[l]}$<br>缓存变量：$z^{[l]}$</p>
<p>反向传播过程中：<br>输入：$da^{[l]}$<br>输出：$da^{[l-1]}$,$dW^{[l]}$,$db^{[l]}$<br>参数：$W^{[l]}$,$b^{[l]}$<br><img src="/2018/02/21/深层神经网络/Ilayer" alt="Ilayer"></p>
<p>刚才这是第l层的流程块图，对于神经网络所有层，整体的流程块图正向传播过程和反向传播过程如下所示：<br><img src="/2018/02/21/深层神经网络/Ilayer2" alt="Ilayer2"></p>
<h3 id="Forward-and-Backward-Propagation"><a href="#Forward-and-Backward-Propagation" class="headerlink" title="Forward and Backward Propagation"></a>Forward and Backward Propagation</h3><p>我们继续接着上一部分流程块图的内容，推导神经网络正向传播过程和反向传播过程的具体表达式。</p>
<p>首先是正向传播过程，令层数为第l层，输入是$a^{[l-1]}$，输出是$a^{[l]}$，缓存变量是$z^{[l]}$。其表达式如下：</p>
<script type="math/tex; mode=display">z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}</script><script type="math/tex; mode=display">a^{[l]}=g^{[l]}(z^{[l]})</script><p>m个训练样本，向量化形式为：</p>
<script type="math/tex; mode=display">Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}</script><script type="math/tex; mode=display">A^{[l]}=g^{[l]}(Z^{[l]})</script><p>然后是反向传播过程，输入是$da^{[l]}$，输出是$da^{[l-1]}$,$dw^{[l]}$,$db^{[l]}$。其表达式如下：</p>
<script type="math/tex; mode=display">dz^{[l]}=da^{[l]}\ast g^{[l]'}(z^{[l]})</script><script type="math/tex; mode=display">dW^{[l]}=dz^{[l]}\cdot a^{[l-1]}</script><script type="math/tex; mode=display">db^{[l]}=dz^{[l]}</script><script type="math/tex; mode=display">da^{[l-1]}=W^{[l]T}\cdot dz^{[l]}</script><p>由上述第四个表达式可得$da^{[l]}=W^{[l+1]T}\cdot dz^{[l+1]}$，将$da^{[l]}$代入第一个表达式中可以得到：</p>
<script type="math/tex; mode=display">dz^{[l]}=W^{[l+1]T}\cdot dz^{[l+1]}\ast g^{[l]'}(z^{[l]})</script><p>该式非常重要，反映了$dz^{[l+1]}$与<script type="math/tex">dz^{[l]}</script>的递推关系。</p>
<p>m个训练样本，向量化形式为：</p>
<script type="math/tex; mode=display">dZ^{[l]}=dA^{[l]}\ast g^{[l]'}(Z^{[l]})</script><script type="math/tex; mode=display">dW^{[l]}=\frac1mdZ^{[l]}\cdot A^{[l-1]T}</script><script type="math/tex; mode=display">db^{[l]}=\frac1mnp.sum(dZ^{[l]},axis=1,keepdim=True)</script><script type="math/tex; mode=display">dA^{[l-1]}=W^{[l]T}\cdot dZ^{[l]}</script><script type="math/tex; mode=display">dZ^{[l]}=W^{[l+1]T}\cdot dZ^{[l+1]}\ast g^{[l]'}(Z^{[l]})</script><h3 id="Parameters-vs-Hyperparameters"><a href="#Parameters-vs-Hyperparameters" class="headerlink" title="Parameters vs Hyperparameters"></a>Parameters vs Hyperparameters</h3><p>该部分介绍神经网络中的参数（parameters）和超参数（hyperparameters）的概念。</p>
<p>神经网络中的参数就是我们熟悉的$W^{[l]}$和$b^{[l]}$。而超参数则是例如学习速率$\alpha$，训练迭代次数N，神经网络层数L，各层神经元个数$n^{[l]}$，激活函数g(z)等。之所以叫做超参数的原因是它们决定了参数$W^{[l]}$和$b^{[l]}$的值。在后面的第二门课我们还将学习其它的超参数，这里先不讨论。</p>
<p>如何设置最优的超参数是一个比较困难的、需要经验知识的问题。通常的做法是选择超参数一定范围内的值，分别代入神经网络进行训练，测试cost function随着迭代次数增加的变化，根据结果选择cost function最小时对应的超参数值。这类似于validation的方法。</p>
<h3 id="What-does-this-have-to-do-with-the-brain"><a href="#What-does-this-have-to-do-with-the-brain" class="headerlink" title="What does this have to do with the brain?"></a>What does this have to do with the brain?</h3><p>那么，神经网络跟人脑机制到底有什么联系呢？究竟有多少的相似程度？神经网络实际上可以分成两个部分：正向传播过程和反向传播过程。神经网络的每个神经元采用激活函数的方式，类似于感知机模型。这种模型与人脑神经元是类似的，可以说是一种非常简化的人脑神经元模型。如下图所示，人脑神经元可分为树突、细胞体、轴突三部分。树突接收外界电刺激信号（类比神经网络中神经元输入），传递给细胞体进行处理（类比神经网络中神经元激活函数运算），最后由轴突传递给下一个神经元（类比神经网络中神经元输出）。</p>
<p>值得一提的是，人脑神经元的结构和处理方式要复杂的多，神经网络模型只是非常简化的模型。人脑如何进行学习？是否也是通过反向传播和梯度下降算法现在还不清楚，可能会更加复杂。这是值得生物学家探索的事情。也许发现重要的新的人脑学习机制后，让我们的神经网络模型抛弃反向传播和梯度下降算法，能够实现更加准确和强大的神经网络模型！</p>
<p><img src="/2018/02/21/深层神经网络/propagation" alt="propagation"></p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了深层神经网络，是上一节浅层神经网络的拓展和归纳。首先，我们介绍了建立神经网络模型一些常用的标准的标记符号。然后，用流程块图的方式详细推导正向传播过程和反向传播过程的输入输出和参数表达式。我们也从提取特征复杂性和计算量的角度分别解释了深层神经网络为什么优于浅层神经网络。接着，我们介绍了超参数的概念，解释了超参数与参数的区别。最后，我们将神经网络与人脑做了类别，人工神经网络是简化的人脑模型。</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      

      
        
          <div class="social-like">
            
              <div class="vk_like">
                <span id="vk_like"></span>
              </div>
            

            
          </div>
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/02/19/浅层神经网络/" rel="next" title="浅层神经网络">
                <i class="fa fa-chevron-left"></i> 浅层神经网络
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/07/12/python爬虫之模拟登陆新浪微博/" rel="prev" title="python爬虫之模拟登陆新浪微博">
                python爬虫之模拟登陆新浪微博 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
      <div class="ds-thread" data-thread-key="2018/02/21/深层神经网络/"
           data-title="深层神经网络" data-url="http://andeper.cn/2018/02/21/深层神经网络/">
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/update/avatar.jpg"
               alt="Andeper" />
          <p class="site-author-name" itemprop="name">Andeper</p>
           
              <p class="site-description motion-element" itemprop="description">欢迎来到我的技术博客</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">29</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/tags/index.html">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-L-layer-neural-network"><span class="nav-number">1.</span> <span class="nav-text">Deep L-layer neural network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Forward-Propagation-in-a-Deep-Network"><span class="nav-number">2.</span> <span class="nav-text">Forward Propagation in a Deep Network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Getting-your-matrix-dimensions-right"><span class="nav-number">3.</span> <span class="nav-text">Getting your matrix dimensions right</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Why-deep-representations"><span class="nav-number">4.</span> <span class="nav-text">Why deep representations?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Building-blocks-of-deep-neural-networks"><span class="nav-number">5.</span> <span class="nav-text">Building blocks of deep neural networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Forward-and-Backward-Propagation"><span class="nav-number">6.</span> <span class="nav-text">Forward and Backward Propagation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Parameters-vs-Hyperparameters"><span class="nav-number">7.</span> <span class="nav-text">Parameters vs Hyperparameters</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#What-does-this-have-to-do-with-the-brain"><span class="nav-number">8.</span> <span class="nav-text">What does this have to do with the brain?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary"><span class="nav-number">9.</span> <span class="nav-text">Summary</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Andeper</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"andeper"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  













  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



  

</body>
</html>
