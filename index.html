<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="Andeper的个人博客" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="欢迎来到我的技术博客">
<meta property="og:type" content="website">
<meta property="og:title" content="Andeper的个人博客">
<meta property="og:url" content="http://andeper.cn/index.html">
<meta property="og:site_name" content="Andeper的个人博客">
<meta property="og:description" content="欢迎来到我的技术博客">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Andeper的个人博客">
<meta name="twitter:description" content="欢迎来到我的技术博客">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://andeper.cn/"/>





  <title> Andeper的个人博客 </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
<!-- hexo-inject:begin --><!-- hexo-inject:end --><script>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '',
      xfbml      : true,
      version    : 'v2.6'
    });
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "//connect.facebook.net/zh_Hans/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>




	<div id="vk_api_transport"></div>
	<script type="text/javascript">
		window.vkAsyncInit = function() {
			VK.init({
				apiId: 
			});

			

			
		};
		setTimeout(function() {
			var el = document.createElement("script");
			el.type = "text/javascript";
			el.src = "//vk.com/js/api/openapi.js";
			el.async = true;
			document.getElementById("vk_api_transport").appendChild(el);
		}, 0);
	</script>





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?8aabc26c969f399d0abe524a29699f13";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Andeper的个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2019/06/27/动态规划专题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/06/27/动态规划专题/" itemprop="url">
                  动态规划专题
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-27T15:12:16+08:00">
                2019-06-27
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/27/动态规划专题/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/06/27/动态规划专题/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Easy"><a href="#Easy" class="headerlink" title="Easy"></a>Easy</h2><h3 id="Climbing-Stairs"><a href="#Climbing-Stairs" class="headerlink" title="Climbing Stairs"></a>Climbing Stairs</h3><p>You are climbing a stair case. It takes n steps to reach to the top.<br>Each time you can either climb 1 or 2 steps. In how many distinct ways can you climb to the top?<br>Note: Given n will be a positive integer.<br>Example 1:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Input: 2</span><br><span class="line">Output: 2</span><br><span class="line">Explanation: There are two ways to climb to the top.</span><br><span class="line">1. 1 step + 1 step</span><br><span class="line">2. 2 steps</span><br></pre></td></tr></table></figure></p>
<h4 id="题意"><a href="#题意" class="headerlink" title="题意"></a>题意</h4><p>就是爬楼梯一次能上一个台阶或者两个台阶，求爬楼梯方式的种数</p>
<h4 id="题解"><a href="#题解" class="headerlink" title="题解"></a>题解</h4><p>定义dp[i]为当前台阶方式种数<br>dp公式：dp[i]=dp[i-1]+dp[i-2];</p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">climbStairs</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">       <span class="keyword">int</span>[] a = <span class="keyword">new</span> <span class="keyword">int</span>[n+<span class="number">1</span>];</span><br><span class="line">       <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;a.length;i++)&#123;</span><br><span class="line">           <span class="keyword">if</span> (i == <span class="number">0</span>||i==<span class="number">1</span>)&#123;</span><br><span class="line">               a[i] = <span class="number">1</span>;</span><br><span class="line">           &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">               a[i]=a[i-<span class="number">1</span>]+a[i-<span class="number">2</span>];</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> a[a.length-<span class="number">1</span>];</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<h3 id="Maximum-Subarray"><a href="#Maximum-Subarray" class="headerlink" title="Maximum Subarray"></a>Maximum Subarray</h3><p>Given an integer array nums, find the contiguous subarray (containing at least one number) which has the largest sum and return its sum.</p>
<p>Example 1:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: [-2,1,-3,4,-1,2,1,-5,4],</span><br><span class="line">Output: 6</span><br><span class="line">Explanation: [4,-1,2,1] has the largest sum = 6.</span><br></pre></td></tr></table></figure></p>
<h4 id="题意-1"><a href="#题意-1" class="headerlink" title="题意"></a>题意</h4><p>求连续数组相加值最大的值</p>
<h4 id="题解-1"><a href="#题解-1" class="headerlink" title="题解"></a>题解</h4><p>num[i]表示数组当前的值<br>定义dp[i]表示以num[i]结尾的连续子数组的最大和<br>所以dp[i]的值等于以num[i]开头的数组和dp[i-1]加上当前值得最大值<br>dp公式：dp[i] = max(nums[i],nums[i]+dp[i-1]);</p>
<h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[nums.length];</span><br><span class="line">        <span class="keyword">int</span> max = dp[<span class="number">0</span>]=nums[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>;i&lt;nums.length;i++)&#123;</span><br><span class="line">            dp[i] = Math.max(nums[i],nums[i]+dp[i-<span class="number">1</span>]);</span><br><span class="line">            <span class="keyword">if</span> (dp[i]&gt;max)</span><br><span class="line">                max=dp[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> max;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="House-Robber"><a href="#House-Robber" class="headerlink" title="House Robber"></a>House Robber</h3><p>You are a professional robber planning to rob houses along a street. Each house has a certain amount of money stashed, the only constraint stopping you from robbing each of them is that adjacent houses have security system connected and it will automatically contact the police if two adjacent houses were broken into on the same night.</p>
<p>Given a list of non-negative integers representing the amount of money of each house, determine the maximum amount of money you can rob tonight without alerting the police.</p>
<p>Example 1:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input: [1,2,3,1]</span><br><span class="line">Output: 4</span><br><span class="line">Explanation: Rob house 1 (money = 1) and then rob house 3 (money = 3).</span><br><span class="line">             Total amount you can rob = 1 + 3 = 4.</span><br></pre></td></tr></table></figure></p>
<h4 id="题意-2"><a href="#题意-2" class="headerlink" title="题意"></a>题意</h4><p>不能连续抢两间房子求抢的最多钱数</p>
<h4 id="题解-2"><a href="#题解-2" class="headerlink" title="题解"></a>题解</h4><p>定义dp[i]为前i个房子抢的最大钱数<br>所以dp[i]的值为抢不抢第i个房子<br>如果抢第i个房子dp值为前i-2个房子钱数最大值加上当前房子钱数<br>如果不抢dp值为前i-1个房子钱数最大值<br>所以求两者最大值<br>dp公式：dp[i] = max(dp[i-1],dp[i-2]+nums[i]);</p>
<h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">rob</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.length==<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[nums.length];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;dp.length;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span> (i ==<span class="number">0</span>)</span><br><span class="line">                dp[i]=nums[<span class="number">0</span>];</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> (i==<span class="number">1</span>)&#123;</span><br><span class="line">                dp[i]=Math.max(nums[<span class="number">0</span>],nums[<span class="number">1</span>]);</span><br><span class="line">            &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                dp[i] = Math.max(dp[i-<span class="number">1</span>],dp[i-<span class="number">2</span>]+nums[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[dp.length-<span class="number">1</span>];</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h2 id="Medium"><a href="#Medium" class="headerlink" title="Medium"></a>Medium</h2><h3 id="Jump-Game"><a href="#Jump-Game" class="headerlink" title="Jump Game"></a>Jump Game</h3><p>Given an array of non-negative integers, you are initially positioned at the first index of the array.</p>
<p>Each element in the array represents your maximum jump length at that position.</p>
<p>Determine if you are able to reach the last index.</p>
<p>Example 1:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: [2,3,1,1,4]</span><br><span class="line">Output: true</span><br><span class="line">Explanation: Jump 1 step from index 0 to 1, then 3 steps to the last index.</span><br></pre></td></tr></table></figure></p>
<h4 id="题意-3"><a href="#题意-3" class="headerlink" title="题意"></a>题意</h4><p>nums[i]表示能往前跳几步</p>
<h4 id="题解-3"><a href="#题解-3" class="headerlink" title="题解"></a>题解</h4><p>dp[i]表示是否能跳到这一步，把dp[0]赋值为1，<br>如果dp[i]的值为true就把dp[i+1]到dp[i+nums[i]]的值赋值为true<br>其实怪怪的，感觉效率不高，也不是传统的dp解法，暂时没想到好解法</p>
<h4 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">canJump</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">boolean</span>[] dp = <span class="keyword">new</span> <span class="keyword">boolean</span>[nums.length];</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="keyword">true</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;nums.length;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span> (dp[i])&#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> j= i+<span class="number">1</span>;j&lt;=i+nums[i];j++)&#123;</span><br><span class="line">                    <span class="keyword">if</span> (j&gt;=nums.length)</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    dp[j] = <span class="keyword">true</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[nums.length-<span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Unique-Paths"><a href="#Unique-Paths" class="headerlink" title="Unique Paths"></a>Unique Paths</h3><p>A robot is located at the top-left corner of a m x n grid (marked ‘Start’ in the diagram below).</p>
<p>The robot can only move either down or right at any point in time. The robot is trying to reach the bottom-right corner of the grid (marked ‘Finish’ in the diagram below).</p>
<p>How many possible unique paths are there?<br>Example 1:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Input: m = 3, n = 2</span><br><span class="line">Output: 3</span><br><span class="line">Explanation:</span><br><span class="line">From the top-left corner, there are a total of 3 ways to reach the bottom-right corner:</span><br><span class="line">1. Right -&gt; Right -&gt; Down</span><br><span class="line">2. Right -&gt; Down -&gt; Right</span><br><span class="line">3. Down -&gt; Right -&gt; Right</span><br></pre></td></tr></table></figure></p>
<h4 id="题意-4"><a href="#题意-4" class="headerlink" title="题意"></a>题意</h4><p>二维的走楼梯题目，但每次只走一步就很简单</p>
<h4 id="题解-4"><a href="#题解-4" class="headerlink" title="题解"></a>题解</h4><p>dp[i][j]表示走到i行j列走的种数<br>dp公式：dp[i][j] = dp[i-1][j]+dp[i][j-1]<br>很简单</p>
<h4 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">uniquePaths</span><span class="params">(<span class="keyword">int</span> m, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(m==<span class="number">1</span>&amp;&amp;n==<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[m][n];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;m;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>;j&lt;n;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span> (i ==<span class="number">0</span>&amp;&amp;j==<span class="number">0</span>)&#123;</span><br><span class="line">                    dp[i][j] =<span class="number">0</span>;</span><br><span class="line">                &#125;<span class="keyword">else</span> <span class="keyword">if</span> (i==<span class="number">0</span>||j==<span class="number">0</span>)&#123;</span><br><span class="line">                    dp[i][j] = <span class="number">1</span>;</span><br><span class="line">                &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                    dp[i][j] = dp[i-<span class="number">1</span>][j]+dp[i][j-<span class="number">1</span>];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[m-<span class="number">1</span>][n-<span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Coin-Change"><a href="#Coin-Change" class="headerlink" title="Coin Change"></a>Coin Change</h3><p>You are given coins of different denominations and a total amount of money amount. Write a function to compute the fewest number of coins that you need to make up that amount. If that amount of money cannot be made up by any combination of the coins, return -1.</p>
<p>Example 1:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: coins = [1, 2, 5], amount = 11</span><br><span class="line">Output: 3</span><br><span class="line">Explanation: 11 = 5 + 5 + 1</span><br></pre></td></tr></table></figure></p>
<h4 id="题意-5"><a href="#题意-5" class="headerlink" title="题意"></a>题意</h4><p>给出硬币种数，求换硬币最少硬币数</p>
<h4 id="题解-5"><a href="#题解-5" class="headerlink" title="题解"></a>题解</h4><p>dp[i]表示换i块钱的最小硬币数<br>dp公式：dp[i] = Math.min(dp[i-coins[1]]+1, $\dots$,dp[i-coins[n]]+1)</p>
<h4 id="代码-5"><a href="#代码-5" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">coinChange</span><span class="params">(<span class="keyword">int</span>[] coins, <span class="keyword">int</span> amount)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (amount==<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[amount+<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> coin:coins)&#123;</span><br><span class="line">            <span class="keyword">if</span> (coin&lt;=amount)</span><br><span class="line">                dp[coin]=<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;=amount;i++)&#123;</span><br><span class="line">            <span class="keyword">if</span> (dp[i]==<span class="number">0</span>)&#123;</span><br><span class="line">                dp[i] = Integer.MAX_VALUE;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> coin:coins)&#123;</span><br><span class="line">                <span class="keyword">if</span> (i-coin&gt;=<span class="number">0</span>)&#123;</span><br><span class="line">                    <span class="keyword">if</span> (dp[i-coin]!=Integer.MAX_VALUE)&#123;</span><br><span class="line">                        dp[i] = Math.min(dp[i],dp[i-coin]+<span class="number">1</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (dp[amount]==Integer.MAX_VALUE)</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> dp[amount];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Longest-Increasing-Subsequence"><a href="#Longest-Increasing-Subsequence" class="headerlink" title="Longest Increasing Subsequence"></a>Longest Increasing Subsequence</h3><p>Given an unsorted array of integers, find the length of longest increasing subsequence.</p>
<p>Example 1:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: [10,9,2,5,3,7,101,18]</span><br><span class="line">Output: 4</span><br><span class="line">Explanation: The longest increasing subsequence is [2,3,7,101], therefore the length is 4.</span><br></pre></td></tr></table></figure></p>
<h4 id="题意-6"><a href="#题意-6" class="headerlink" title="题意"></a>题意</h4><p>求数组最长的连续子序列</p>
<h4 id="题解-6"><a href="#题解-6" class="headerlink" title="题解"></a>题解</h4><p>dp[i]表示以nums[i]结尾的最长子序列长度<br>如果当前数大于之前最长子序列最大值，在此基础上加1<br>所以dp公式为：<br>dp[i] = Math.max(dp[i],dp[j]+1)<br>$j\in [0,i]$</p>
<h4 id="代码-6"><a href="#代码-6" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">lengthOfLIS</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.length==<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span>[] dp =<span class="keyword">new</span> <span class="keyword">int</span>[nums.length];</span><br><span class="line">        <span class="keyword">int</span> max = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;nums.length;i++)&#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j =i;j&gt;=<span class="number">0</span>;j--)&#123;</span><br><span class="line">                <span class="keyword">if</span> (i == j)</span><br><span class="line">                    dp[i] =<span class="number">1</span>;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (nums[i]&gt;nums[j])</span><br><span class="line">                    dp[i] = Math.max(dp[i],dp[j]+<span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (dp[i]&gt;max)</span><br><span class="line">                max = dp[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> max;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Hard"><a href="#Hard" class="headerlink" title="Hard"></a>Hard</h2><h3 id="Maximum-Product-Subarray"><a href="#Maximum-Product-Subarray" class="headerlink" title="Maximum Product Subarray"></a>Maximum Product Subarray</h3><p>Given an integer array nums, find the contiguous subarray within an array (containing at least one number) which has the largest product.</p>
<p>Example 1:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: [2,3,-2,4]</span><br><span class="line">Output: 6</span><br><span class="line">Explanation: [2,3] has the largest product 6.</span><br></pre></td></tr></table></figure></p>
<h4 id="题意-7"><a href="#题意-7" class="headerlink" title="题意"></a>题意</h4><p>求连续乘积最大值</p>
<h4 id="题解-7"><a href="#题解-7" class="headerlink" title="题解"></a>题解</h4><p>dp[j]表示以nums[j]开头nums[i]结尾的乘积<br>因为这个题目是二维dp，其实还是可以用dp[j][i]表示以nums[j]开头nums[i]结尾的乘积<br>只是用一维数组表示节省空间<br>每次算dp[i]的时候从后往前计算，先算[i,i]的乘积，再算[i-1,i]的乘积，直到算到[0,i]的乘积取最大值<br>所以dp公式为：<br>dp[j] = dp[j+1]*nums[j];</p>
<h4 id="代码-7"><a href="#代码-7" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">maxProduct</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.length==<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[nums.length];</span><br><span class="line">        <span class="keyword">int</span> max = Integer.MIN_VALUE;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;nums.length;i++ )&#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = i;j&gt;=<span class="number">0</span>;j--)&#123;</span><br><span class="line">                <span class="keyword">if</span> (i == j)&#123;</span><br><span class="line">                    dp[j] = nums[i];</span><br><span class="line">                &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                    dp[j] = dp[j+<span class="number">1</span>]*nums[j];</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (dp[j]&gt;max)</span><br><span class="line">                    max = dp[j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> max;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Decode-Ways"><a href="#Decode-Ways" class="headerlink" title="Decode Ways"></a>Decode Ways</h3><p>A message containing letters from A-Z is being encoded to numbers using the following mapping:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&apos;A&apos; -&gt; 1</span><br><span class="line">&apos;B&apos; -&gt; 2</span><br><span class="line">...</span><br><span class="line">&apos;Z&apos; -&gt; 26</span><br></pre></td></tr></table></figure></p>
<p>Example 1:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: &quot;12&quot;</span><br><span class="line">Output: 2</span><br><span class="line">Explanation: It could be decoded as &quot;AB&quot; (1 2) or &quot;L&quot; (12).</span><br></pre></td></tr></table></figure></p>
<h4 id="题意-8"><a href="#题意-8" class="headerlink" title="题意"></a>题意</h4><p>把数字字符串编码为字母<br>给出数字字符串求能编码的方式<br>dp思路很简单但是分情况比较复杂<br>dp[i]b</p>
<h4 id="题解-8"><a href="#题解-8" class="headerlink" title="题解"></a>题解</h4><p>dp思路很简单但是分情况比较复杂<br>dp[i]以字符串第i个字母结尾的编码方式<br>dp公式：<br>如果最后两个数字大于10小于26且不等于10或20：<br>dp[i] = dp[i-1]+dp[i-2];<br>如果最后两个数字是10或20:<br>dp[i] = dp[i-2];<br>如果最后两个数字大于26或者在1到9之间：<br>dp[i] = dp[i-2];<br>其他情况<br>dp[i] = 0;</p>
<h4 id="代码-8"><a href="#代码-8" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">numDecodings</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (s.length() == <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[s.length()];</span><br><span class="line">        <span class="keyword">if</span> (s.charAt(<span class="number">0</span>)!=<span class="string">'0'</span>)</span><br><span class="line">            dp[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span> (s.length() &gt;<span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">int</span> n = Integer.valueOf(s.substring(<span class="number">0</span>,<span class="number">2</span>));</span><br><span class="line">            <span class="keyword">if</span> (n&gt;=<span class="number">10</span>&amp;&amp;n&lt;=<span class="number">26</span>)&#123;</span><br><span class="line">                <span class="keyword">if</span> (n == <span class="number">10</span>||n==<span class="number">20</span>)&#123;</span><br><span class="line">                    dp[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">                &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                    dp[<span class="number">1</span>] = <span class="number">2</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span> (n&gt;<span class="number">26</span>&amp;&amp;s.charAt(<span class="number">1</span>)!=<span class="string">'0'</span>)</span><br><span class="line">                dp[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>;i&lt;s.length();i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> n = Integer.valueOf(s.substring(i-<span class="number">1</span>,i+<span class="number">1</span>));</span><br><span class="line">            <span class="keyword">if</span> (n&gt;=<span class="number">10</span>&amp;&amp;n&lt;=<span class="number">26</span>)&#123;</span><br><span class="line">                <span class="keyword">if</span> (n == <span class="number">10</span>||n==<span class="number">20</span>)&#123;</span><br><span class="line">                    dp[i] = dp[i-<span class="number">2</span>];</span><br><span class="line">                &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                    dp[i] = dp[i-<span class="number">1</span>]+dp[i-<span class="number">2</span>];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span> ((n&gt;<span class="number">26</span>&amp;&amp;s.charAt(i)!=<span class="string">'0'</span>)||(n&gt;<span class="number">0</span>&amp;&amp;n&lt;<span class="number">10</span>))&#123;</span><br><span class="line">                dp[i] = dp[i-<span class="number">1</span>];</span><br><span class="line">            &#125;<span class="keyword">else</span></span><br><span class="line">                dp[i] = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[dp.length-<span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Best-Time-to-Buy-and-Sell-Stock-with-Cooldown"><a href="#Best-Time-to-Buy-and-Sell-Stock-with-Cooldown" class="headerlink" title="Best Time to Buy and Sell Stock with Cooldown"></a>Best Time to Buy and Sell Stock with Cooldown</h3><p>Say you have an array for which the ith element is the price of a given stock on day i.</p>
<p>Design an algorithm to find the maximum profit. You may complete as many transactions as you like (ie, buy one and sell one share of the stock multiple times) with the following restrictions:</p>
<p>You may not engage in multiple transactions at the same time (ie, you must sell the stock before you buy again).<br>After you sell your stock, you cannot buy stock on next day. (ie, cooldown 1 day)</p>
<p>Example:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: [1,2,3,0,2]</span><br><span class="line">Output: 3</span><br><span class="line">Explanation: transactions = [buy, sell, cooldown, buy, sell]</span><br></pre></td></tr></table></figure></p>
<h4 id="题意-9"><a href="#题意-9" class="headerlink" title="题意"></a>题意</h4><p>买卖股票但是卖完之后要隔一天才能买</p>
<h4 id="题解-9"><a href="#题解-9" class="headerlink" title="题解"></a>题解</h4><p>特殊的dp，也比较难想<br>定义buy[i]表示第i天的操作是买的赚钱最大值<br>定义sell[i]表示第i天的操作是卖的赚钱最大值<br>prices[i]表示第i天的股票价格<br>所以初始化的时候第一天买buy[0] = -prices[0]<br>第一天不能卖sell[0] = 0<br>所以buy[i]包含两种情况的最大值：第i天买不买，买的话buy[i] = sell[i-2]+prices[i],不买的话buy[i] = buy[i-1]<br>sell[i]也包含两种情况的最大值：第i天卖不卖，卖的话sell[i] = buy[i-1]+prices[i],不卖的话sell[i] = sell[i-1]<br>dp公式：<br>buy[i] = max(buy[i-1],sell[i-2]-prices[i]);<br>sell[i] = max(sell[i-1],buy[i-1]+prices[i]);</p>
<h4 id="代码-9"><a href="#代码-9" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">maxProfit</span><span class="params">(<span class="keyword">int</span>[] prices)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] buy = <span class="keyword">new</span> <span class="keyword">int</span>[prices.length];</span><br><span class="line">        <span class="keyword">int</span>[] sell = <span class="keyword">new</span> <span class="keyword">int</span>[prices.length];</span><br><span class="line">        <span class="keyword">if</span> (prices.length&lt;<span class="number">2</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        buy[<span class="number">0</span>] = <span class="number">0</span>-prices[<span class="number">0</span>];</span><br><span class="line">        sell[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        buy[<span class="number">1</span>] = Math.max(buy[<span class="number">0</span>],<span class="number">0</span>-prices[<span class="number">1</span>]);</span><br><span class="line">        sell[<span class="number">1</span>] = Math.max(sell[<span class="number">0</span>],prices[<span class="number">1</span>]-prices[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>;i&lt;prices.length;i++)&#123;</span><br><span class="line">            buy[i] = Math.max(buy[i-<span class="number">1</span>],sell[i-<span class="number">2</span>]-prices[i]);</span><br><span class="line">            sell[i] = Math.max(sell[i-<span class="number">1</span>],buy[i-<span class="number">1</span>]+prices[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> sell[prices.length-<span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Perfect-Squares"><a href="#Perfect-Squares" class="headerlink" title="Perfect Squares"></a>Perfect Squares</h3><p>Given a positive integer n, find the least number of perfect square numbers (for example, 1, 4, 9, 16, …) which sum to n.</p>
<p>Example 1:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: n = 12</span><br><span class="line">Output: 3</span><br><span class="line">Explanation: 12 = 4 + 4 + 4.</span><br></pre></td></tr></table></figure></p>
<h4 id="题意-10"><a href="#题意-10" class="headerlink" title="题意"></a>题意</h4><p>求一个数是平方数的和的最少平方数个数</p>
<h4 id="题解-10"><a href="#题解-10" class="headerlink" title="题解"></a>题解</h4><p>dp[i]表示以相加为i的最少平方数个数<br>求dp[i]时，遍历小于它的平方数，求最小值<br>dp[i] = min(dp[i-j<em>j]+1)<br>$j</em>j\in[1,i]$</p>
<h4 id="代码-10"><a href="#代码-10" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">numSquares</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[n+<span class="number">1</span>];</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>;i&lt;=n;i++)&#123;</span><br><span class="line">            dp[i] = Integer.MAX_VALUE;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>;j*j&lt;=i;j++)&#123;</span><br><span class="line">                dp[i] = Math.min(dp[i-j*j]+<span class="number">1</span>,dp[i]);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[n];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Word-Break"><a href="#Word-Break" class="headerlink" title="Word Break"></a>Word Break</h3><p>Given a non-empty string s and a dictionary wordDict containing a list of non-empty words, determine if s can be segmented into a space-separated sequence of one or more dictionary words.</p>
<p>Note:</p>
<p>The same word in the dictionary may be reused multiple times in the segmentation.<br>You may assume the dictionary does not contain duplicate words.</p>
<p>Example 1:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: s = &quot;leetcode&quot;, wordDict = [&quot;leet&quot;, &quot;code&quot;]</span><br><span class="line">Output: true</span><br><span class="line">Explanation: Return true because &quot;leetcode&quot; can be segmented as &quot;leet code&quot;.</span><br></pre></td></tr></table></figure></p>
<h4 id="题意-11"><a href="#题意-11" class="headerlink" title="题意"></a>题意</h4><p>求一个字符串是否能被字典中的词拼接</p>
<h4 id="题解-11"><a href="#题解-11" class="headerlink" title="题解"></a>题解</h4><p>dp[i]表示以第i个字母为结尾的字符串是否能被字典中的词拼接<br>如果以第i个字母为结尾的字符串是词典中的词那么dp[i] = true<br>如果以第i个字母为结尾的字符串尾部为词典中的词那么判断dp[i-词的长度]是不是为true<br>如果是那么dp[i] = true<br>遍历字典</p>
<h4 id="代码-11"><a href="#代码-11" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">wordBreak</span><span class="params">(String s, List&lt;String&gt; wordDict)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">boolean</span>[] dp = <span class="keyword">new</span> <span class="keyword">boolean</span>[s.length()];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;s.length();i++)&#123;</span><br><span class="line">            String sub = s.substring(<span class="number">0</span>,i+<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">if</span> (wordDict.contains(sub))</span><br><span class="line">                dp[i] = <span class="keyword">true</span>;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>;j&lt;i;j++)&#123;</span><br><span class="line">                    <span class="keyword">if</span> (dp[j])&#123;</span><br><span class="line">                        String sub1 = s.substring(j+<span class="number">1</span>,i+<span class="number">1</span>);</span><br><span class="line">                        <span class="keyword">if</span> (wordDict.contains(sub1)) &#123;</span><br><span class="line">                            dp[i] = <span class="keyword">true</span>;</span><br><span class="line">                            <span class="keyword">break</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[s.length() - <span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Word-Break-II"><a href="#Word-Break-II" class="headerlink" title="Word Break II"></a>Word Break II</h3><p>Given a non-empty string s and a dictionary wordDict containing a list of non-empty words, add spaces in s to construct a sentence where each word is a valid dictionary word. Return all such possible sentences.</p>
<p>Note:</p>
<p>The same word in the dictionary may be reused multiple times in the segmentation.<br>You may assume the dictionary does not contain duplicate words.</p>
<p>Example 1:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Input:</span><br><span class="line">s = &quot;catsanddog&quot;</span><br><span class="line">wordDict = [&quot;cat&quot;, &quot;cats&quot;, &quot;and&quot;, &quot;sand&quot;, &quot;dog&quot;]</span><br><span class="line">Output:</span><br><span class="line">[</span><br><span class="line">  &quot;cats and dog&quot;,</span><br><span class="line">  &quot;cat sand dog&quot;</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p>
<h4 id="题意-12"><a href="#题意-12" class="headerlink" title="题意"></a>题意</h4><p>求一个字符串被字典中的词拼接的所有组合</p>
<h4 id="题解-12"><a href="#题解-12" class="headerlink" title="题解"></a>题解</h4><p>使用dp+dfs相结合的方法<br>因为dp是自底向上的，dfs是自顶向下的<br>第一步使用dp存储字符串被组合的所有路径，第二步使用dfs求出所有组合<br>空间效率满分<br>dp[i]为一个数组，存储能构成以第i个字符结尾的字符串尾部的字典中的词<br>dp公式与上题一样</p>
<h4 id="代码-12"><a href="#代码-12" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;String&gt; <span class="title">wordBreak</span><span class="params">(String s, List&lt;String&gt; wordDict)</span> </span>&#123;</span><br><span class="line">        ArrayList[] dp = <span class="keyword">new</span> ArrayList[s.length()];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;s.length();i++)&#123;</span><br><span class="line">            String sub = s.substring(<span class="number">0</span>,i+<span class="number">1</span>);</span><br><span class="line">            <span class="keyword">if</span> (wordDict.contains(sub)) &#123;</span><br><span class="line">                dp[i] = <span class="keyword">new</span> ArrayList();</span><br><span class="line">                dp[i].add(sub);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>;j&lt;i;j++)&#123;</span><br><span class="line">                <span class="keyword">if</span> (dp[j]!=<span class="keyword">null</span>)&#123;</span><br><span class="line">                    String sub1 = s.substring(j+<span class="number">1</span>,i+<span class="number">1</span>);</span><br><span class="line">                    <span class="keyword">if</span> (wordDict.contains(sub1)) &#123;</span><br><span class="line">                        <span class="keyword">if</span> (dp[i] == <span class="keyword">null</span>)</span><br><span class="line">                            dp[i] = <span class="keyword">new</span> ArrayList();</span><br><span class="line">                        dp[i].add(sub1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        List&lt;String&gt; res = <span class="keyword">new</span> LinkedList&lt;String&gt;();;</span><br><span class="line">        <span class="keyword">if</span> (dp[s.length()-<span class="number">1</span>] == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> res;</span><br><span class="line">        dfs(dp,s.length(),res,<span class="keyword">new</span> ArrayList&lt;&gt;());</span><br><span class="line">        Collections.reverse(res);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">dfs</span><span class="params">(List&lt;String&gt;[] dp, <span class="keyword">int</span> end, List&lt;String&gt; res , List&lt;String&gt; tmp)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(end&lt;= <span class="number">0</span>)&#123;</span><br><span class="line">            String s = <span class="string">""</span>;</span><br><span class="line">            <span class="keyword">for</span> (String stemp:tmp)&#123;</span><br><span class="line">                s = stemp+ <span class="string">" "</span> + s;</span><br><span class="line">            &#125;</span><br><span class="line">            s = s.substring(<span class="number">0</span>,s.length()-<span class="number">1</span>);</span><br><span class="line">            res.add(s);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span> (String word:dp[end-<span class="number">1</span>])&#123;</span><br><span class="line">            tmp.add(word);</span><br><span class="line">            dfs(dp,end-word.length(),res,tmp);</span><br><span class="line">            tmp.remove(tmp.size()-<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Burst-Balloons"><a href="#Burst-Balloons" class="headerlink" title="Burst Balloons"></a>Burst Balloons</h3><p>Given n balloons, indexed from 0 to n-1. Each balloon is painted with a number on it represented by array nums. You are asked to burst all the balloons. If the you burst balloon i you will get nums[left] <em> nums[i] </em> nums[right] coins. Here left and right are adjacent indices of i. After the burst, the left and right then becomes adjacent.</p>
<p>Find the maximum coins you can collect by bursting the balloons wisely.<br>Example 1:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input: [3,1,5,8]</span><br><span class="line">Output: 167</span><br><span class="line">Explanation: nums = [3,1,5,8] --&gt; [3,5,8] --&gt;   [3,8]   --&gt;  [8]  --&gt; []</span><br><span class="line">             coins =  3*1*5      +  3*5*8    +  1*3*8      + 1*8*1   = 167</span><br></pre></td></tr></table></figure></p>
<h4 id="题意-13"><a href="#题意-13" class="headerlink" title="题意"></a>题意</h4><p>给出一串气球，每个气球包含一个数字，拍烂一个气球，爆的钱数为这个气球的数字和相邻气球数字的乘积，求最大的钱数<br>左右两边的气球相邻的钱数为1</p>
<h4 id="题解-13"><a href="#题解-13" class="headerlink" title="题解"></a>题解</h4><p>dp[start][end]表示打烂[start,end]区间的气球的钱数最大值<br>dp公式为：dp[start][end] = max(dp[start][i-1]+dp[i+1][end]+nums[i-1]*nums[i]=nums[i+1])<br>$i\in [start,end]$<br>迭代顺序有点讲究，end-start的值依次从0到length-1迭代</p>
<h4 id="代码-13"><a href="#代码-13" class="headerlink" title="代码"></a>代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">maxCoins</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.length ==<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span>[] newnums = <span class="keyword">new</span> <span class="keyword">int</span>[nums.length+<span class="number">2</span>];</span><br><span class="line">        newnums[<span class="number">0</span>] =<span class="number">1</span>;</span><br><span class="line">        newnums[newnums.length-<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">        <span class="comment">//在数组两端添加1;</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;nums.length;i++)</span><br><span class="line">            newnums[i+<span class="number">1</span>] = nums[i];</span><br><span class="line">        <span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[nums.length+<span class="number">1</span>][nums.length+<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> delta = <span class="number">0</span>;delta&lt;nums.length;delta++)&#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> start = <span class="number">1</span>;start+delta&lt;newnums.length-<span class="number">1</span>;start++)&#123;</span><br><span class="line">                <span class="keyword">int</span> end = start+delta;</span><br><span class="line">                <span class="keyword">if</span> (delta ==<span class="number">0</span>) &#123;</span><br><span class="line">                    dp[start][end] = newnums[start-<span class="number">1</span>]*newnums[start]*newnums[start+<span class="number">1</span>];</span><br><span class="line">                &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">                    dp[start][end] = Math.max(newnums[start-<span class="number">1</span>]*newnums[start]*newnums[end+<span class="number">1</span>]+dp[start+<span class="number">1</span>][end],newnums[start-<span class="number">1</span>]*newnums[end]*newnums[end+<span class="number">1</span>]+dp[start][end-<span class="number">1</span>]);;</span><br><span class="line">                    <span class="keyword">for</span> (<span class="keyword">int</span> i = start+<span class="number">1</span>;i&lt;end;i++)</span><br><span class="line">                        dp[start][end] = Math.max(dp[start][end],dp[start][i-<span class="number">1</span>]+newnums[start-<span class="number">1</span>]*newnums[i]*newnums[end+<span class="number">1</span>]+dp[i+<span class="number">1</span>][end]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">1</span>][nums.length];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2019/06/24/2019/验收/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/06/24/2019/验收/" itemprop="url">
                  验收
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-24T15:35:00+08:00">
                2019-06-24
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/24/2019/验收/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/06/24/2019/验收/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>pzj/sentiment analysis/new-words-discovery-master/new_word_discover.ipynb<br>新词发现<br>统计词频  互信息统计量 邻接熵统计量<br>阈值筛选<br>py3<br>pzj/sentiment analysis/new-words-discovery-master/pos60000utf8.txt<br>微博语料</p>
<p>pzj/sentiment analysis/sentence_sim_cluster/weibo_cluster.ipynb<br>聚类分析<br>[112]  weibo_topic[i][0:500]拿到文本<br>tf-idf加权词向量<br>平均词向量<br>lda<br>lsi</p>
<p>STCSMM<br>‘/home/inet702/wyl/ipython/pzj/atec_nlp/online/sentence_sim/data/new_sen_sim_train.txt’<br>训练语义匹配模型训练集<br>[3]</p>
<p>阿里金服比赛<br><a href="https://10.245.142.238:9999/edit/pzj/atec_nlp/online/sentence_sim/train_kerasv3.py" target="_blank" rel="noopener">https://10.245.142.238:9999/edit/pzj/atec_nlp/online/sentence_sim/train_kerasv3.py</a><br>文本1 文本2 相似/不相似<br>模型 卷积<br>esim</p>
<p>finnum<br><a href="https://10.245.142.238:9999/notebooks/pzj/FinNum/NTCIR14-FinNum/notebook/1.0-finum_classification.ipynb" target="_blank" rel="noopener">https://10.245.142.238:9999/notebooks/pzj/FinNum/NTCIR14-FinNum/notebook/1.0-finum_classification.ipynb</a><br><a href="https://10.245.142.238:9999/edit/pzj/FinNum/NTCIR14-FinNum/src/finnum_model.py" target="_blank" rel="noopener">https://10.245.142.238:9999/edit/pzj/FinNum/NTCIR14-FinNum/src/finnum_model.py</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2019/06/17/2019/BERT源码/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/06/17/2019/BERT源码/" itemprop="url">
                  BERT源码
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-17T15:30:00+08:00">
                2019-06-17
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/17/2019/BERT源码/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/06/17/2019/BERT源码/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="math/tex; mode=display">pooler\ output</script><script type="math/tex; mode=display">\uparrow Tanh(Linear())</script><script type="math/tex; mode=display">enconder\ output</script><script type="math/tex; mode=display">\uparrow *12层</script><script type="math/tex; mode=display">output_{(768,)}</script><script type="math/tex; mode=display">\uparrow LayerNorm(dropout(input)+x)</script><script type="math/tex; mode=display">[x]_{(768,)}</script><script type="math/tex; mode=display">\uparrow Linear(input)</script><script type="math/tex; mode=display">[x]_{(3072,)}</script><script type="math/tex; mode=display">\uparrow dropout(Relu(Linear(input)))</script><script type="math/tex; mode=display">[x]_{(768,)}</script><script type="math/tex; mode=display">\uparrow LayerNorm(dropout(input)+x)</script><script type="math/tex; mode=display">MultiHead(Q,K,V)=Concat(head_1,\dots,head_h)W^O</script><script type="math/tex; mode=display">\uparrow head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)</script><script type="math/tex; mode=display">[att]_{(768,)}</script><script type="math/tex; mode=display">\uparrow att = softmax(\frac{Q*K^T}{\sqrt{d_k}}+mask)*V</script><script type="math/tex; mode=display">Q_{(768,)},K_{(768,)},V_{(768,)},mask</script><script type="math/tex; mode=display">\uparrow 线性变换</script><script type="math/tex; mode=display">[x]_{(768,)},mask</script><p><img src="/assets/the-annotated-transformer_33_0.png" alt="the-annotated-transformer_33_0"><br><img src="/assets/the-annotated-transformer_38_0.png" alt="the-annotated-transformer_38_0"></p>
<p>输入部分<br>它的输入部分是个线性序列，两个句子通过分隔符分割，最前面和最后增加两个标识符号。每个单词有三个embedding:位置信息embedding，这是因为NLP中单词顺序是很重要的特征，需要在这里对位置信息进行编码；单词embedding,这个就是我们之前一直提到的单词embedding；第三个是句子embedding，因为前面提到训练数据都是由两个句子构成的，那么每个句子有个句子整体的embedding项对应给每个单词。把单词对应的三个embedding叠加，就形成了Bert的输入。<br><img src="/assets/v2-3898b02c6b71662a1076b6621a6661a2_r.jpg" alt="v2-3898b02c6b71662a1076b6621a6661a2_r"><br>input_ids  : [ListOf] token的id，在chinese模式中就是每个分词的id，对应一个word vector<br>input_mask : [ListOfInt] 真实字符对应1，补全字符对应0<br>segment_ids: [ListOfInt] 句子标识符，第一句全为0，第二句全为1<br>label_id   : [ListOfInt] 将Label_list转化为相应的id表示<br><img src="/assets/Snipaste_2019-06-25_09-57-15.png" alt="Snipaste_2019-06-25_09-57-15"></p>
<p>mask任务<br>随机选择语料中15%的单词，把它抠掉，也就是用[Mask]掩码代替原始单词，然后要求模型去正确预测被抠掉的单词。但是这里有个问题：训练过程大量看到[mask]标记，但是真正后面用的时候是不会有这个标记的，这会引导模型认为输出是针对[mask]这个标记的，但是实际使用又见不到这个标记，这自然会有问题。为了避免这个问题，Bert改造了一下，15%的被上天选中要执行[mask]替身这项光荣任务的单词中，只有80%真正被替换成[mask]标记，10%被狸猫换太子随机替换成另外一个单词，10%情况这个单词还待在原地不做改动。这就是Masked双向语音模型的具体做法。<br><img src="/assets/BERT-language-modeling-masked-lm.png" alt="BERT-language-modeling-masked-lm"><br>下一句任务<br>一种是选择语料中真正顺序相连的两个句子；另外一种是第二个句子从语料库中抛色子，随机选择一个拼到第一个句子后面。我们要求模型除了做上述的Masked语言模型任务外，附带再做个句子关系预测，判断第二个句子是不是真的是第一个句子的后续句子。之所以这么做，是考虑到很多NLP任务是句子关系判断任务，单词预测粒度的训练到不了句子关系这个层级，增加这个任务有助于下游句子关系判断任务。<br><img src="/assets/20181205171102586.png" alt="20181205171102586"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2019/06/09/Attention机制/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/06/09/Attention机制/" itemprop="url">
                  Attention机制
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-09T21:06:06+08:00">
                2019-06-09
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/09/Attention机制/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/06/09/Attention机制/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>我们有编码器隐藏层状态$h_1,\dots,h_N\in R^h$<br>在时间步t时，我们有解码器隐藏层状态 $s_t\in R^h$<br>对于这步我们得到注意力分数$e^t$：</p>
<script type="math/tex; mode=display">e^t=[s_t^Th_1,\dots,s_t^Th_N]\in R^N</script><p>使用softmax函数得到注意力的分布式概率表示：</p>
<script type="math/tex; mode=display">\alpha^t = softmax(e^t)\in R^N</script><p>使用与编码器隐藏层权重相加得到注意力输出$a_t$</p>
<script type="math/tex; mode=display">a_t = \sum_{i=1}^N\alpha^t_ih_i\in R^N</script><p>最后把隐藏层状态与注意力输出相连：</p>
<script type="math/tex; mode=display">[a_t;s_t]\in R^{2h}</script>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2019/05/23/文献摘要之Distant-Supervision-for-Relation-Extraction-via-Piecewise-Convolutional-Neural-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/05/23/文献摘要之Distant-Supervision-for-Relation-Extraction-via-Piecewise-Convolutional-Neural-Networks/" itemprop="url">
                  文献摘要之Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-23T10:56:16+08:00">
                2019-05-23
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/23/文献摘要之Distant-Supervision-for-Relation-Extraction-via-Piecewise-Convolutional-Neural-Networks/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/05/23/文献摘要之Distant-Supervision-for-Relation-Extraction-via-Piecewise-Convolutional-Neural-Networks/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Two problems arise when using distant supervision for relation extraction. First, in this method, an already existing knowledge base is heuristically aligned to texts, and the alignment results are treated as labeled data. However, the heuristic alignment can fail, resulting in wrong label problem. In addition, in previous approaches, statistical models have typically been applied to ad hoc features. The noise that originates from the feature extraction process can cause poor performance.<br>In this paper, we propose a novel model dubbed the Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address these two problems. To solve the first problem, distant supervised relation extraction is treated as a multi-instance problem in which the uncertainty of instance labels is taken into account. To address the latter problem, we avoid feature engineering and instead adopt convolutional architecture with piecewise max pooling to automatically learn relevant features. Experiments show that our method is effective and outperforms several competitive baseline methods.<br>1099/5000<br>使用远程监督进行关系提取时会出现两个问题。首先，在该方法中，已经存在的知识库启发式地与文本对齐，并且对齐结果被视为标记数据。但是，启发式对齐可能会失败，从而导致错误的标签问题。此外，在先前的方法中，统计模型通常已应用于特设特征。源自特征提取过程的噪声可能导致性能不佳。<br>在本文中，我们提出了一个新的模型，称为分段卷积神经网络（PCNN）与多实例学习，以解决这两个问题。为了解决第一个问题，远程监督关系提取被视为多实例问题，其中考虑了实例标签的不确定性。为了解决后一个问题，我们避免使用特征工程，而是采用具有分段最大池的卷积体系结构来自动学习相关特征。实验表明，我们的方法是有效的，并且优于几种竞争性基线方法。</p>
<p>Distant supervised relation extraction is formulated as multi-instance problem. In this section, we present innovative solutions that incorporate multi-instance learning into a convolutional neural network to fulfill this task. PCNNs are proposed for the automatic learning of features without complicated NLP preprocessing. Figure 3 shows our neural network architecture for distant supervised relation extraction. It illustrates the procedure that handles one instance of a bag. This procedure includes four main parts: Vector Representation, Convolution, Piecewise Max Pooling and Softmax Output. We describe these parts in detail below.<br>远程监督关系提取被公式化为多实例问题。 在本节中，我们提出创新的解决方案，将多实例学习结合到卷积神经网络中以完成此任务。 PCNN被提议用于自动学习特征而无需复杂的NLP预处理。 图3显示了我们用于远程监督关系提取的神经网络架构。 它说明了处理一个包实例的过程。 此过程包括四个主要部分：矢量表示，卷积，分段最大池和Softmax输出。 我们在下面详细描述这些部分。</p>
<p>Word embeddings are distributed representations of words that map each word in a text to a ‘k’- dimensional real-valued vector. They have recently been shown to capture both semantic and syntactic information about words very well, setting performance records in several word similarity tasks (Mikolov et al., 2013; Pennington et al., 2014). Using word embeddings that have been trained a priori has become common practice for enhancing many other NLP tasks<br>单词嵌入是将文本中的每个单词映射到’k’维实值向量的单词的分布式表示。 最近他们被证明可以很好地捕获关于单词的语义和句法信息，在几个单词相似性任务中设置性能记录（Mikolov等，2013; Pennington等，2014）。 使用先验训练的单词嵌入已成为增强许多其他NLP任务的常用做法</p>
<p>In relation extraction, we focus on assigning labels to entity pairs. Similar to Zeng et al. (2014), we use PFs to specify entity pairs. A PF is defined as the combination of the relative distances from the current word to e1 and e2. For instance, in the following example, the relative distances from son to e1 (Kojo Annan) and e2 (Kofi Annan) are 3 and -2, respectively.<br>Two position embedding matrixes (PF1 and PF2) are randomly initialized. We then transform the relative distances into real valued vectors by<br>looking up the position embedding matrixes. In the example shown in Figure 3, it is assumed that the size of the word embedding is dw = 4 and that the size of the position embedding is dp = 1. In combined word embeddings and position embeddings, the vector representation part transforms an instance into a matrix S ∈ Rs×d , where s is the sentence length and d = dw + dp ∗ 2. The matrix S is subsequently fed into the convolution part.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2019/05/09/文献翻译之A-Survey-of-Deep-Learning-Methods-for-Relation-Extraction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/05/09/文献翻译之A-Survey-of-Deep-Learning-Methods-for-Relation-Extraction/" itemprop="url">
                  文献翻译之A Survey of Deep Learning Methods for Relation Extraction
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-09T17:16:18+08:00">
                2019-05-09
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/09/文献翻译之A-Survey-of-Deep-Learning-Methods-for-Relation-Extraction/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/05/09/文献翻译之A-Survey-of-Deep-Learning-Methods-for-Relation-Extraction/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2019/04/14/文献摘要之Web-Scale-Distributional-Similarity-and-Entity-Set-Expansion/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/04/14/文献摘要之Web-Scale-Distributional-Similarity-and-Entity-Set-Expansion/" itemprop="url">
                  文献摘要之Web-Scale-Distributional-Similarity-and-Entity-Set-Expansion
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-14T17:12:03+08:00">
                2019-04-14
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/14/文献摘要之Web-Scale-Distributional-Similarity-and-Entity-Set-Expansion/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/04/14/文献摘要之Web-Scale-Distributional-Similarity-and-Entity-Set-Expansion/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Computing the pairwise semantic similarity between all words on the Web is a computationally challenging task. Parallelizationand optimizations are necessary. We propose a highly scalable implementation based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web. The pairwise similarity between 500 million terms is computed in 50 hours using 200 quad-core nodes. We apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size. We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia.<br>计算Web上所有单词之间的成对语义相似性是计算上具有挑战性的任务。 并行化和优化是必要的。 我们提出了一种基于分布相似性的高度可扩展的实现，在MapReduce框架中实现并部署了超过2000亿字的Web爬行。 使用200个四核节点在50小时内计算5亿个术语之间的成对相似性。 我们将学习的相似性矩阵应用于自动集扩展的任务，并提出了一个大的实证研究来量化对语料库大小，语料库质量，种子组成和种子大小的扩展性能的影响。 我们公开了一个集扩展分析的实验测试平台，其中包括从维基百科中提取的大量不同实体集。.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2019/04/14/文献摘要之A-Survey-on-Recent-Advances-in-NER-from-Deep-Learning-models/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/04/14/文献摘要之A-Survey-on-Recent-Advances-in-NER-from-Deep-Learning-models/" itemprop="url">
                  文献摘要之A-Survey-on-Recent-Advances-in-NER-from-Deep-Learning-models
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-14T09:30:22+08:00">
                2019-04-14
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/14/文献摘要之A-Survey-on-Recent-Advances-in-NER-from-Deep-Learning-models/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/04/14/文献摘要之A-Survey-on-Recent-Advances-in-NER-from-Deep-Learning-models/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h3><p>Named entity recognition is the task of identifying named entities like person, location, organization,drug, time, clinical procedure, biological protein, etc. in text. NER systems are often used as the first step in question answering, information retrieval, co-reference resolution, topic modeling, etc. Thus it is important to highlight recent advances in named entity recognition, especially recent neural NER architectures which have achieved state of the art performance with minimal feature engineering.<br>The first NER task was organized by Grishman and Sundheim (1996) in the Sixth Message Understanding Conference. Since then, there have been numerous NER tasks (Tjong Kim Sang and De Meulder, 2003; Tjong Kim Sang, 2002; Piskorski et al., 2017; Segura Bedmar et al., 2013; Bossy et al., 2013;Uzuner et al., 2011). Early NER systems were based on handcrafted rules, lexicons, orthographic features and ontologies. These systems were followed by NER systems based on feature-engineering and machine learning (Nadeau and Sekine, 2007). Starting with Collobert et al. (2011), neural network NER systems with minimal feature engineering have become popular. Such models are appealing because they typically do not require domain specific resources like lexicons or ontologies, and are thus poised to be more domain independent. Various neural architectures have been proposed, mostly based on some form of recurrent neural networks (RNN) over characters, sub-words and/or word embeddings.<br>We present a comprehensive survey of recent advances in named entity recognition. We describe knowledge-based and feature-engineered NER systems that combine in-domain knowledge, gazetteers, orthographic and other features with supervised or semi-supervised learning. We contrast these systems with neural network architectures for NER based on minimal feature engineering, and compare amongst the neural models with different representations of words and sub-word units. We show in Table 1 and Table 2 and discuss in Section 7 how neural NER systems have improved performance over past works including supervised, semi-supervised, and knowledge based NER systems. For example, NN models on news corpora improved the previous state-of-the-art by 1.59% in Spanish, 2.34% in German, 0.36% in English, and 0.14%, in Dutch, without any external resources or feature engineering. We provide resources, including links to shared tasks on NER, and links to the code for each category of NER system. To the best of our knowledge, this is the first survey focusing on neural architectures for NER,and comparing to previous feature-based systems.<br>We first discuss previous summary research on NER in section 2. Then we explain our selectioncriterion and methodology for selecting which systems to review in section 3. We highlight standard,past and recent NER datasets (from shared tasks and other research) in section 4 and evaluation metrics in section 5. We then describe NER systems in section 6 categorized into knowledge-based (section 6.1),bootstrapped (section 6.2), feature-engineered (section 6.3) and neural networks (section 6.4).<br>命名实体识别是在文本中识别诸如人，位置，组织，药物，时间，临床程序，生物蛋白等命名实体的任务。 NER系统通常被用作问题回答，信息检索，共同参考分辨率，主题建模等的第一步。因此，重要的是突出命名实体识别的最新进展，特别是最近已经实现状态的神经NER架构。艺术表现与最小的特征工程。<br>第一个NER任务由Grishman和Sundheim（1996）在第六次消息理解会议上组织。从那以后，有许多NER任务（Tjong Kim Sang和De Meulder，2003; Tjong Kim Sang，2002; Piskorski等，2017; Segura Bedmar等，2013; Bossy等，2013; Uzuner等。，2011）。早期的NER系统基于手工制作的规则，词典，正交特征和本体。这些系统之后是基于特征工程和机器学习的NER系统（Nadeau和Sekine，2007）。从Collobert等人开始。 （2011），具有最小特征工程的神经网络NER系统已经变得流行。这些模型很有吸引力，因为它们通常不需要域特定资源，如词典或本体，因此可以更加独立于域。已经提出了各种神经架构，主要基于对字符，子字和/或字嵌入的某种形式的递归神经网络（RNN）。<br>我们对命名实体识别的最新进展进行了全面的调查。我们描述了基于知识和特征设计的NER系统，它们将领域内知识，地名录，正字法和其他特征与监督或半监督学习相结合。我们将这些系统与基于最小特征工程的NER的神经网络架构进行对比，并在具有不同表示的单词和子单词单元的神经模型之间进行比较。我们在表1和表2中显示，并在第7节中讨论神经NER系统如何比过去的工作（包括监督，半监督和基于知识的NER系统）提高了性能。例如，新闻语料库中的NN模型在西班牙语中提高了先前的最新技术水平1.59％，德语为2.34％，英语为0.36％，荷兰语为0.14％，没有任何外部资源或特征工程。我们提供资源，包括NER上共享任务的链接，以及每个NER系统类别代码的链接。据我们所知，这是第一个专注于NER神经架构的调查，并与之前基于特征的系统进行比较。<br>我们首先在第2节讨论NER的先前总结研究。然后我们解释我们的选择标准和方法，以选择第3节中要审查的系统。我们在第4节中突出显示标准，过去和最近的NER数据集（来自共享任务和其他研究）和第5节中的评估指标。然后我们在第6节中描述NER系统，分为基于知识（第6.1节），自举（第6.2节），特征设计（第6.3节）和神经网络（第6.4节）。</p>
<h3 id="2-Previous-surveys"><a href="#2-Previous-surveys" class="headerlink" title="2 Previous surveys"></a>2 Previous surveys</h3><p>The first comprehensive NER survey was Nadeau and Sekine (2007), which covered a variety of supervised, semi-supervised and unsupervised NER systems, highlighted common features used by NER systems during that time, and explained NER evaluation metrics that are still in use today. Sharnagat(2014) presented a more recent NER survey that also included supervised, semi-supervised, and unsupervised NER systems, and included a few introductory neural network NER systems. There have also been surveys focused on NER systems for specific domains and languages, including biomedical NER,(Leaman and Gonzalez, 2008), Chinese clinical NER (Lei et al., 2013), Arabic NER (Shaalan, 2014;Etaiwi et al., 2017), and NER for Indian languages (Patil et al., 2016).<br>The existing surveys primarily cover feature-engineered machine learning models (including supervised, semi-supervised, and unsupervised systems), and mostly focus on a single language or a single domain. There is not yet, to our knowledge, a comprehensive survey of modern neural network NER systems, nor is there a survey that compares feature engineered and neural network systems in both multi-lingual (CoNLL 2002 and CoNLL 2003) and multi-domain (e.g., news and medical) settings.<br>第一次全面的NER调查是Nadeau和Sekine（2007），其中涵盖了各种监督，半监督和无监督的NER系统，突出了NER系统在此期间使用的常见特征，并解释了当前仍在使用的NER评估指标 。 Sharnagat（2014）提出了一项更新的NER调查，其中还包括监督，半监督和无监督的NER系统，并包括一些入门神经网络NER系统。还有针对特定领域和语言的NER系统的调查，包括生物医学NER，（Leaman和Gonzalez，2008），中国临床NER（Lei等，2013），阿拉伯语NER（Shaalan，2014; Etaiwi等。 ，2017）和印度语言的NER（Patil等，2016）。<br>现有的调查主要涵盖特征设计的机器学习模型（包括监督，半监督和无监督系统），主要集中在单一语言或单一领域。据我们所知，尚未对现代神经网络NER系统进行全面调查，也没有一项调查比较多语言（CoNLL 2002和CoNLL 2003）和多域（例如CoNLL 2002和CoNLL 2003）中的特征工程和神经网络系统。 ，新闻和医疗）设置。</p>
<h3 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3 Methodology"></a>3 Methodology</h3><p>To identify articles for this survey, we searched Google, Google Scholar, and Semantic Scholar. Our query terms included named entity recognition, neural architectures for named entity recognition, neural network based named entity recognition models, deep learning models for named entity recognition,etc. We sorted the papers returned from each query by citation count and read at least the top three,considering a paper for our survey if it either introduced a neural architecture for named entity recognition, or represented a top-performing model on an NER dataset. We included an article presenting a neural architecture only if it was the first article to introduce the architecture; otherwise, we traced citations back until we found the original source of the architecture. We followed the same approach for feature-engineering NER systems. We also included articles that implemented these systems for different languages or domain. In total, 154 articles were reviewed and 83 articles were selected for the survey<br>为了识别此调查的文章，我们搜索了Google，Google学术搜索和语义学者。我们的查询术语包括命名实体识别，命名实体识别的神经架构，基于神经网络的命名实体识别模型，命名实体识别的深度学习模型等。我们通过引用计数对每个查询返回的论文进行排序，并至少读取前三个，考虑到我们调查的论文，如果它引入了用于命名实体识别的神经架构，或者代表了NER数据集上的最佳表现模型。我们收录了一篇介绍神经结构的文章，只要它是第一篇介绍该结构的文章;否则，我们追溯引用，直到找到建筑的原始来源。我们对特征工程NER系统采用了相同的方法。我们还包括为不同语言或域实现这些系统的文章。共审查了154篇文章，选择了83篇文章进行调查</p>
<h3 id="4-NER-datasets"><a href="#4-NER-datasets" class="headerlink" title="4 NER datasets"></a>4 NER datasets</h3><p>Since the first shared task on NER (Grishman and Sundheim, 1996)1, many shared tasks and datasets for NER have been created. CoNLL 2002 (Tjong Kim Sang, 2002)2 and CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003)3 were created from newswire articles in four different languages (Spanish, Dutch, English, and German) and focused on 4 entities - PER (person), LOC (location), ORG (organization) and MISC (miscellaneous including all other types of entities).<br>NER shared tasks have also been organized for a variety of other languages, including Indian languages (Rajeev Sangal and Singh, 2008), Arabic (Shaalan, 2014), German (Benikova et al., 2014), and slavic languages (Piskorski et al., 2017). The named entity types vary widely by source of dataset and language. For example, Rajeev Sangal and Singh (2008)’s southeast Asian language data has named entity types person, designation, temporal expressions, abbreviations, object number, brand, etc. Benikova et al. (2014)’s data, which is based on German wikipedia and online news, has named entity types similar to that of CoNLL 2002 and 2003: PERson, ORGanization, LOCation and OTHer. The shared task4 organized by Piskorski et al. (2017) covering 7 slavic languages (Croatian, Czech, Polish, Russian, Slovak,Slovene, Ukrainian) also has person, location, organization and miscellaneous as named entity types.<br>In the biomedical domain, Kim et al. (2004) organized a BioNER task on MedLine abstracts, focusing on protien, DNA, RNA and cell attribute entity types. Uzuner et al. (2007) presented a clinical note de-identification task that required NER to locate personal patient data phrases to be anonymized. The 2010 I2B2 NER task5 (Uzuner et al., 2011), which considered clinical data, focused on clinical problem, test and treatment entity types. Segura Bedmar et al. (2013) organized a Drug NER shared task6 as part of SemEval 2013 Task 9, which focused on drug, brand, group and drug n (unapproved or new drugs) entity types. (Krallinger et al., 2015) introduced the similar CHEMDNER task 7 focusing more on chemical and drug entities like trivial, systematic, abbreviation, formula, family, identifier, etc. Biology and microbiology NER datasets8(Hirschman et al., 2005; Bossy et al., 2013; Deleger et al., 2016) ˙have been collected from PubMed and biology websites, and focus mostly on bacteria, habitat and geolocation entities. In biomedical NER systems, segmentation of clinical and drug entities is considered to be a difficult task because of complex orthographic structures of named entities (Liu et al., 2015).<br>NER tasks have also been organized on social media data, e.g., Twitter, where the performance of classic NER systems degrades due to issues like variability in orthography and presence of grammatically incomplete sentences (Baldwin et al., 2015). Entity types on Twitter are also more variable (person, company, facility, band, sportsteam, movie, TV show, etc.) as they are based on user behavior on Twitter.Though most named entity annotations are flat, some datasets include more complex structures. Ohta et al. (2002) constructed a dataset of nested named entities, where one named entity can contain another.Strassel et al. (2003) highlighted both entity and entity head phrases. And discontinuous entities are common in chemical and clinical NER datasets (Krallinger et al., 2015). Eltyeb and Salim (2014) presented an survey of various NER systems developed for such NER datasets with a focus on chemical NER.<br>自从第一个关于NER的共同任务（Grishman和Sundheim，1996）1以来，已经创建了许多NER的共享任务和数据集。 CoNLL 2002（Tjong Kim Sang，2002）2和CoNLL 2003（Tjong Kim Sang和De Meulder，2003）3是由四种不同语言（西班牙语，荷兰语，英语和德语）的新闻专线文章创建的，专注于4个实体 -  PER （人），LOC（地点），ORG（组织）和MISC（杂项包括所有其他类型的实体）。<br>还为各种其他语言组织了NER共享任务，包括印度语（Rajeev Sangal和Singh，2008），阿拉伯语（Shaalan，2014），德语（Benikova等，2014）和斯拉夫语（Piskorski等。 。，2017）。命名的实体类型因数据集和语言的来源而异。例如，Rajeev Sangal和Singh（2008）的东南亚语言数据已将实体类型命名为人，名称，时间表达，缩写，对象编号，品牌等.Benikova等。 （2014）的数据基于德国维基百科和在线新闻，已经命名了类似于CoNLL 2002和2003的实体类型：PERson，ORGanization，LOCation和OTHer。由Piskorski等人组织的共享任务4。 （2017年）涵盖7种斯拉夫语（克罗地亚语，捷克语，波兰语，俄语，斯洛伐克语，斯洛文尼亚语，乌克兰语）也有人，地点，组织和杂项作为命名实体类型。<br>在生物医学领域，Kim等人。 （2004）在MedLine摘要上组织了BioNER任务，重点关注protien，DNA，RNA和细胞属性实体类型。 Uzuner等。 （2007）提出了临床记录去识别任务，该任务要求NER定位要匿名的个人患者数据短语。考虑临床数据的2010 I2B2 NER task5（Uzuner et al。，2011）专注于临床问题，测试和治疗实体类型。 Segura Bedmar等。 （2013）组织了一个药物NER共享任务6，作为SemEval 2013任务9的一部分，其重点是药物，品牌，群体和药物n（未批准或新药）实体类型。 （Krallinger等，2015）介绍了类似的CHEMDNER任务7，重点关注化学和药物实体，如琐碎，系统，缩写，公式，家族，标识符等。生物学和微生物学NER数据集8（Hirschman等，2005; Bossy） et al。，2013; Deleger et al。，2016）˙已经从PubMed和生物学网站收集，主要关注细菌，栖息地和地理定位实体。在生物医学NER系统中，由于命名实体的复杂正交结构，临床和药物实体的分割被认为是一项艰巨的任务（Liu et al。，2015）。<br>还在社交媒体数据（例如，Twitter）上组织了NER任务，其中经典NER系统的性能由于诸如拼写法的可变性和语法上不完整的句子的存在之类的问题而降级（Baldwin等人，2015）。 Twitter上的实体类型也更加多变（人，公司，设施，乐队，体育团队，电影，电视节目等），因为它们基于Twitter上的用户行为。虽然大多数命名实体注释都是扁平的，但有些数据集包含更复杂的结构。 Ohta等人。 （2002）构建了一个嵌套命名实体的数据集，其中一个命名实体可以包含另一个.Strassel等。 （2003）强调了实体和实体头短语。不连续实体在化学和临床NER数据集中很常见（Krallinger等，2015）。 Eltyeb和Salim（2014）提出了针对此类NER数据集开发的各种NER系统的调查，重点是化学NER。</p>
<h3 id="5-NER-evaluation-metrics"><a href="#5-NER-evaluation-metrics" class="headerlink" title="5 NER evaluation metrics"></a>5 NER evaluation metrics</h3><p>Grishman and Sundheim (1996) scored NER performance based on type, whether the predicted label was correct regardless of entity boundaries, and text, whether the predicted entity boundaries were correct regardless of the label. For each score category, precision was defined as the number of entities a system predicted correctly divided by the number that the system predicted, recall was defined as the number of entities a system predicted correctly divided by the number that were identified by the human annotators, and (micro) F-score was defined as the harmonic mean of precision and recall from both type and text.<br>The exact match metrics introduced by CoNLL (Tjong Kim Sang and De Meulder, 2003; Tjong Kim Sang, 2002) considers a prediction to be correct only when the predicted label for the complete entity is matched to exactly the same words as the gold label of that entity. CoNLL also used (micro) F-score, taking the harmonic mean of the exact match precision and recall.<br>The relaxed F1 and strict F1 metrics have been used in many NER shared tasks (Segura Bedmar et al.,2013; Krallinger et al., 2015; Bossy et al., 2013; Deleger et al., 2016). Relaxed F1 considers a prediction to be correct as long as part of the named entity is identified correctly. Strict F1 requires the character offsets of a prediction and the human annotation to match exactly. In these data, unlike CoNLL, word offsets are not given, so relaxed F1 is intended to allow comparison despite different systems having different word boundaries due to different segmentation techniques (Liu et al., 2015).<br>Grishman和Sundheim（1996）根据类型评估了NER表现，无论实体边界如何，预测标签是否正确，以及文本，无论标签如何，预测实体边界是否正确。对于每个分数类别，精度定义为系统正确预测的实体数量除以系统预测的数量，召回定义为系统正确预测的实体数量除以人类注释器识别的数量，和（微）F分数被定义为精度的调和平均值和从类型和文本中回忆。<br>CoNLL引入的精确匹配度量（Tjong Kim Sang和De Meulder，2003; Tjong Kim Sang，2002）认为只有当完整实体的预测标签与完全相同的单词匹配时，预测才是正确的。那个实体。 CoNLL还使用（微）F分数，取精确匹配精度和召回的调和平均值。<br>轻松的F1和严格的F1指标已经用于许多NER共享任务中（Segura Bedmar等人，2013; Krallinger等人，2015; Bossy等人，2013; Deleger等人，2016）。只要正确识别出部分命名实体，轻松F1就会认为预测是正确的。严格的F1要求预测的字符偏移和人类注释完全匹配。在这些数据中，与CoNLL不同，没有给出字偏移，因此松弛的F1旨在允许进行比较，尽管由于不同的分割技术，不同的系统具有不同的字边界（Liu等，2015）。</p>
<h3 id="6-NER-systems"><a href="#6-NER-systems" class="headerlink" title="6 NER systems"></a>6 NER systems</h3><h4 id="6-1-Knowledge-based-systems"><a href="#6-1-Knowledge-based-systems" class="headerlink" title="6.1 Knowledge-based systems"></a>6.1 Knowledge-based systems</h4><p>Knowledge-based NER systems do not require annotated training data as they rely on lexicon resources and domain specific knowledge. These work well when the lexicon is exhaustive, but fail, for example,on every example of the drug n class in the DrugNER dataset (Segura Bedmar et al., 2013), since drug n is defined as unapproved or new drugs, which are by definition not in the DrugBank dictionaries (Knoxet al., 2010). Precision is generally high for knowledge-based NER systems because of the lexicons, but recall is often low due to domain and language-specific rules and incomplete dictionaries. Another drawback of knowledge based NER systems is the need of domain experts for constructing and maintaining the knowledge resources.<br>基于知识的NER系统不需要带注释的训练数据，因为它们依赖于词典资源和领域特定知识。 当词典是详尽的时，这些方法很有效，但是例如，在DrugNER数据集（Segura Bedmar等，2013）中药物类的每个例子都失败了，因为药物n被定义为未经批准或新药， 根据定义，不在DrugBank词典中（Knoxet al。，2010）。 由于词汇的原因，基于知识的NER系统的精度通常较高，但由于域和语言特定的规则以及不完整的词典，召回通常较低。 基于知识的NER系统的另一个缺点是需要领域专家来构建和维护知识资源。</p>
<h4 id="6-2-Unsupervised-and-bootstrapped-systems"><a href="#6-2-Unsupervised-and-bootstrapped-systems" class="headerlink" title="6.2 Unsupervised and bootstrapped systems"></a>6.2 Unsupervised and bootstrapped systems</h4><p>Some of the earliest systems required very minimal training data. Collins and Singer (1999) used only labeled seeds, and 7 features including orthography (e.g., capitalization), context of the entity, words contained within named entities, etc. for classifying and extracting named entities. Etzioni et al. (2005) proposed an unsupervised system to improve the recall of NER systems applying 8 generic pattern extractors to open web text, e.g., NP is a <class1>, NP1 such as NPList2. Nadeau et al. (2006) presented an unsupervised system for gazetteer building and named entity ambiguity resolution based on Etzioniet al. (2005) and Collins and Singer (1999) that combined an extracted gazetteer with commonly available gazetteers to achieve F-scores of 88%, 61%, and 59% on MUC-7 (Chinchor and Robinson, 1997) location, person, and organization entities, respectively.<br>Zhang and Elhadad (2013) used shallow syntactic knowledge and inverse document frequency (IDF) for an unsupervised NER system on biology (Kim et al., 2004) and medical (Uzuner et al., 2011) data, achieving 53.8% and 69.5% accuracy, respectively. Their model uses seeds to discover text having potential named entities, detects noun phrases and filters any with low IDF values, and feeds the filtered list to a classifier (Alfonseca and Manandhar, 2002) to predict named entity tags.<br>一些最早的系统需要非常少的训练数据。 Collins和Singer（1999）仅使用标记的种子，并且7个特征包括正字法（例如，大写），实体的上下文，命名实体中包含的单词等，用于分类和提取命名实体。 Etzioni等。 （2005）提出了一种无监督系统来改进NER系统的召回，该系统应用8个通用模式提取器来打开网络文本，例如，NP是<class1>，NP1，例如NPList2。 Nadeau等。 （2006）提出了一个无监督的地名录建筑系统和基于Etzioniet al的命名实体模糊解决方案。 （2005）和柯林斯和辛格（1999）将提取的地名词典与常用的地名录相结合，使得MUC-7（Chinchor和Robinson，1997）的位置，人和的F分数分别达到88％，61％和59％。组织实体。<br>Zhang和Elhadad（2013）对生物学（Kim et al。，2004）和医学（Uzuner et al。，2011）数据的无监督NER系统使用浅层句法知识和逆文档频率（IDF），达到53.8％和69.5％准确性，分别。他们的模型使用种子来发现具有潜在命名实体的文本，检测名词短语并过滤任何具有低IDF值的文本，并将过滤后的列表提供给分类器（Alfonseca和Manandhar，2002）以预测命名实体标签。</class1></class1></p>
<h4 id="6-3-Feature-engineered-supervised-systems"><a href="#6-3-Feature-engineered-supervised-systems" class="headerlink" title="6.3 Feature-engineered supervised systems"></a>6.3 Feature-engineered supervised systems</h4><p>Supervised machine learning models learn to make predictions by training on example inputs and their expected outputs, and can be used to replace human curated rules. Hidden Markov Models (HMM),Support Vector Machines (SVM), Conditional Random Fields (CRF), and decision trees were common machine learning systems for NER.<br>Zhou and Su (2002) used HMM (Rabiner and Juang, 1986; Bikel et al., 1997) an NER system on MUC-6 and MUC-7 data, achieving 96.6% and 94.1% F score, respectively. They included 11 orthographic features (1 numeral, 2 numeral, 4 numeral, all caps, numerals and alphabets, contains underscore or not, etc.) a list of trigger words for the named entities (e.g., 36 trigger words and affixes, like river, for the location entity class), and a list of words (10000 for the person entity class) from various gazetteers.<br>Malouf (2002) compared the HMM with Maximum Entropy (ME) by adding multiple features. Their best model included capitalization, whether a word was the first in a sentence, whether a word had appeared before with a known last name, and 13281 first names collected from various dictionaries. The model achieved 73.66%, 68.08% Fscore on Spanish and Dutch CoNLL 2002 dataset respectively.<br>The winner of CoNLL 2002 (Carreras et al., 2002) used binary AdaBoost classifiers, a boosting algorithm that combines small fixed-depth decision trees (Schapire, 2013). They used features like capitalization, trigger words, previous tag prediction, bag of words, gazetteers, etc. to represent simple binary relations and these relations were used in conjunction with previously predicted labels. They achieved 81.39% and 77.05% F scores on the Spanish and Dutch CoNLL 2002 datasets, respectively.<br>Li et al. (2005) implemented a SVM model on the CoNLL 2003 dataset and CMU seminar documents.They experimented with multiple window sizes, features (orthographic, prefixes suffixes, labels, etc.)from neighboring words, weighting neighboring word features according to their position, and class weights to balance positive and negative class. They used two SVM classifiers, one for detecting named entity starts and one for detecting ends. They achieved 88.3% F score on the English CoNLL 2003 data.<br>On the MUC6 data, Takeuchi and Collier (2002) used part-of-speech (POS) tags, orthographic features, a window of 3 words to the left and to the right of the central word, and tags of the last 3 words as features to the SVM. The final tag was decided by the voting of multiple one-vs-one SVM outputs.<br>Ando and Zhang (2005a) implemented structural learning (Ando and Zhang, 2005b) to divide the main task into many auxiliary tasks, for example, predicting labels by looking just at the context and masking the current word. The best classifier for each auxiliary task was selected based on its confidence. This model had achieved 89.31% and 75.27% F score on English and German, respectively.<br>Agerri and Rigau (2016) developed a semi-supervised system9 by presenting NER classifiers with features including orthography, character n-grams, lexicons, prefixes, suffixes, bigrams, trigrams, and unsupervised cluster features from the Brown corpus, Clark corpus and k-means clustering of open text using word embeddings (Mikolov et al., 2013). They achieved near state of the art performance on CoNLL datasets: 84.16%, 85.04%, 91.36%, 76.42% on Spanish, Dutch, English, and German, respectively.<br>In DrugNER (Segura Bedmar et al., 2013), Liu et al. (2015) achieved state-of-the-art results by using a CRF with features like lexicon resources from Food and Drug Administration (FDA), DrugBank, Jochem (Hettne et al., 2009) and word embeddings (trained on a MedLine corpus). For the same task, Rocktaschel et al. (2013) used a CRF with features constructed from dictionaries (e.g., Jochem (Hettne ¨et al., 2009)), ontologies (ChEBI ontologies), prefixes-suffixes from chemical entities, etc.<br>监督机器学习模型学习通过对示例输入及其预期输出的训练来进行预测，并且可以用于替换人类策划的规则。隐马尔可夫模型（HMM），支持向量机（SVM），条件随机场（CRF）和决策树是NER的常用机器学习系统。<br>Zhou和Su（2002）使用HMM（Rabiner和Juang，1986; Bikel等，1997）对MUC-6和MUC-7数据的NER系统，分别达到96.6％和94.1％F得分。它们包括11个正交特征（1个数字，2个数字，4个数字，所有大写字母，数字和字母，包含下划线或不包括下划线等）命名实体的触发词列表（例如，36个触发词和词缀，如河，对于位置实体类），以及来自各种地名录的单词列表（人类实体类为10000）。<br>Malouf（2002）通过添加多个特征将HMM与最大熵（ME）进行了比较。他们最好的模型包括大写，一个单词是句子中的第一个单词，一个单词是否曾出现过已知的姓氏，以及从各种词典中收集的13281个名字。该模型分别在西班牙和荷兰CoNLL 2002数据集上获得了73.66％，68.08％的Fscore。<br>2002年CoNLL的获胜者（Carreras等人，2002年）使用了二元AdaBoost分类器，这是一种结合了小型固定深度决策树的增强算法（Schapire，2013）。他们使用诸如大写，触发词，先前的标签预测，词袋，地名词典等特征来表示简单的二元关系，并且这些关系与先前预测的标签结合使用。他们分别在西班牙和荷兰的CoNLL 2002数据集上获得了81.39％和77.05％的F分数。<br>李等人。 （2005）在CoNLL 2003数据集和CMU研讨会文档上实现了SVM模型。他们试验了相邻单词的多个窗口大小，特征（正字法，前缀后缀，标签等），根据它们的位置加权相邻的单词特征，以及阶级权重以平衡正面和负面阶级。他们使用了两个SVM分类器，一个用于检测命名实体启动，另一个用于检测结束。他们在英国CoNLL 2003数据上获得了88.3％的F分数。<br>在MUC6数据上，Takeuchi和Collier（2002）使用了词性（POS）标签，正交特征，中心词左侧和右侧的3个单词窗口，以及后3个单词的标签SVM的功能。最终标签由多个一对一SVM输出的投票决定。<br>Ando和Zhang（2005a）实施了结构学习（Ando和Zhang，2005b），将主要任务划分为许多辅助任务，例如，通过查看上下文和屏蔽当前单词来预测标签。基于其置信度选择每个辅助任务的最佳分类器。该模型分别在英语和德语上获得89.31％和75.27％的F分。<br>Agerri和Rigau（2016）通过呈现NER分类器开发了一个半监督系统9，其具有包括正字法，字符n-gram，词典，前缀，后缀，双字母，三元组和来自布朗语料库，克拉克语料库和k-的无监督聚类特征的特征。意味着使用文字嵌入聚类开放文本（Mikolov等，2013）。他们在CoNLL数据集上取得了近乎最先进的表现：分别为西班牙语，荷兰语，英语和德语的84.16％，85.04％，91.36％，76.42％。<br>在DrugNER（Segura Bedmar等，2013）中，Liu等。 （2015）通过使用具有食品和药物管理局（FDA），DrugBank，Jochem（Hettne等，2009）的词汇资源和词汇嵌入（在MedLine语料库上训练）等特征的CRF获得了最新成果）。对于同样的任务，Rocktaschel等人。 （2013）使用具有由字典构建的特征的CRF（例如，Jochem（Hettne¨etal。，2009）），本体（ChEBI本体），来自化学实体的前缀 - 后缀等。</p>
<h4 id="6-4-Feature-inferring-neural-network-systems"><a href="#6-4-Feature-inferring-neural-network-systems" class="headerlink" title="6.4 Feature-inferring neural network systems"></a>6.4 Feature-inferring neural network systems</h4><p>Collobert and Weston (2008) proposed one of the first neural network architectures for NER, with feature vectors constructed from orthographic features (e.g., capitalization of the first character), dictionaries and lexicons. Later work replaced these manually constructed feature vectors with word embeddings (Collobert et al., 2011), which are representations of words in n-dimensional space, typically learned over large collections of unlabeled data through an unsupervised process such as the skip-gram model (Mikolov et al., 2013). Studies have shown the importance of such pre-trained word embeddings for neural network based NER systems (Habibi et al., 2017), and similarly for pre-trained character embeddings in character-based languages like Chinese (Li et al., 2015; Yin et al., 2016).<br>Modern neural architectures for NER can be broadly classified into categories depending upon their representation of the words in a sentence. For example, representations may be based on words, characters, other sub-word units or any combination of these.<br>Collobert和Weston（2008）提出了NER的第一个神经网络架构之一，其特征向量由正交特征（例如，第一个字符的大写），字典和词典构成。后来的工作用字嵌入替代了这些手工构造的特征向量（Collobert等，2011），它们是n维空间中单词的表示，通常通过无监督过程（例如skip-gram模型）在大量未标记数据集上学习。 （Mikolov等，2013）。研究表明，这种预训练的单词嵌入对于基于神经网络的NER系统的重要性（Habibi等，2017），以及类似于在中文等基于字符的语言中预训练的字符嵌入（Li et al。，2015; Yin et al。，2016）。<br>NER的现代神经结构可以根据它们在句子中的单词的表示大致分类。例如，表示可以基于单词，字符，其他子单词单元或这些的任何组合。</p>
<h5 id="6-4-1-Word-level-architectures"><a href="#6-4-1-Word-level-architectures" class="headerlink" title="6.4.1 Word level architectures"></a>6.4.1 Word level architectures</h5><p>In this architecture, the words of a sentence are given as input to Recurrent Neural Networks (RNN) and each word is represented by its word embedding, as shown in Figure 1.<br>The first word-level NN model was proposed by Collobert et al. (2011)10. The architecture was similar to the one shown in Figure 1, but a convolution layer was used instead of the Bi-LSTM layer and the output of the convolution layer was given to a CRF layer for the final prediction. The authors achieved 89.59% F1 score on English CoNLL 2003 dataset by including gazetteers and SENNA embeddings.<br>Huang et al. (2015) presented a word LSTM model (Figure 1) and showed that adding a CRF layer to the top of the word LSTM improved performance, achieving 84.26% F1 score on English CoNLL 2003 dataset. Similar systems were applied to other domains: DrugNER by Chalapathy et al. (2016) achieving 85.19% F1 score (under an unofficial evaluation) on MedLine test data (Segura Bedmar et al., 2013), and medical NER by Xu et al. (2017) achieving 80.22% F1 on disease NER corpus using this architecture.In similar tasks, Plank et al. (2016) implemented the same model for multilingual POS tagging.<br>With slight variations, Yan et al. (2016) implemented word level feed forward NN, bi-directional LSTM (bi-LSTM) and window bi-LSTM for NER of English, German and Arabic. They also highlighted the performance improvement after adding various features like CRF, case, POS, word embeddings and achieved 88.91% F1 score on English and 76.12% on German.<br>在这种体系结构中，句子的单词作为递归神经网络（RNN）的输入给出，每个单词由其单词嵌入表示，如图1所示。<br>第一个词级NN模型由Collobert等人提出。 （2011）10。该体系结构类似于图1中所示的体系结构，但是使用卷积层代替Bi-LSTM层，并且将卷积层的输出给予CRF层用于最终预测。通过包括地名索引和SENNA嵌入，作者在英语CoNLL 2003数据集上获得了89.59％的F1分数。<br>黄等人。 （2015）提出了一个单词LSTM模型（图1），并显示在单词LSTM的顶部添加CRF层提高了性能，在英语CoNLL 2003数据集上获得了84.26％的F1分数。类似的系统应用于其他领域：Champathy等人的Drugner。 （2016）在MedLine测试数据（Segura Bedmar等人，2013）和Xu等人的医学NER上获得85.19％F1评分（在非官方评估下）。 （2017）使用这种架构在疾病NER语料库中实现了80.22％的F1。在类似的任务中，Plank等人。 （2016）实现了多语言POS标签的相同模型。<br>Yan等人略有变化。 （2016）为英语，德语和阿拉伯语的NER实施了单词级前馈NN，双向LSTM（bi-LSTM）和窗口双LSTM。他们还强调了在添加CRF，案例，POS，文字嵌入等各种功能后的性能提升，英语成绩为88.91％，德语成绩为76.12％。</p>
<h5 id="6-4-2-Character-level-architectures"><a href="#6-4-2-Character-level-architectures" class="headerlink" title="6.4.2 Character level architectures"></a>6.4.2 Character level architectures</h5><p>In this model, a sentence is taken to be a sequence of characters. This sequence is passed through an RNN, predicting labels for each character (Figure 2). Character labels transformed into word labels via post processing. The potential of character NER neural models was first highlighted by Kim et al. (2016) using highway networks over convolution neural networks (CNN) on character sequences of words and then using another layer of LSTM + softmax for the final predictions.<br>This model was implemented by Pham and Le-Hong (2017) for Vietnamese NER and achieved 80.23% F-score on Nguyen et al. (2016)’s Vietnamese test data. Character models were also used in various other languages like Chinese (Dong et al., 2016) where it has achieved near state of the art performance.<br>Kuru et al. (2016) proposed CharNER 11 which implemented the character RNN model for NER on 7 different languages. In this character model, tag prediction over characters were converted to word tags using Viterbi decoder(Forney, 1973) achieving 82.18% on Spanish, 79.36% on Dutch, 84.52% on English and 70.12% on German CoNLL datasets. They also achieved 78.72 on Arabic, 72.19 on Czech and 91.30 on Turkish. Ling et al. (2015) proposed word representation using RNN (Bi-LSTM) over characters of the word and achieved state of the art results on POS task using this representation in multiple languages including 97.78% accuracy on English PTB(Marcus et al., 1993).<br>Gillick et al. (2015) implemented sequence to sequence model (Byte to Span- BTS) using encoder decoder architecture over sequence of characters of words in a window of 60 characters. Each character was encoded in bytes and BTS achieved high performance on CoNLL 2002 and 2003 dataset without any feature engineering. BTS achieved 82.95%, 82.84%,86.50%,76.22% Fscore on Spanish, Dutch, English and German CoNLL datasets respectively.<br>在该模型中，句子被认为是一系列字符。该序列通过RNN传递，预测每个字符的标签（图2）。字符标签通过后期处理转换为单词标签。 Kim等人首先强调了角色NER神经模型的潜力。 （2016）在卷积神经网络（CNN）上使用高速公路网络对字的字符序列，然后使用另一层LSTM + softmax进行最终预测。<br>该模型由Pham和Le-Hong（2017）为越南NER实施，并在Nguyen等人上获得了80.23％的F-分数。 （2016）的越南测试数据。人物模型也被用于其他语言，如中文（Dong et al。，2016），它已经达到了近乎最先进的性能。<br>库鲁等人。 （2016）提出了Charner 11，它为7种不同语言的NER实现了RNN模型。在这个角色模型中，使用维特比解码器（Forney，1973）将字符上的标签预测转换为字标签，西班牙语为82.18％，荷兰语为79.36％，英语为84.52％，德国CoNLL数据集为70.12％。他们在阿拉伯语中获得78.72，在捷克语中获得72.19，在土耳其语中获得91.30。凌等人。 （2015）提出了使用RNN（Bi-LSTM）对单词的字符进行单词表示，并使用多种语言的表示在POS任务上实现了最先进的结果，包括英语PTB的97.78％准确度（Marcus等，1993）。<br>吉利克等人。 （2015）使用编码器解码器架构在60个字符的窗口中的字符序列上实现序列到序列模型（字节到Span-BTS）。每个字符都以字节编码，BTS在CoNLL 2002和2003数据集上实现了高性能，无需任何特征工程。西班牙语，荷兰语，英语和德语CoNLL数据集中，BTS分别达到82.95％，82.84％，86.50％，76.22％Fscore。</p>
<h5 id="6-4-3-Character-Word-level-architectures"><a href="#6-4-3-Character-Word-level-architectures" class="headerlink" title="6.4.3 Character+Word level architectures"></a>6.4.3 Character+Word level architectures</h5><p>Systems combining word context and the characters of a word have proved to be strong NER systems that need little domain specific knowledge or resources. There are two base models in this category. The first type of model represents words as a combination of a word embedding and a convolution over the characters of the word, follows this with a Bi-LSTM layer over the word representations of a sentence, and finally uses a softmax or CRF layer over the Bi-LSTM to generate labels. The architecture diagram for this model is same as Figure 3 but with the character Bi-LSTM replaced with a CNN12.<br>Ma and Hovy (2016) implemented this model to achieve 91.21% F1 score on the CoNLL 2003 English dataset and 97.55% POS-tagging accuracy on the WSJ portion of PTB (Marcus et al., 1993). They also showed lower performance by this model for out of vocabulary words.<br>Chiu and Nichols (2015) achieved 91.62% F1 score on the CoNLL 2003 English dataset and 86.28% F score on Onto notes 5.0 dataset (Pradhan et al., 2013) by adding lexicons and capitalization features to this model. Lexicon feature were encoded in the form or B(begin), I(inside) or E(end) PER, LOC, ORG and MISC depending upon the match from the dictionary.<br>This model has also been utilized for NER in languages like Japanese where Misawa et al. (2017) showed that this architecture outperformed other neural architectures on the organization entity class.<br>Limsopatham and Collier (2016) implemented a character+word level NER model for Twitter NER (Baldwin et al., 2015) by concatenating a CNN over characters, a CNN over orthographic features of characters, a word embedding, and a word orthographic feature embedding. This concatenated representation is passed through another Bi-LSTM layer and the output is given to CRF for predicting. This model achieved 65.89% F score on segmentation alone and 52.41% F score on segmentation and categorization.<br>Santos and Guimaraes (2015) implemented a model with a CNN over the characters of word, concatenated with word embeddings of the central word and its neighbors, fed to a feed forward network, and followed by the Viterbi algorithm to predict labels for each word. The model achieved 82.21% F score on Spanish CoNLL 2002 data and 71.23% F score on Portuguese NER data (Santos and Cardoso, 2007).<br>The second type of model concatenates word embeddings with LSTMs (sometimes bi-directional) over the characters of a word, passing this representation through another sentence-level Bi-LSTM, and predicting the final tags using a final softmax or CRF layer (Figure 3). Lample et al. (2016)13 introduced this architecture and achieved 85.75%, 81.74%, 90.94%, 78.76% Fscores on Spanish, Dutch, English and German NER dataset respectively from CoNLL 2002 and 2003.<br>Dernoncourt et al. (2017) implemented this model in the NeuroNER toolkit14 with the main goal of providing easy usability and allowing easy plotting of real time performance and learning statistics of the model. The BRAT annotation tool15 is also integrated with NeuroNER to ease the development of NN NER models in new domains. NeuroNER achieved 90.50% F score on the English CoNLL 2003 data.<br>Habibi et al. (2017) implemented the model for various biomedical NER tasks and achieved higher performance than the majority of other participants. For example, they achieved 83.71 F-score on the CHEMDNER data (Krallinger et al., 2015).<br>Bharadwaj et al. (2016)16 utilized phonemes (from Epitran) for NER in addition to characters and words. They also utilize attention knowledge over sequence of characters in word which is concatenated with the word embedding and character representation of word. This model achieved state of the art performance (85.81% F score) on Spanish CoNLL 2002 dataset.<br>A slightly improved system focusing on multi-task and multi-lingual joint learning was proposed by Yang et al. (2016) where word representation given by GRU (Gated Recurrent Unit) cell over characters plus word embedding was passed through another RNN layer and the output was given to CRF models trained for different tasks like POS, chunking and NER. Yang et al. (2017) further proposed transfer learning for multi-task and multi-learning, and showed small improvements on CoNLL 2002 and 2003 NER data, achieving 85.77%, 85.19%, 91.26% F scores on Spanish, Dutch and English, respectively.<br>结合单词上下文和单词字符的系统已被证明是强大的NER系统，需要很少的领域特定知识或资源。此类别中有两种基本模型。第一种类型的模型将单词表示为单词嵌入和单词字符上的卷积的组合，在句子的单词表示上使用Bi-LSTM层，最后使用softmax或CRF层。 Bi-LSTM生成标签。该模型的架构图与图3相同，但字符Bi-LSTM替换为CNN12。<br>Ma和Hovy（2016）实施了这个模型，在CoNLL 2003英语数据集上获得91.21％的F1分数，在PTB的WSJ部分获得97.55％的POS标记准确率（Marcus等，1993）。对于词汇单词，他们也表现出较低的表现。<br>Chiu和Nichols（2015）通过在该模型中添加词典和大写特征，在CoNLL 2003英语数据集上获得91.62％F1得分，在Onto notes 5.0数据集上获得86.28％F得分（Pradhan等，2013）。词典特征以形式或B（开始），I（内部）或E（结束）PER，LOC，ORG和MISC编码，取决于字典中的匹配。<br>该模型也被用于日本语言中的NER，Misawa等人。 （2017）表明，这种架构在组织实体类上优于其他神经架构。<br>Limsopatham和Collier（2016）通过在字符上连接CNN，在字符的正字特征，字嵌入和单词正字特征嵌入上连接CNN，为Twitter NER（Baldwin等，2015）实现了字符+单词级NER模型。 。该连接表示通过另一个Bi-LSTM层传递，输出被提供给CRF进行预测。该模型单独分割得分为65.89％，分割和分类得分为52.41％。<br>Santos和Guimaraes（2015）在字的字符上实现了CNN模型，与中心字及其邻居的字嵌入连接，馈送到前馈网络，然后是维特比算法来预测每个字的标签。该模型在西班牙CoNLL 2002数据上获得82.21％F评分，在葡萄牙NER数据上获得71.23％F评分（Santos和Cardoso，2007）。<br>第二种类型的模型将单词嵌入与LSTM（有时是双向）连接在一个单词的字符上，将该表示通过另一个句子级别的Bi-LSTM，并使用最终的softmax或CRF层预测最终的标签（图3） ）。 Lample等。 （2016）13引入了这种架构，分别从2002年和2003年的CoNLL上获得了西班牙语，荷兰语，英语和德语NER数据集的85.75％，81.74％，90.94％，78.76％Fscores。<br>Dernoncourt等人。 （2017）在NeuroNER工具包14中实现了这个模型，其主要目标是提供简单的可用性，并允许轻松绘制模型的实时性能和学习统计数据。 BRAT注释工具15还与NeuroNER集成，以便在新领域中轻松开发NN NER模型。 NeuroNER在英国CoNLL 2003数据上获得了90.50％的F评分。<br>Habibi等。 （2017）实施了各种生物医学NER任务的模型，并取得了比大多数其他参与者更高的性能。例如，他们在CHEMDNER数据上获得了83.71 F-分数（Krallinger等，2015）。<br>Bharadwaj等人。 （2016）16除了字符和单词之外，还使用了来自NIT的音素（来自Epitran）。它们还利用对词中字符序列的注意知识，这些字符与单词嵌入和单词的字符表示相结合。该模型在西班牙CoNLL 2002数据集上实现了最先进的性能（85.81％F得分）。<br>Yang等人提出了一种略微改进的系统，侧重于多任务和多语言联合学习。 （2016）其中由GRU（门控递归单元）单元格给出的字符加字符加上字嵌入的字表示通过另一个RNN层，并且输出被给予针对不同任务（如POS，分块和NER）训练的CRF模型。杨等人。 （2017）进一步提出了多任务和多学习的转学习，并对CoNLL 2002和2003 NER数据进行了小幅改进，分别达到西班牙语，荷兰语和英语的85.77％，85.19％，91.26％F分数。</p>
<h5 id="6-4-4-Character-Word-affix-model"><a href="#6-4-4-Character-Word-affix-model" class="headerlink" title="6.4.4 Character + Word + affix model"></a>6.4.4 Character + Word + affix model</h5><p>Yadav et al. (2018) implemented a model that augments the character+word NN architecture with one of the most successful features from feature-engineering approaches: affixes. Affix features were used in early NER systems for CoNLL 2002 (Tjong Kim Sang, 2002; Cucerzan and Yarowsky, 2002) and 2003 (Tjong Kim Sang and De Meulder, 2003) and for biomedical NER (Saha et al., 2009), but had not been used in neural NER systems. They extended the Lample et al. (2016) character+word model to learn affix embeddings17 alongside the word embeddings and character RNNs (Figure 4). They considered all n-gram prefixes and suffixes of words in the training corpus, and selected only those whose frequency was above a threshold, T. Their word+character+affix model achieved 87.26%, 87.54%, 90.86%, 79.01% on Spanish, Dutch, English and German CoNLL datasets respectively. Yadav et al. (2018) also showed that affix embeddings capture complementary information to that captured by RNNs over the characters of a word, that selecting only high frequency (realistic) affixes was important, and that embedding affixes was better than simply expanding the other embeddings to reach a similar number of hyper-parameters.<br>亚达夫等人。 （2018）实现了一个模型，该模型使用特征工程方法中最成功的特征之一来增强字符+单词NN体系结构：词缀。在CoNLL 2002（Tjong Kim Sang，2002; Cucerzan和Yarowsky，2002）和2003（Tjong Kim Sang和De Meulder，2003）以及生物医学NER（Saha等，2009）的早期NER系统中使用了词缀特征，但是尚未用于神经NER系统。他们扩展了Lample等人。 （2016）字符+单词模型学习词缀嵌入17以及单词嵌入和字符RNN（图4）。他们考虑了训练语料库中所有n-gram前缀和单词后缀，并且仅选择频率高于阈值T的那些。他们的单词+字符+词缀模型在西班牙语上达到87.26％，87.54％，90.86％，79.01％ ，荷兰语，英语和德语CoNLL数据集。亚达夫等人。 （2018）还表明，词缀嵌入捕获RNN捕获的关于词的字符的补充信息，仅选择高频（真实）词缀是重要的​​，并且嵌入词缀比简单地扩展其他嵌入到达a相似数量的超参数。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2019/04/03/文献摘要之Short-Text-Clustering-via-Convolutional-Neural-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/04/03/文献摘要之Short-Text-Clustering-via-Convolutional-Neural-Networks/" itemprop="url">
                  文献摘要之Short-Text-Clustering-via-Convolutional-Neural-Networks
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-03T20:37:31+08:00">
                2019-04-03
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/03/文献摘要之Short-Text-Clustering-via-Convolutional-Neural-Networks/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/04/03/文献摘要之Short-Text-Clustering-via-Convolutional-Neural-Networks/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Short Text Clustering via Convolutional Neural Networks</p>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Short text clustering has become an increasing important task with the popularity of social media, and it is a challenging problem due to its sparseness of text representation. In this paper, we propose a Short Text Clustering via Convolutional neural networks (abbr. to STCC), which is more beneficial for clustering by considering one constraint on learned features through a self-taught learning framework without using any external tags/labels. First, we embed the original keyword features into compact binary codes with a localitypreserving constraint. Then, word embeddings are explored and fed into convolutional neural networks to learn deep feature representations, with the output units fitting the pre-trained binary code in the training process. After obtaining the learned representations, we use K-means to cluster them. Our extensive experimental study on two public short text datasets shows that the deep feature representation learned by our approach can achieve a significantly better performance than some other existing features, such as term frequency-inverse document frequency, Laplacian eigenvectors and average embedding, for clustering.<br>随着社交媒体的普及，短文本聚类已成为一项日益重要的任务，由于其文本表示的稀疏性，它是一个具有挑战性的问题。在本文中，我们通过卷积神经网络（简称STCC）提出了一种短文本聚类，通过自学习学习框架考虑学习特征的一个约束而不使用任何外部标签/标签，这对聚类更有利。首先，我们将原始关键字特征嵌入到具有局部保持约束的紧凑二进制代码中。然后，探索单词嵌入并将其馈入卷积神经网络以学习深度特征表示，其中输出单元在训练过程中拟合预训练的二进制代码。在获得学习的表示后，我们使用K-means来聚类它们。我们对两个公共短文本数据集的广泛实验研究表明，通过我们的方法学习的深度特征表示可以实现比其他一些现有特征明显更好的性能，例如术语频率 - 逆文档频率，拉普拉斯特征向量和平均嵌入，用于聚类。</p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h3><p>Different from the normal text clustering, short text clustering has the problem of sparsity(Aggarwal and Zhai, 2012). Most words only occur once in each short text, as a result, the term frequencyinverse document frequency (TF-IDF) measure cannot work well in the short text setting. In order to address this problem, some researchers work on expanding and enriching the context of data from Wikipedia (Banerjee et al., 2007) or an ontology (Fodeh et al., 2011). However, these methods involve solid natural language processing (NLP) knowledge and still use high-dimensional representation which may result in a waste of both memory and computation time. Another way to overcome these issues is to explore some sophisticated models to cluster short texts. For example, Yin and Wang (2014) proposed a Dirichlet multinomial mixture model-based approach for short text clustering and Cai et al. (2005) clustered texts using Locality Preserving Indexing (LPI) algorithm. Yet how to design an effective model is an open question, and most of these methods directly trained based on bagof-words (BoW) are shallow structures which cannot preserve the accurate semantic similarities.<br>与普通文本聚类不同，短文本聚类具有稀疏性问题（Aggarwal和Zhai，2012）。大多数单词仅在每个短文本中出现一次，因此，术语频率反向文档频率（TF-IDF）度量在短文本设置中不能很好地起作用。为了解决这个问题，一些研究人员致力于扩展和丰富维基百科（Banerjee等，2007）或本体论（Fodeh等，2011）的数据背景。然而，这些方法涉及固体自然语言处理（NLP）知识并且仍然使用高维表示，这可能导致浪费存储器和计算时间。克服这些问题的另一种方法是探索一些复杂的模型来聚类短文本。例如，Yin和Wang（2014）提出了一种基于Dirichlet多项式混合模型的短文本聚类方法和Cai等人。 （2005）使用局部保持索引（LPI）算法的聚类文本。然而，如何设计一个有效的模型是一个悬而未决的问题，而且大多数基于bagof-words（BoW）直接训练的方法都是浅层结构，不能保持准确的语义相似性。<br>With the recent revival of interest in Deep Neural Network (DNN), many researchers have concentrated on using Deep Learning to learn features. Hinton and Salakhutdinov (2006) use deep auto encoder (DAE) to learn text representation from raw text representation. Recently, with the help of word embedding, neural networks demonstrate their great performance in terms of constructing text representation, such as Recursive Neural Network (RecNN) (Socher et al., 2011; Socher et al., 2013) and Recurrent Neural Network (RNN) (Mikolov et al.,2011). However, RecNN exhibits high time complexity to construct the textual tree, and RNN, using the layer computed at the last word to represent the text, is a biased model (Lai et al., 2015). More recently, Convolution Neural Network (CNN), applying convolutional filters to capture local features, has achieved a better performance in many NLP applications, such as sentence modeling (Blunsom et al.,2014), relation classification (Zeng et al., 2014), and other traditional NLP tasks (Collobert et al., 2011).Most of the previous works focus CNN on solving supervised NLP tasks, while in this paper we aim to explore the power of CNN on one unsupervised NLP task, short text clustering.<br>随着最近人们对深度神经网络（DNN）兴趣的兴起，许多研究人员将注意力集中在使用深度学习来学习特征。 Hinton和Salakhutdinov（2006）使用深度自动编码器（DAE）来学习原始文本表示的文本表示。最近，在文字嵌入的帮助下，神经网络在构建文本表示方面表现出了很好的表现，如递归神经网络（RecNN）（Socher等，2011; Socher等，2013）和递归神经网络（ RNN）（Mikolov等，2011）。然而，RecNN表现出构建文本树的高时间复杂度，并且使用在最后一个词处计算的层来表示文本的RNN是偏向模型（Lai等人，2015）。最近，卷积神经网络（CNN）应用卷积滤波器捕获局部特征，在许多NLP应用中取得了更好的性能，例如句子建模（Blunsom等，2014），关系分类（Zeng et al。，2014） ）和其他传统的NLP任务（Collobert等，2011）。以前的大部分工作都集中在CNN上解决有监督的NLP任务，而在本文中我们的目的是探讨CNN在一个无监督的NLP任务，短文本聚类上的力量。 。<br><img src="/2019/04/03/文献摘要之Short-Text-Clustering-via-Convolutional-Neural-Networks/TIM图片20190403220837.png" alt="figure1"><br>To address the above challenges, we systematically introduce a short text clustering method via convolutional neural networks. An overall architecture of the proposed method is illustrated in Figure 1. Given a short text collection X, the goal of this work is to cluster these texts into clusters C based on the deep feature representation h learned from CNN models. In order to train the CNN models, we,inspired by (Zhang et al., 2010), utilize a self-taught learning framework in our work. In particular, we first embed the original features into compact binary code B with a locality-preserving constraint. Then word vectors S projected from word embeddings are fed into a CNN model to learn the feature representation h and the output units are used to fit the pretrained binary code B. After obtaining the learned features, traditional K-means algorithm is employed to cluster texts into clusters C. The main contributions of this paper are summarized as follows:<br>1). To the best of our knowledge, this is the first attempt to explore the feasibility and effectiveness of combining CNN and traditional semantic constraint, with the help of word embedding to solve one unsupervised learning task, short text clustering.<br>2). We learn deep feature representations with locality-preserving constraint through a self-taught learning framework, and our approach do not use any external tags/labels or complicated NLP preprocessing.<br>3). We conduct experiments on two short text datasets. The experimental results demonstrate that the proposed method achieves excellent performance in terms of both accuracy and normalized mutual information.The remainder of this paper is organized as follows: In Section 2, we first describe the proposed approach STCC and implementation details. Experimental results and analyses are presented in Section 3. In Section 4, we briefly survey several related works. Finally, conclusions are given in the last Section.<br>为了解决上述挑战，我们通过卷积神经网络系统地引入了一种短文本聚类方法。图1中示出了所提出方法的总体结构。给定短文本集X，该工作的目标是基于从CNN模型学习的深度特征表示将这些文本聚类成聚类C.为了训练CNN模型，我们受到（Zhang et al。，2010）的启发，在我们的工作中使用自学的学习框架。特别是，我们首先将原始特征嵌入到具有局部性保留约束的紧凑二进制代码B中。然后将从字嵌入投射的字向量S馈入CNN模型以学习特征表示h，并且使用输出单元来拟合预训练的二进制码B.在获得学习的特征之后，使用传统的K均值算法来对文本进行聚类。本文的主要贡献概括如下：<br>1）。据我们所知，这是首次尝试探索CNN与传统语义约束相结合的可行性和有效性，借助于单词嵌入来解决一个无监督学习任务，即短文本聚类。<br>2）。我们通过自学的学习框架学习具有局部性保留约束的深度特征表示，并且我们的方法不使用任何外部标签/标签或复杂的NLP预处理。<br>3）。我们对两个短文本数据集进行了实验。实验结果表明，该方法在准确性和规范化互信息方面均取得了良好的性能。本文的其余部分安排如下：第2节，我们首先描述了提出的方法STCC和实现细节。实验结果和分析见第3节。在第4节中，我们简要地调查了几个相关的工作。最后，最后一节给出了结论。</p>
<h3 id="2-Methodology"><a href="#2-Methodology" class="headerlink" title="2 Methodology"></a>2 Methodology</h3><h4 id="2-1-Convolutional-Neural-Networks"><a href="#2-1-Convolutional-Neural-Networks" class="headerlink" title="2.1 Convolutional Neural Networks"></a>2.1 Convolutional Neural Networks</h4><p><img src="/2019/04/03/文献摘要之Short-Text-Clustering-via-Convolutional-Neural-Networks/TIM图片20190403220922.png" alt="figure2"><br>In this section, we will briefly review one popular deep convolutional neural network, Dynamic Convolutional Neural Network (DCNN) (Blunsom et al., 2014), which is the foundation of our proposed method.<br>Taking a neural network with two convolutional layers in Figure 2 as an example, the network transforms raw input text to a powerful representation.Particularly, let $X = \{x_i:x_i \in \mathbb{R}^{d\times1} \}_{i=1,2,\cdots,n}$ denote the set of input $n$ texts, where $d$ is the dimensionality of the original keyword features. Each raw text vector $x_i$ is projected into a matrix representation $S \in \mathbb{R}^{d_w\times s}$ by looking up a word embedding E, where $d_w$ is the dimension of word embedding features and $s$ is the length of one text. We also let $\tilde W = \{W_i\}_{i=1,2}$ and $W_O$ denote the weights of the neural networks. The network defines a transformation $f(\cdot):\mathbb R^{d\times1}\to \mathbb R^{r\times1}(d\gg r)$ which transforms an raw input text $x$ to a r-dimensional deep representation h. There are three basic operations described as follows:<br><strong>– Wide one-dimensional convolution</strong> This operation is applied to an individual row of the sentence matrix $S\in \mathbb R^{d_w\times s}$, and yields a set of sequences $C_i\in \mathbb R^{s+m-1}$where $m$ is the width of convolutional filter.<br><strong>– Folding</strong> In this operation, every two rows in a feature map component-wise are simply summed.For a map of $d_w$ rows, folding returns a map of $d_w/2$ rows, thus halving the size of the representation.<br><strong>– Dymantic k-max pooling</strong> Given a fixed pooling parameter ktop for the topmost convolutional layer, the parameter k of k-max pooling in the l-th convolutional layer can be computed as follows:<script type="math/tex">k_l = max(k_{top},\left \lceil \frac{L-l}{L} \right \rceil)</script><br>where L is the total number of convolutional layers in the network.<br>在本节中，我们将简要回顾一种流行的深度卷积神经网络 - 动态卷积神经网络(DCNN) (Blunsom et al., 2014)，这是我们提出的方法的基础。<br>以图2中带有两个卷积层的神经网络为例，网络将原始输入文本转换为强大的表示。特别是，让$X = \{x_i:x_i \in \mathbb{R}^{d\times1} \}_{i=1,2,\cdots,n}$表示输入$n$ 个文本的集合，其中$d$是原始关键字要素的维度。通过查找嵌入E的单词，将每个原始文本向量$x_i$投影到矩阵表示$S \in \mathbb{R}^{d_w\times s}$中，其中$d_w$是单词嵌入要素的维度$s$是一个文本的长度。我们还让$\tilde W = \{W_i\}_{i=1,2}$和$W_O$表示神经网络的权重。网络定义转换$f(\cdot):\mathbb R^{d\times1}\to \mathbb R^{r\times1}(d\gg r)$将原始输入文本$x$转换为r维深度表示h。有三种基本操作描述如下：</p>
<ul>
<li>宽一维卷积 此操作适用于句子矩阵 $S\in \mathbb R^{d_w\times s}$的单个行，并产生一组序列$C_i\in \mathbb R^{s+m-1}$where $m$其中$m$是卷积滤波器的宽度。</li>
<li>折叠在此操作中，特征映射中的每两行都是简单求和的。对于$d_w$ rows的映射，folding返回$d_w/2$ rows的映射，从而将表示的大小减半。</li>
<li>Dymantic k-max pooling给定最顶层卷积层的固定池参数ktop，第l个卷积层中k-max池的参数k可以如下计算：<script type="math/tex">k_l = max(k_{top},\left \lceil \frac{L-l}{L} \right \rceil)</script><br>其中L是网络中卷积层的总数。<h4 id="2-2-Locality-preserving-Constraint"><a href="#2-2-Locality-preserving-Constraint" class="headerlink" title="2.2 Locality-preserving Constraint"></a>2.2 Locality-preserving Constraint</h4>Here, we first pre-train binary code B based on the keyword features with a locality-preserving constraint, and choose Laplacian affinity loss, also used in some previous works (Weiss et al., 2009; Zhang et al., 2010). The optimization can be written as:<script type="math/tex">\min_B\sum_{i,j=1}^nS_{ij}\left \| b_i-b_j \right \|_F^2   s.t.B\in\{-1,1\}^{n\times q},B^T1=0,B^TB=I</script><br>where $S_{ij}$ is the pairwise similarity between texts $x_i$ and $x_j$ , and $\left | \cdot \right |_F$ is the Frobenius norm. The problem is relaxed by discarding $B \in \{-1, 1\}^{n\times q}$, and the q-dimensional real-valued vectors $\tilde B$ can be learned from Laplacian Eigenmap. Then, we get the binary code B via the media vector $median(\tilde B)$. In particular, we construct the $n\times n$ local similarity matrix $S$ by using heat kernel as follows:<script type="math/tex">S_{ij}=\left\{\begin{matrix}exp(-\frac{\left \| x_i-x_j \right \|^2}{2\sigma^2}),&if\,x_i\in N_k(x_j)or\,vice\,versa \\0,&otherwise\end{matrix}\right.</script><br>where,$\sigma$is a tuning parameter (default is 1) and $N_k(x)$ represents the set of k-nearest-neighbors of x.The last layer of CNN is an output layer as follows:<script type="math/tex">O = W_Oh, (4)</script>where $h$ is the deep feature representation, $o\in \mathbb R^q$ is the output vector and $W_O\in \mathbb R^{q\times r}$is weight matrix. In order to fit the pre-trained binary code B, we apply q logistic operations to the output vector O as follows:<script type="math/tex">p_i=\frac{exp(O_i)}{1+exp(O_i)}(5)</script><br>在这里，我们首先基于具有局部性保留约束的关键字特征预先训练二进制代码B，并选择拉普拉斯亲和力损失，也用于先前的一些工作中(Weiss et al., 2009; Zhang et al., 2010)。优化可写为：<script type="math/tex">\min_B\sum_{i,j=1}^nS_{ij}\left \| b_i-b_j \right \|_F^2   s.t.B\in\{-1,1\}^{n\times q},B^T1=0,B^TB=I</script><br>其中$S_{ij}$是文本$x_i$和$x_j$之间的成对相似性,$\left | \cdot \right |_F$是Frobenius规范。通过丢弃$B \in \{-1, 1\}^{n\times q}$来放宽问题，并且可以从拉普拉斯算子图中学习q维实值向量$\tilde B$。然后，我们通过媒体向量$median(\tilde B)$得到二进制代码B.特别是，我们使用热内核构造$n\times n$局部相似性矩阵$S$，如下所示：<script type="math/tex">S_{ij}=\left\{\begin{matrix}exp(-\frac{\left \| x_i-x_j \right \|^2}{2\sigma^2}),&if\,x_i\in N_k(x_j)or\,vice\,versa \\0,&otherwise\end{matrix}\right.</script><br>其中，$ \sigma $是一个调整参数（默认为1），$N_k(x)$表示x的k-最近邻居的集合.CNN的最后一层是输出层，如下所示：<script type="math/tex">O =W_Oh(4)</script>其中$ h $是深度特征表示，$o\in \mathbb R^q$是输出向量而$W_O\in \mathbb R^{q\times r}$是权重矩阵。为了拟合预先训练的二进制代码B，我们将q逻辑运算应用于输出向量O，如下所示：<script type="math/tex">p_i=\frac{exp(O_i)}{1+exp(O_i)}(5)</script><h4 id="2-3-Learning"><a href="#2-3-Learning" class="headerlink" title="2.3 Learning"></a>2.3 Learning</h4>All of the parameters to be trained are defined as $\theta$.<script type="math/tex; mode=display">\theta=\{E,\tilde W,W_O\}(6)</script>Given the training text collection X, and the pretrained binary code B, the log likelihood of the parameters can be written down as follows:<script type="math/tex">J(\theta)=\sum_{i=1}^nlogp(b_i|x_i,\theta)(6)</script><br>Following the previous work (Blunsom et al.,2014), we train the network with mini-batches by back-propagation and perform the gradient-based optimization using the Adagrad update rule (Duchi et al., 2011). For regularization, we employ dropout with 50% rate to the penultimate layer (Blunsom et al., 2014; Kim, 2014).<br>所有要训练的参数都定义为$\theta$。<script type="math/tex; mode=display">\theta=\{E,\tilde W,W_O\}(6)</script>给定训练文本集合X和预训练二进制代码B，参数的对数似然可以写成如下：<script type="math/tex">J(\theta)=\sum_{i=1}^nlogp(b_i|x_i,\theta)(6)</script><br>在之前的工作(Blunsom et al.,2014)之后，我们通过反向传播对小批量网络进行训练，并使用Adagrad更新规则执行基于梯度的优化(Duchi et al., 2011)。 对于正则化，我们使用50％率的退出率到倒数第二层(Blunsom et al., 2014; Kim, 2014)<h4 id="2-4-K-means-for-Clustering"><a href="#2-4-K-means-for-Clustering" class="headerlink" title="2.4 K-means for Clustering"></a>2.4 K-means for Clustering</h4>With the given short texts, we first utilize the trained deep neural network to obtain the semantic representations h, and then employ traditional K-means algorithm to perform clustering.<br>利用给定的短文本，我们首先利用训练好的深度神经网络获得语义表示h，然后采用传统的K-means算法进行聚类。</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/" itemprop="url">
                  文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-01T15:40:37+08:00">
                2019-04-01
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>声明：作者翻译论文仅为学习，如有侵权请联系作者删除博文，谢谢！</strong><br>Semi-supervised Clustering for Short Text via Deep Representation Learning<br>基于深度表示学习的短文本半监督聚类</p>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>In this work, we propose a semi-supervised method for short text clustering, where we represent texts as distributed vectors with neural networks, and use a small amount of labeled data to specify our intention for clustering. We design a novel objective to combine the representation learning process and the kmeans clustering process together, and optimize the objective with both labeled data and unlabeled data iteratively until convergence through three steps: (1) assign each short text to its nearest centroid based on its representation from the current neural networks; (2) reestimate the cluster centroids based on cluster assignments from step (1); (3) update neural networks according to the objective by keeping centroids and cluster assignments fixed.Experimental results on four datasets show that our method works significantly better than several other text clustering methods.</p>
<p>在这项工作中，我们提出了一种用于短文本聚类的半监督方法，其中我们将文本表示为具有神经网络的分布式向量，并使用少量标记数据来指定我们的聚类意图。 我们设计了一个新的目标，将表示学习过程和kmeans聚类过程结合在一起，迭代地用标记数据和未标记数据优化目标，直到通过三个步骤收敛：（1）基于其分配每个短文本到最近的质心 来自当前神经网络的表示; （2）根据步骤（1）中的聚类分配重新估计聚类质心; （3）通过保持质心和聚类分配固定，根据目标更新神经网络。对四个数据集的实验结果表明，我们的方法比其他几种文本聚类方法效果明显更好。</p>
<h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h3><p>Text clustering is a fundamental problem in text mining and information retrieval. Its task is to group similar texts together such that texts within a cluster are more similar to texts in other clusters. Usually, a text is represented as a bag-of-words or term frequency-inverse document frequency (TFIDF) vector, and then the k-means algorithm (MacQueen, 1967) is performed to partition a set of texts into homogeneous groups.<br>However, when dealing with short texts, the characteristics of short text and clustering task raise several issues for the conventional unsupervised clustering algorithms. First, the number of uniqe words in each short text is small, as a result, the lexcical sparsity issue usually leads to poor clustering quality (Dhillon and Guan, 2003). Second, for a specific short text clustering task, we have prior knowledge or paticular intenstions before clustering, while fully unsupervised approaches may learn some classes the other way around. Take the sentences in Table 1 for example, those sentences can be clustered into different partitions based on different intentions: apple{a, b, c} and orange {d, e, f} with a fruit type intension, or what-question {a, d}, when-question {b, e},and yes/no-question cluster {c, f} with a question type intension.<br>文本聚类是文本挖掘和信息检索中的基本问题。 其任务是将类似的文本组合在一起，使得群集内的文本更类似于其他群集中的文本。 通常，文本被表示为词袋或术语频率 - 逆文档频率（TFIDF）向量，然后执行k均值算法（MacQueen，1967）以将一组文本划分为同类组。<br>然而，在处理短文本时，短文本和聚类任务的特征为传统的无监督聚类算法提出了若干问题。首先，每个短文本中的单词数量很少，因此，词汇稀疏性问题通常会导致较差的聚类质量（Dhillon和Guan，2003）。其次，对于特定的短文本聚类任务，我们在聚类之前具有先验知识或特定强度，而完全无监督的方法可以反过来学习一些类。 以表1中的句子为例，这些句子可以根据不同的意图聚类到不同的分区：苹果{a，b，c}和橘子{d，e，f}，具有水果类型内涵，或者什么问题{a，d}，什么时候问题{b，e}，是否问题{c，f}，带有问题类型含义。<br><img src="/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/TIM图片20190401153334.png" alt="table1"><br>To address the lexical sparity issue, one direction is to enrich text representations by extracting features and relations from Wikipedia (Banerjee et al.,2007) or an ontology (Fodeh et al., 2011). But this approach requires the annotated knowlege, which is also language dependent. So the other direction, which directly encode texts into distributed vectors with neural networks (Hinton and Salakhutdinov,2006; Xu et al., 2015), becomes more interesing. To tackle the second problem, semi-supervised approaches (e.g. (Bilenko et al., 2004; Davidson and Basu, 2007; Bair, 2013)) have gained significant popularity in the past decades. Our question is can we have a unified model to integrate netural networks into the semi-supervied framework?<br>为了解决词汇间性问题，一个方向是通过从维基百科(Banerjee et al.,2007)或本体论(Fodeh et al., 2011)中提取特征和关系来丰富文本表示。但是这种方法需要带注释的知识，这也是语言依赖的。因此，使用神经网络将文本直接编码为分布式向量的另一个方向（Hinton和Salakhutdinov，2006; Xu等，2015）变得更加有意义。为了解决第二个问题，半监督方法 (e.g. (Bilenko et al., 2004; Davidson and Basu, 2007; Bair, 2013))在过去几十年中获得了极大的欢迎。我们的问题是，我们可以有一个统一的模型将神经网络整合到半监督框架中吗？</p>
<p>In this paper, we propose a unified framework for the short text clustering task. We employ a deep neural network model to represent short sentences, and integrate it into a semi-supervised algorithm. Concretely, we extend the objective in the classical unsupervised k-means algorithm by adding a penalty term from labeled data. Thus, the new objective covers three key groups of parameters: centroids of clusters, the cluster assignment for each text, and the parameters within deep neural networks. In the training procedure, we start from random initialization of centroids and neural networks, and then optimize the objective iteratively through three steps until converge:<br>(1) assign each short text to its nearest centroid based on its representation from the current neural networks;<br>(2) re-estimate cluster centroids based on cluster assignments from step (1);<br>(3) update neural networks according to the objective by keeping centroids and cluster assignments fixed.<br>Experimental results on four different datasets show that our method achieves significant improvements over several other text clustering methods In following parts, we first describe our neural network models for text representaion (Section 2).Then we introduce our semi-supervised clustering method and the learning algorithm (Section 3). Finally, we evaluate our method on four different datasets (Section 4).<br>在本文中，我们提出了一个用于短文本聚类任务的统一框架。我们采用深度神经网络模型来表示短句，并将其整合到半监督算法中。具体地说，我们通过从标记数据中添加惩罚项来扩展经典无监督k均值算法的目标。因此，新目标涵盖三个关键参数组：聚类的质心，每个文本的聚类分配以及深度神经网络中的参数。在训练过程中，我们从质心和神经网络的随机初始化开始，然后通过三个步骤迭代地优化目标直到收敛：<br>（1）根据当前神经网络的表示，将每个短文本分配到最近的质心;<br>（2）根据步骤（1）中的聚类分配重新估计聚类质心;<br>（3）通过保持质心和簇分配固定，根据目标更新神经网络。<br>四个不同数据集的实验结果表明，我们的方法比其他几种文本聚类方法有了显着的改进。在下面的部分中，我们首先描述了用于文本表示的神经网络模型（第2节）。然后我们介绍了我们的半监督聚类方法和学习。算法（第3节）。最后，我们在四个不同的数据集上评估我们的方法（第4节）。</p>
<h3 id="2-Representation-Learning-for-Short-Texts"><a href="#2-Representation-Learning-for-Short-Texts" class="headerlink" title="2 Representation Learning for Short Texts"></a>2 Representation Learning for Short Texts</h3><p>We represent each word with a dense vector w, so that a short text s is first represented as a matrix $S = [w_1, …, w_{|s|}]$, which is a concatenation of all vectors of w in s, |s| is the length of s. Then we design two different types of neural networks to ingest the word vector sequence S: the convolutional neural networks (CNN) and the long short-term memory(LSTM). More formally, we define the presentation function as $x = f(s)$, where x is the represent vector of the text s. We test two encoding functions (CNN and LSTM) in our experiments.<br>我们用密集向量w表示每个单词，因此短文本s首先表示为矩阵$S = [w_1, …, w_{|s|}]$，它是s中所有w的向量的连接，|s|是s的长度。然后我们设计了两种不同类型的神经网络来摄取单词矢量序列S：卷积神经网络（CNN）和长短期记忆（LSTM）。更正式地，我们将表示函数定义为$x = f(s)$，其中x是文本s的表示向量。我们在实验中测试了两种编码函数（CNN和LSTM）。<br><img src="/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/]DFHXJ5RX}G[A242WKCXNT6.png" alt="figure1"><br>Inspired from Kim (2014), our CNN model views the sequence of word vectors as a matrix, and applies two sequential operations: convolution and maxpooling. Then, a fully connected layer is employed to convert the final representation vector into a fixed size. Figure 1 gives the diagram of the CNN model. In the convolution operation, we define a list of filters ${w_o}$, where the shape of each filter is d × h, dis the dimension of word vectors and h is the window size. Each filter is applied to a patch (a window size h of vectors) of S, and generates a feature. We apply this filter to all possible patches in S, and produce a series of features. The number of features depends on the shape of the filter wo and the length of the input short text. To deal with variable feature size, we perform a max-pooling operation over all the features to select the maximum value. Therefore, after the two operations, each filter generates only one feature. We define several filters by varying the window size and the initial values. Thus, a vector of features is captured after the max-pooling operation, and the feature dimension is equal to the number of filters.<br>受Kim（2014）的启发，我们的CNN模型将单词向量序列视为矩阵，并应用两个连续运算：卷积和最大化。然后，使用完全连接的层将最终表示矢量转换为固定大小。图1给出了CNN模型的示意图。在卷积运算中，我们定义了一个过滤器列表${w_o}$，其中每个过滤器的形状是d×h，dis是单词向量的维度，h是窗口大小。每个滤波器应用于S的贴片（矢量的窗口大小h），并生成特征。我们将此过滤器应用于S中的所有可能的补丁，并生成一系列功能。特征的数量取决于滤波器的形状和输入短文本的长度。为了处理可变特征尺寸，我们对所有特征执行最大池操作以选择最大值。因此，在两次操作之后，每个过滤器仅生成一个特征。我们通过改变窗口大小和初始值来定义几个过滤器。因此，在最大池操作之后捕获特征向量，并且特征维度等于过滤器的数量。<br><img src="/2019/04/01/文献摘要之Semi-supervised-Clustering-for-Short-Text-via-Deep-Representation-Learning/图2.png" alt="tu2"><br>Figure 2 gives the diagram of our LSTM model. We implement the standard LSTM block described in Graves (2012). Each word vector is fed into the LSTM model sequentially, and the mean of the hidden states over the entire sentence is taken as the final representation vector.<br>图2给出了我们的LSTM模型图。我们实现了Graves（2012）中描述的标准LSTM块。每个单词矢量被顺序地馈送到LSTM模型中，并且整个句子上的隐藏状态的平均值被作为最终表示矢量。</p>
<h3 id="3-Semi-supervised-Clustering-for-Short-Texts"><a href="#3-Semi-supervised-Clustering-for-Short-Texts" class="headerlink" title="3 Semi-supervised Clustering for Short Texts"></a>3 Semi-supervised Clustering for Short Texts</h3><h4 id="3-1-Revisiting-K-means-Clustering"><a href="#3-1-Revisiting-K-means-Clustering" class="headerlink" title="3.1 Revisiting K-means Clustering"></a>3.1 Revisiting K-means Clustering</h4><p>Given a set of texts $\{s_1,s_2,\cdots,s_N\}$, we represent them as a set of data points $\{x_1,x_2,\cdots,x_N\}$, where $x_i$ can be a bag-of-words or TF-IDF vector in traditional approaches, or a dense vector in Section 2.The task of text clustering is to partition the data set into some numberK of clusters, such that the sum of the squared distance of each data point to its closest cluster centroid is minimized. For each data point $x_n$, we define a set of binary variables $r_{nk}\in\{0,1\}$,where $k\in\{1,\cdots,K\}$ describing which of the K clusters $x_n$ is assigned to. So that if $x_n$ is assigned to cluster k, then $r_{nk} = 1$, and $r_{nj} = 0$ for$j\neq k$.Let’s define $u_k$ as the centroid of the k-th cluster.We can then formulate the objective function as<script type="math/tex">J_{unsup} = \sum_{n=1}^N\sum_{k=1}^K r_{nk}\left \|x_n-u_k\right \|^2</script>Our goal is the find the values of $\{r_{nk}\}$ and $\{u_k\}$ so as to minimize Junsup.<br>给出一组文本$\{s_1,s_2,\cdots,s_N\}$，我们将它们表示为一组数据点$\{x_1,x_2,\cdots,x_N\}$，其中$x_i$可以是传统方法中的词袋或TF-IDF向量，或者是第2节中的密集向量。文本聚类的任务是将数据集划分为某些数量的簇，使得总和 每个数据点到其最近的聚类质心的平方距离最小化。 对于每个数据点$x_n$，我们定义一组二进制变量$r_{nk}\in\{0,1\}$，其中$k\in\{1,\cdots,K\}$描述$x_n$分配到哪个群集。 因此，如果将$x_n$分配给簇k，则$r_{nk} = 1$，并且对于$j\neq k,r_{nj} = 0$。让我们将$u_k$定义为第k个簇的质心。然后我们可以将目标函数表示为<script type="math/tex">J_{unsup} = \sum_{n=1}^N\sum_{k=1}^K r_{nk}\left \|x_n-u_k\right \|^2</script>我们的目标是找到$\{r_{nk}\}$和$\{u_k\}$的值，以便最小化$J_{unsup}$。</p>
<p>The k-means algorithm optimizes $J_{semi}$ through the gradient descent approach, and results in an iterative procedure (Bishop, 2006). Each iteration involves two steps: E-step and M-step. In the Estep, the algorithm minimizes $J_{semi}$ with respect to $\{r_{nk}\}$ by keeping $\{u_k\}$ fixed. $J_{semi}$ is a linear function for $\{r_{nk}\}$, so we can optimize for each data point separately by simply assigning the n-th data point to the closest cluster centroid. In the M-step,the algorithm minimizes $J_{semi}$ with respect to $\{u_k\}$ by keeping $\{r_{nk}\}$ fixed. $J_{semi}$ is a quadratic function of $\{u_k\}$, and it can be minimized by setting its derivative with respect to $\{u_k\}$ to zero.<script type="math/tex">\frac{\partial J_{unsup}}{\partial \mu_k} = 2\sum_{n=1}^Nr_{nk}(x_n-\mu_k)=0(2)</script><br>Then, we can easily solve $\{u_k\}$ as<script type="math/tex">\mu_k=\frac{\sum_{n=1}^Nr_{nk}x_n}{\sum_{n=1}^Nr_{nk}}(3)</script><br>In other words, $\{u_k\}$ is equal to the mean of all the data points assigned to cluster k.<br>k-means算法通过梯度下降方法优化$J_{semi}$，并产生迭代过程（Bishop，2006）。 每次迭代都涉及两个步骤：E步和M步。 在E-step中，算法通过保持$\{u_k\}$固定来最小化$J_{semi}$相对于$\{r_{nk}\}$。 $J_{semi}$是$\{r_{nk}\}$的线性函数，因此我们可以通过简单地将第n个数据点分配给最近的聚类质心来分别优化每个数据点。 在M步骤中，算法通过保持$\{r_{nk}\}$固定来最小化$J_{semi}$相对于$\{u_k\}$。 $J_{semi}$是$\{u_k\}$的二次函数，可以通过将其导数相对于$\{u_k\}$设置为零来最小化。<script type="math/tex">\frac{\partial J_{unsup}}{\partial \mu_k} = 2\sum_{n=1}^Nr_{nk}(x_n-\mu_k)=0(2)</script><br>我们很容易得到<script type="math/tex">\mu_k=\frac{\sum_{n=1}^Nr_{nk}x_n}{\sum_{n=1}^Nr_{nk}}(3)</script><br>换句话说，$\{u_k\}$等于分配给簇k的所有数据点的平均值。</p>
<h4 id="3-2-Semi-supervised-K-means-with-Neural-Networks"><a href="#3-2-Semi-supervised-K-means-with-Neural-Networks" class="headerlink" title="3.2 Semi-supervised K-means with Neural Networks"></a>3.2 Semi-supervised K-means with Neural Networks</h4><p>The classical k-means algorithm only uses unlabeled data, and solves the clustering problem under the unsupervised learning framework. As already mentioned, the clustering results may not be consistent to our intention. In order to acquire useful clustering results, some supervised information should be introduced into the learning procedure. To this end,we employ a small amount of labeled data to guide the clustering process.<br>Following Section 2, we represent each text s as a dense vector x via neural networks $f(s)$. Instead of training the text representation model separately,we integrate the training process into the k-means algorithm, so that both the labeled data and the unlabeled data can be used for representation learning and text clustering. Let us denote the labeled data set as $\{(s_1,y_1),(s_2,y_2),\cdots,(s_L,y_L)\}$, and the unlabeled data set as $\{s_{L+1},s_{L+2},\cdots,s_N\}$, where $y_i$ is the given label for $s_i$. We then define the objective function as:<script type="math/tex">J_{semi} =\alpha \sum_{n=1}^N\sum_{k=1}^Kr_{nk}\left \|  f(s_n)-\mu_k\right \|^2+(1-\alpha)\sum_{n=1}^L\{\left \|  f(s_n)-\mu_{g_n}\right \|^2+\sum_{j\neq g_n}[l+\sum_{n=1}^L\left \|  f(s_n)-\mu_{g_n}\right \|^2-\sum_{n=1}^L\left \|  f(s_n)-\mu_{j}\right \|^2]_+\}</script><br>经典的k-means算法仅使用未标记的数据，并解决了无监督学习框架下的聚类问题。 如前所述，聚类结果可能与我们的意图不一致。为了获得有用的聚类结果，应该在学习过程中引入一些监督信息。为此，我们使用少量标记数据来指导群集过程。<br>在第2节之后，我们通过神经网络$f(s)$将每个文本s表示为密集向量x。我们不是单独训练文本表示模型，而是将训练过程集成到k-means算法中，以便标记数据和未标记数据都可以用于表示学习和文本聚类。 让我们将标记数据集表示为$\{(s_1,y_1),(s_2,y_2),\cdots,(s_L,y_L)\}$，并将未标记数据集表示为$\{s_{L+1},s_{L+2},\cdots,s_N\}$，其中$y_i$是$s_i$的给定标签。 然后我们将目标函数定义为：</p>
<script type="math/tex; mode=display">J_{semi} =\alpha \sum_{n=1}^N\sum_{k=1}^Kr_{nk}\left \|  f(s_n)-\mu_k\right \|^2+(1-\alpha)\sum_{n=1}^L\{\left \|  f(s_n)-\mu_{g_n}\right \|^2+\sum_{j\neq g_n}[l+\sum_{n=1}^L\left \|  f(s_n)-\mu_{g_n}\right \|^2-\sum_{n=1}^L\left \|  f(s_n)-\mu_{j}\right \|^2]_+\}</script><p>The objective function contains two terms. The first term is adapted from the unsupervised k-means algorithm in Eq. (1), and the second term is defined to encourage labeled data being clustered in correlation with the given labels. $\alpha\in[0,1]$ is used to tune the importance of unlabeled data. The second term contains two parts. The first part penalizes large distance between each labeled instance and its correct cluster centroid, where $g_n=G(y_n)$ is the cluster ID mapped from the given label yn, and the mapping function $G(\cdot)$ is implemented with the Hungarian algorithm (Munkres, 1957). The second part is denoted as a hinge loss with a margin l, where $[x]_+ = max(x,0)$. This part incurs some loss if the distance to the correct centroid is not shorter (by the margin l) than distances to any of incorrect cluster centroids.<br>目标函数包含两个部分。第一项改编自方程式中的无监督k均值算法。(1)，并且第二项被定义为鼓励标记数据与给定标签相关联地聚类。$\alpha\in[0,1]$用于调整未标记数据的重要性。第二项包含两部分。第一部分惩罚每个标记实例与其正确的聚类质心之间的大距离，其中$g_n=G(y_n)$是从给定标签$y_n$映射的聚类ID，映射函数$G(\cdot)$用匈牙利算法(Munkres, 1957)实现。第二部分表示为具有边界l的铰链损耗，其中$[x]_+ = max(x,0)$。如果到正确质心的距离不短（通过边缘l）而不是到任何不正确的聚类质心的距离，则该部分会引起一些损失。</p>
<p>There are three groups of parameters in $J_{semi}$: the cluster assignment of each text $\{r_{nk}\}$, the cluster centroids $\{\mu_k\}$, and the parameters within the neural network model $f(\cdot)$. Our goal is the find the values of $\{r_{nk}\}$, $\{\mu_k\}$ and parameters in $f(\cdot)$, so as to minimize $J_{semi}$. Inspired from the k-means algorithm,we design an algorithm to successively minimize $J_{semi}$ with respect to $\{r_{nk}\}$, $\{\mu_k\}$, and parameters in $f(\cdot)$. Table 2 gives the corresponding pseudocode.First, we initialize the cluster centroids $\{\mu_k\}$ with the k-means++ strategy (Arthur and Vassilvitskii,2007), and randomly initialize all the parameters in the neural network model. Then, the algorithm iteratively goes through three steps (assign cluster, estimate centroid, and update parameter) until $J_{semi}$ converges.<br>$J_{semi}$中有三组参数：每个文本的集群分配$\{r_{nk}\}$，集群质心$\{\mu_k\}$，以及神经网络模型$f(\cdot)$中的参数。我们的目标是在$f(\cdot)$中找到$\{r_{nk}\}$，$\{\mu_k\}$和参数的值，以便最小化$J_{semi}$。受k-means算法的启发，我们设计了一种算法，相对于$\{r_{nk}\}$，$\{\mu_k\}$和$f(\cdot)$中的参数，连续地最小化$J_{semi}$。表2给出了相应的伪代码。首先，我们用k-means ++策略初始化聚类中心$\{\mu_k\}$（Arthur和Vassilvitskii，2007），并随机初始化神经网络模型中的所有参数。然后，算法迭代地经历三个步骤（分配簇，估计质心和更新参数）直到$J_{semi}$收敛。</p>
<p>The assign cluster step minimizes $J_{semi}$ with respect to $\{r_{nk}\}$ by keeping $f(\cdot)$ and $\{\mu_k\}$ fixed. Its goal is to assign a cluster ID for each data point. We can see that the second term in Eq. (4) has no relation with $\{r_{nk}\}$. Thus, we only need to minimize the first term by assigning each text to its nearest cluster centroid, which is identical to the E-step in the k-means algorithm. In this step, we also calculate the mappings between the given labels $\{y_i\}$ and the cluster IDs (with the Hungarian algorithm) based on cluster assignments of all labeled data.<br>分配簇步骤通过保持$f(\cdot)$和$\{\mu_k\}$固定来最小化$J_{semi}$相对于$\{r_{nk}\}$。其目标是为每个数据点分配一个集群ID。我们可以看到方程式(4)中的第二项与$\{r_{nk}\}$无关。因此，我们只需要通过将每个文本分配到其最近的聚类质心来最小化第一项，这与k均值算法中的E步骤相同。在此步骤中，我们还基于所有标记数据的集群分配来计算给定标签$\{y_i\}$与集群ID（使用匈牙利算法）之间的映射。</p>
<p>The estimate centroid step minimizes $J_{semi}$ with respect to $\{\mu_k\}$ by keeping $\{r_{nk}\}$ and $f(\cdot)$ fixed,which corresponds to the M-step in the k-means algorithm. It aims to estimate the cluster centroids$\{\mu_k\}$ based on the cluster assignments $\{r_{nk}\}$ from the assign cluster step. The second term in Eq.(4) makes each labeled instance involved in the estimating process of cluster centroids. By solving $\partial J_{semi}/\partial \mu_k = 0$, we get<script type="math/tex">u_k =\frac{\sum_{n=1}^N\alpha r_{nk}f(s_n)+\sum_{n=1}^L w_{nk}f(s_n)}{\sum_{n=1}^N\alpha r_{nk}+\sum_{n=1}^L w_{nk}} (5)</script><br>${I}’_{nk} = \delta(k,g_n)$<br>${I}’’_{nkj} = \delta(k,j)\cdot \delta’_{nj}$<br>${I}’’’_{nkj} = (1-\delta(k,j))\cdot \delta’_{nj}$<br>$\delta’_{nj} = \delta(l+\left|f(s_n)-\mu_{g_n}\right|^2-\left|f(s_n)-\mu_{j}\right|^2&gt;0)(6)$<br>where $\delta(x_1, x_2)=1$ if $x_1$ is equal to $x_2$, otherwise $\delta(x_1, x_2)=0$, and $\delta(x)=1$ if x is true, otherwise $\delta(x)=0$. The first term in the numerator of Eq. (5) is the contributions from all data points, and $\alpha r_{nk}$ is the weight of $s_n$ for $\mu_k$. The second term is acquired from labeled data, and $w_{nk}$ is the weight of a labeled instance $s_n$ for $\mu_k$.<br>通过保持$\{r_{nk}\}$和$f(\cdot)$固定，估计质心步骤使$J_{semi}$相对于$\{\mu_k\}$最小化，这对应于k均值算法中的M步。它旨在根据分配集群步骤中的集群分配$\{r_{nk}\}$估计集群质心$\{\mu_k\}$。方程(4)中的第二项使得每个标记实例参与聚类质心的估计过程。通过求解$\partial J_{semi}/\partial \mu_k = 0$，我们得到了<script type="math/tex">u_k =\frac{\sum_{n=1}^N\alpha r_{nk}f(s_n)+\sum_{n=1}^L w_{nk}f(s_n)}{\sum_{n=1}^N\alpha r_{nk}+\sum_{n=1}^L w_{nk}}</script><br>${I}’_{nk} = \delta(k,g_n)$<br>${I}’’_{nkj} = \delta(k,j)\cdot \delta’_{nj}$<br>${I}’’’_{nkj} = (1-\delta(k,j))\cdot \delta’_{nj}$<br>$\delta’_{nj} = \delta(l+\left|f(s_n)-\mu_{g_n}\right|^2-\left|f(s_n)-\mu_{j}\right|^2&gt;0)$<br>其中$ \delta(x_1,x_2)= 1 $如果$ x_1 $等于$ x_2 $，否则$ \delta(x_1,x_2)= 0 $，如果x为真，$ \delta(x)= 1 $ ，否则$ \delta(x)= 0 $。 方程式(5)分子中的第一项是所有数据点的贡献，$ \alpha r_{nk} $是$ \mu_k $的$ s_n $的权重。 第二个术语是从标记数据中获取的，$ w_{nk} $是$ \mu_k $的标记实例$s_n$的权重。</p>
<p>The update parameter step minimizes $J_{semi}$ with respect to $f(\cdot)$ by keeping $\{r_{nk}\}$ and $\{\mu_k\}$ fixed,which has no counterpart in the k-means algorithm.The main goal is to update parameters for the text representation model. We take $J_{semi}$ as the loss function, and train neural networks with the Adam algorithm (Kingma and Ba, 2014).<br>更新参数步骤通过保持$\{r_{nk}\}$和$\{\mu_k\}$固定来最小化$J_{semi}$相对于$f(\cdot)$，其在k均值算法中没有对应物。主要目标是更新文本表示模型的参数。 我们将$J_{semi}$作为损失函数，并使用Adam算法训练神经网络（Kingma和Ba，2014）</p>
<h3 id="4-Experiment"><a href="#4-Experiment" class="headerlink" title="4 Experiment"></a>4 Experiment</h3><h4 id="4-1-Experimental-Setting"><a href="#4-1-Experimental-Setting" class="headerlink" title="4.1 Experimental Setting"></a>4.1 Experimental Setting</h4><p>We evaluate our method on four short text datasets.(1) question type is the TREC question dataset (Li and Roth, 2002), where all the questions are classified into 6 categories: abbreviation, description,entity, human, location and numeric. (2) ag news dataset contains short texts extracted from the AG’s news corpus, where all the texts are classified into 4 categories: World, Sports, Business, and Sci/Tech(Zhang and LeCun, 2015). (3) dbpedia is the DBpedia ontology dataset, which is constructed by picking 14 non-overlapping classes from DBpedia 2014(Lehmann et al., 2014). (4) yahoo answer is the 10 topics classification dataset extracted from Yahoo! Answers Comprehensive Questions and Answers version 1.0 dataset by Zhang and LeCun(2015). We use all the 5,952 questions for the question type dataset. But the other three datasets contain too many instances (e.g. 1,400,000 instances in yahoo answer). Running clustering experiments on such a large dataset is quite inefficient. Following the same solution in (Xu et al., 2015), we randomly choose 1,000 samples for each classes individually for the other three datasets. Within each dataset, we randomly sample 10% of the instances as labeled data, and evaluate the performance on the remaining 90% instances. Table 3 summarizes the statistics of these datasets.<br>In all experiments, we set the size of word vector dimension as d=300 1, and pre-train the word vectors with the word2vec toolkit (Mikolov et al., 2013) on the English Gigaword (LDC2011T07). The number of clusters is set to be the same number of labels in the dataset. The clustering performance is evaluated with two metrics: Adjusted Mutual Information (AMI) (Vinh et al., 2009) and accuracy (ACC) (Amigo et al., 2009). In order to show the statistical significance, the performance of each experiment is the average of 10 trials.<br>我们在四个短文本数据集上评估我们的方法。（1）问题类型是TREC问题数据集（Li和Roth，2002），其中所有问题分为6类：缩写，描述，实体，人，位置和数字。 （2）ag新闻数据集包含从AG的新闻语料库中提取的短文本，其中所有文本分为4类：世界，体育，商业和科学/技术（Zhang和LeCun，2015）。 （3）dbpedia是DBpedia本体数据集，它是通过从DBpedia 2014中挑选14个非重叠类来构建的（Lehmann等，2014）。 （4）雅虎答案是从Yahoo!中提取的10个主题分类数据集。由Zhang和LeCun（2015）回答综合问题和答案版本1.0数据集。我们对问题类型数据集使用了所有5,952个问题。但其他三个数据集包含太多实例（例如雅虎答案中的1,400,000个实例）。在如此大的数据集上运行聚类实验是非常低效的。按照（Xu et al。，2015）中的相同解决方案，我们为其他三个数据集分别为每个类随机选择1,000个样本。在每个数据集中，我们将10％的实例随机抽样为标记数据，并评估剩余90％实例的性能。表3总结了这些数据集的统计数据。<br>在所有实验中，我们将字向量维度的大小设置为d = 300 1，并使用word2vec工具包（Mikolov等人，2013）对英语千兆字（LDC2011T07）预先训练单词向量。簇的数量设置为数据集中的标签数量。使用两个度量来评估聚类性能：调整的互信息（AMI）（Vinh等人，2009）和准确度（ACC）（Amigo等人，2009）。为了显示统计学显着性，每个实验的性能是10次试验的平均值。</p>
<h4 id="4-2-Model-Properties"><a href="#4-2-Model-Properties" class="headerlink" title="4.2 Model Properties"></a>4.2 Model Properties</h4><p>There are several hyper-parameters in our model,e.g., the output dimension of the text representation models, and the α in Eq. (4). The choice of these hyper-parameters may affect the final performance.In this subsection, we present some experiments to demonstrate the properties of our model, and find a good configuration that we use to evaluate our final model. All the experiments in this subsection were performed on the question type dataset.<br>First, we evaluated the effectiveness of the output dimension in text representation models. We switched the dimension size among {50, 100, 300,500, 1000}, and fixed the other options as: α =0.5, the filter types in the CNN model including{unigram, bigram, trigram} and 500 filters for each type. Figure 3 presents the AMIs from both CNN and LSTM models. We found that 100 is the best output dimension for both CNN and LSTM models.Therefore, we set the output dimension as 100 in the following experiments.<br>Second, we studied the effect of α in Eq. (4),which tunes the importance of unlabeled data. We varied α among {0.00001, 0.0001, 0.001, 0.01, 0.1}, and remain the other options as the last experiment. Figure 4 shows the AMIs from both CNN and LSTM models. We found that the clustering performance is not good when using a very small α<br>By increasing the value of α, we acquired progressive improvements, and reached to the peak point at α=0.01. After that, the performance dropped.Therefore, we choose α=0.01 in the following experiments. This results also indicate that the unlabeled data are useful for the text representation learning process.<br>Third, we tested the influence of the size of labeled data. We tuned the ratio of labeled instances from the whole dataset among [1%, 10%], and kept the other configurations as the previous experiment.The AMIs are shown in Figure 5. We can see that the more labeled data we use, the better performance we get. Therefore, the labeled data are quite useful for the clustering process.<br>Fourth, we checked the effect of the pre-training strategy for our models. We added a softmax layer on top of our CNN and LSTM models, where the size of the output layer is equal to the number of labels in the dataset. We then trained the model through the classification task using all labeled data.After this process, we removed the top layer, and used the remaining parameters to initialize our CNN and LSTM models. The performance for our models with and without pre-training strategy are given in Figure 6. We can see that the pre-training strategy is quite effective for our models. Therefore, we use the pre-training strategy in the following experiments.<br>在我们的模型中有几个超参数，例如，文本表示模型的输出维度，以及方程式中的α。 （4）。这些超参数的选择可能会影响最终的性能。在本小节中，我们提供了一些实验来演示模型的属性，并找到一个好的配置，用于评估我们的最终模型。本小节中的所有实验都是在问题类型数据集上执行的。<br>首先，我们评估了文本表示模型中输出维度的有效性。我们将维度大小切换为{50,100,300,500,1000}，并将其他选项固定为：α= 0.5，CNN模型中的过滤器类型包括{unigram，bigram，trigram}和每种类型的500个过滤器。图3显示了CNN和LSTM模型的AMI。我们发现100是CNN和LSTM模型的最佳输出维度。因此，我们在以下实验中将输出维度设置为100。<br>其次，我们研究了α在方程式中的影响。 （4），调整未标记数据的重要性。我们在{0.00001,0.0001,0.001,0.01,0.1}中改变α，并且作为最后一个实验保留其他选项。图4显示了CNN和LSTM模型的AMI。我们发现当使用非常小的α时，聚类性能不好<br>通过增加α的值，我们获得了渐进的改进，并达到α= 0.01的峰值点。之后，性能下降。因此，我们在以下实验中选择α= 0.01。该结果还表明未标记的数据对于文本表示学习过程是有用的。<br>第三，我们测试了标记数据大小的影响。我们调整了整个数据集中标记实例的比例[1％，10％]，并将其他配置保留为之前的实验。图6显示了AMI。我们可以看到我们使用的标记数据越多，我们得到更好的表现。因此，标记数据对于聚类过程非常有用。<br>第四，我们检查了预训练策略对我们模型的影响。我们在CNN和LSTM模型的顶部添加了softmax图层，其中输出图层的大小等于数据集中的标签数量。然后，我们使用所有标记数据通过分类任务训练模型。在此过程之后，我们移除了顶层，并使用其余参数初始化我们的CNN和LSTM模型。图6给出了有和没有预训练策略的模型的性能。我们可以看到预训练策略对我们的模型非常有效。因此，我们在以下实验中使用预训练策略。</p>
<h4 id="4-3-Comparing-with-other-Models"><a href="#4-3-Comparing-with-other-Models" class="headerlink" title="4.3 Comparing with other Models"></a>4.3 Comparing with other Models</h4><p>In this subsection, we compared our method with some representative systems. We implemented a series of clustering systems. All of these systems are based on the k-means algorithm, but they represent short texts differently:<br>bow represents each text as a bag-of-words vector.<br>tf-idf represents each text as a TF-IDF vector.<br>average-vec represents each text with the average of all word vectors within the text.<br>metric-learn-bow employs the metric learning method proposed by Weinberger et al. (2005), and learns to project a bag-of-words vector into a 300-dimensional vector based on labeled data.<br>metric-learn-idf uses the same metric learning method, and learns to map a TF-IDF vectorinto a 300-dimensional vector based on labeled data.<br>metric-learn-ave-vec also uses the metric learning method, and learns to project an averaged word vector into a 100-dimensional vector based on labeled data.<br>We designed two classifiers (cnn-classifier and lstm-classifier) by adding a softmax layer on top of our CNN and LSTM models. We trained these two classifiers with labeled data, and utilized them to predict labels for unlabeled data. We also built two text representation models (“cnn-represent.” and “lstm-represent.”) by setting parameters of our CNN and LSTM models with the corresponding parameters in cnn-classifier and lstm-classifier. Then, we used them to represent short texts into vectors, and applied the k-means algorithm for clustering.<br>Table 4 summarizes the results of all systems on each dataset, where “semi-cnn” is our semisupervised clustering algorithm with the CNN model, and “semi-lstm” is our semi-supervised clustering algorithm with the LSTM model. We grouped all the systems into three categories: unsupervised (Unsup.), supervised (Sup.), and semi-supervised (Semisup.) 2. We found that the supervised systems worked much better than the unsupervised counterparts, which implies that the small amount of labeled data is necessary for better performance. We also noticed that within the supervised systems, the systems using deep learning (CNN or LSTM) models worked better than the systems using metric learning method, which shows the power of deep learning models for short text modeling. Our “semi-cnn” system got the best performance on almost all the datasets.<br>Figure 7 visualizes clustering results on the question type dataset from four representative systems. In Figure 7(a), clusters severely overlap with each other. When using the CNN sentence representation model, we can clearly identify all clusters in Figure 7(b), but the boundaries between clusters are still obscure. The clustering results from our semisupervised clustering algorithm are given in Figure 7(c) and Figure 7(d). We can see that the boundaries between clusters become much clearer. Therefore, our algorithm is very effective for short text clustering.<br>在本小节中，我们将我们的方法与一些代表性系统进行了比较。我们实施了一系列集群系统。所有这些系统都基于k-means算法，但它们以不同的方式表示短文本：<br>bow将每个文本表示为一个词袋矢量。<br>tf-idf将每个文本表示为TF-IDF向量。<br>average-vec用文本中所有单词向量的平均值表示每个文本。<br>metric-learn-bow采用Weinberger等人提出的度量学习方法。 （2005），并且学习基于标记数据将袋子矢量投影到300维矢量中。<br>metric-learn-idf使用相同的度量学习方法，并且学习基于标记数据将TF-IDF向量映射到300维向量。<br>metric-learn-ave-vec还使用度量学习方法，并学习基于标记数据将平均单词向量投影到100维向量中。<br>我们通过在CNN和LSTM模型之上添加softmax层来设计两个分类器（cnn-classifier和lstm-classifier）。我们使用标记数据训练这两个分类器，并利用它们来预测未标记数据的标签。我们还通过使用cnn-classifier和lstm-classifier中的相应参数设置CNN和LSTM模型的参数，构建了两个文本表示模型（“cnn-represent。”和“lstm-represent。”）。然后，我们使用它们将短文本表示为向量，并应用k-means算法进行聚类。<br>表4总结了每个数据集上所有系统的结果，其中“semi-cnn”是我们使用CNN模型的半监督聚类算法，“semi-lstm”是我们使用LSTM模型的半监督聚类算法。我们将所有系统分为三类：无监督（Unsup。），监督（Sup。）和半监督（Semisup。）2。我们发现监督系统比无监督系统工作得更好，这意味着小标记数据量是提高性能所必需的。我们还注意到，在监督系统中，使用深度学习（CNN或LSTM）模型的系统比使用度量学习方法的系统工作得更好，这显示了深度学习模型用于短文本建模的能力。我们的“半cnn”系统在几乎所有数据集上都获得了最佳性能。<br>图7显示了来自四个代表性系统的问题类型数据集的聚类结果。在图7（a）中，簇严重地彼此重叠。当使用CNN语句表示模型时，我们可以清楚地识别图7（b）中的所有聚类，但聚类之间的边界仍然模糊不清。我们的半监督聚类算法的聚类结果如图7（c）和图7（d）所示。我们可以看到集群之间的界限变得更加清晰。因此，我们的算法对短文本聚类非常有效。</p>
<h3 id="5-Related-Work"><a href="#5-Related-Work" class="headerlink" title="5 Related Work"></a>5 Related Work</h3><p>Existing semi-supervised clustering methods fall into two categories: constraint-based and representation-based. In constraint-based methods (Davidson and Basu, 2007), some labeled information is used to constrain the clustering process. In representation-based methods (Bair, 2013), a representation model is first trained to satisfy the labeled information, and all data points are clustered based on representations from the representation model. Bilenko et al. (2004) proposed to integrate there two methods into a unified framework, which shares the same idea of our proposed method. However, they only employed the metric learning model for representation learning, which is a linear projection. Whereas, our method utilized deep learning models to learn representations in a more flexible non-linear space. Xu et al. (2015) also employed deep learning models for short text clustering. However, their method separated the representation learning process from the clustering process, so it belongs to the representation-based method. Whereas, our method combined the representation learning process and the clustering process together, and utilized both labeled data and unlabeled data for representation learning and clustering.<br>现有的半监督聚类方法分为两类：基于约束和基于表示。在基于约束的方法中（Davidson和Basu，2007），一些标记信息用于约束聚类过程。在基于表示的方法（Bair，2013）中，首先训练表示模型以满足标记信息，并且基于来自表示模型的表示来聚类所有数据点。 Bilenko等。 （2004）提出将两种方法整合到一个统一的框架中，它与我们提出的方法有着相同的想法。然而，他们只使用度量学习模型进行表示学习，这是一种线性投影。然而，我们的方法利用深度学习模型在更灵活的非线性空间中学习表示。徐等人。 （2015）也采用深度学习模型进行短文本聚类。但是，他们的方法将表示学习过程与聚类过程分开，因此它属于基于表示的方法。然而，我们的方法将表示学习过程和聚类过程结合在一起，并利用标记数据和未标记数据进行表示学习和聚类。</p>
<h3 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6 Conclusion"></a>6 Conclusion</h3><p>In this paper, we proposed a semi-supervised clustering algorithm for short texts. We utilized deep learning models to learn representations for short texts, and employed a small amount of labeled data to specify our intention for clustering. We integrated the representation learning process and the clustering process into a unified framework, so that both of the two processes get some benefits from labeled data and unlabeled data. Experimental results on four datasets show that our method is more effective than other competitors.<br>在本文中，我们提出了一种用于短文本的半监督聚类算法。 我们利用深度学习模型来学习短文本的表示，并使用少量标记数据来指定我们的聚类意图。 我们将表示学习过程和聚类过程集成到一个统一的框架中，这样两个过程都可以从标记数据和未标记数据中获益。 四个数据集的实验结果表明，我们的方法比其他竞争对手更有效。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>


          
          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/update/avatar.jpg"
               alt="Andeper" />
          <p class="site-author-name" itemprop="name">Andeper</p>
           
              <p class="site-description motion-element" itemprop="description">欢迎来到我的技术博客</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">41</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/tags/index.html">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Andeper</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"andeper"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  













  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



  

</body>
</html>
