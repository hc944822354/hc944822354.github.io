<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="Andeper的个人博客" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="欢迎来到我的技术博客">
<meta property="og:type" content="website">
<meta property="og:title" content="Andeper的个人博客">
<meta property="og:url" content="http://andeper.cn/index.html">
<meta property="og:site_name" content="Andeper的个人博客">
<meta property="og:description" content="欢迎来到我的技术博客">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Andeper的个人博客">
<meta name="twitter:description" content="欢迎来到我的技术博客">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://andeper.cn/"/>





  <title> Andeper的个人博客 </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
<!-- hexo-inject:begin --><!-- hexo-inject:end --><script>
  window.fbAsyncInit = function() {
    FB.init({
      appId      : '',
      xfbml      : true,
      version    : 'v2.6'
    });
  };

  (function(d, s, id){
     var js, fjs = d.getElementsByTagName(s)[0];
     if (d.getElementById(id)) {return;}
     js = d.createElement(s); js.id = id;
     js.src = "//connect.facebook.net/zh_Hans/sdk.js";
     fjs.parentNode.insertBefore(js, fjs);
   }(document, 'script', 'facebook-jssdk'));
</script>




	<div id="vk_api_transport"></div>
	<script type="text/javascript">
		window.vkAsyncInit = function() {
			VK.init({
				apiId: 
			});

			

			
		};
		setTimeout(function() {
			var el = document.createElement("script");
			el.type = "text/javascript";
			el.src = "//vk.com/js/api/openapi.js";
			el.async = true;
			document.getElementById("vk_api_transport").appendChild(el);
		}, 0);
	</script>





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?8aabc26c969f399d0abe524a29699f13";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Andeper的个人博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            公益404
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  
  <div class="algolia-popup popup search-popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>




    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2018/07/12/python爬虫之模拟登陆新浪微博/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/07/12/python爬虫之模拟登陆新浪微博/" itemprop="url">
                  python爬虫之模拟登陆新浪微博
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-12T16:15:22+08:00">
                2018-07-12
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/07/12/python爬虫之模拟登陆新浪微博/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/07/12/python爬虫之模拟登陆新浪微博/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>最近在做新浪微博爬虫这一块，准备把此项目遇到的问题写成博客的一个系列，首先遇到的问题就是新浪的登陆问题，现在大概有两种解决方案吗，第一种是使用python中的selenium模块实现模拟登陆，原理就是打开一个真正的浏览器，提取获取到的html页面中的标签实现点击，输入等事件，来实现模拟登录，这个方法的缺点就是要打开一个真正的浏览器，对资源的利用不是很有效，扩展性也不是很好，优点就是比较简单，考虑到之后要爬取微博的量级，我选择采用第二种方法，也就是抓包+模拟网络请求的方法。</p>
<h3 id="抓包"><a href="#抓包" class="headerlink" title="抓包"></a>抓包</h3><p>抓包就采用chrome自带的开发者工具，微博登陆界面如下图<br><img src="/2018/07/12/python爬虫之模拟登陆新浪微博/2018/07/登陆界面.png" alt="登陆界面"><br>在输入账号后回发送prelogin的请求，随意点击任意地方会出现验证码的输入</p>
<p>在抓包的时候要勾选preserve log选项，防止页面跳转后log消失，抓到log后不难发现其中prelogin是登陆前比较重要的网络请求，pin.php是获取验证码的网络请求，login是真正登陆时的网络请求，如下图所示<br><img src="/2018/07/12/python爬虫之模拟登陆新浪微博/2018/07/抓包.png" alt="抓包"></p>
<p>首先分析prelogin请求，点开请求详情可以看到<br><img src="/2018/07/12/python爬虫之模拟登陆新浪微博/relogin.png" alt="prelogin"><br>这是一个get请求，请求参数为<br>entry: weibo<br>callback: sinaSSOController.preloginCallBack<br>su: MTUyMDA2MjEwNTk=<br>rsakt: mod<br>checkpin: 1<br>client: ssologin.js(v1.4.19)<br>_: 1531447140449</p>
<p>其中只有su:MTUyMDA2MjEwNTk=和_:1531447140449,其中su是用base64加密过的用户名，很明显_是时间戳<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &apos;retcode&apos;: 0,</span><br><span class="line">    &apos;servertime&apos;: 1531448377,</span><br><span class="line">    &apos;pcid&apos;: &apos;tc-25f15c5d820bebb252b6f3ac5603e30c979d&apos;,</span><br><span class="line">    &apos;nonce&apos;: &apos;FHLJRZ&apos;,</span><br><span class="line">    &apos;pubkey&apos;: &apos;EB2A38568661887FA180BDDB5CABD5F21C7BFD59C090CB2D245A87AC253062882729293E5506350508E7F9AA3BB77F4333231490F915F6D63C55FE2F08A49B353F444AD3993CACC02DB784ABBB8E42A9B1BBFFFB38BE18D78E87A0E41B9B8F73A928EE0CCEE1F6739884B9777E4FE9E88A1BBE495927AC4A799B3181D6442443&apos;,</span><br><span class="line">    &apos;rsakv&apos;: &apos;1330428213&apos;,</span><br><span class="line">    &apos;exectime&apos;: 8</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>其中servertime，pcid，nonce，pubkey，rsakv在后面的请求都会用到。</p>
<p>接下来看第二个获取验证码的网络请求<br><img src="/2018/07/12/python爬虫之模拟登陆新浪微博/pin.png" alt="验证码"><br>请求参数只有三个r,s,p。<br>r暂时不知道，s是固定的0，p就是之前得到的pcid。<br>分析js代码可以知道<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">pinCode: <span class="function"><span class="keyword">function</span>(<span class="params">obj</span>) </span>&#123;</span><br><span class="line">                <span class="keyword">var</span> codePic = sinaSSOController.getPinCodeUrl();</span><br><span class="line">                nodes.pincode.src = codePic;</span><br><span class="line">                nodes.vcode.value = <span class="string">""</span>;</span><br><span class="line">                vcodeBox.style.display = <span class="string">""</span>;</span><br><span class="line">                checkCode = <span class="literal">true</span>;</span><br><span class="line">                obj &amp;&amp; obj.reason &amp;&amp; tip.setContent(obj.reason).show();</span><br><span class="line">                loginFuns.getAllTabIndexObjects()</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">this</span>.getPinCodeUrl = <span class="function"><span class="keyword">function</span>(<span class="params">a</span>) </span>&#123;</span><br><span class="line">        a == <span class="literal">undefined</span> &amp;&amp; (a = <span class="number">0</span>);</span><br><span class="line">        pcid &amp;&amp; (me.loginExtraQuery.pcid = pcid);</span><br><span class="line">        <span class="keyword">return</span> pincodeUrl + <span class="string">"?r="</span> + <span class="built_in">Math</span>.floor(<span class="built_in">Math</span>.random() * <span class="number">1e8</span>) + <span class="string">"&amp;s="</span> + a + (pcid.length &gt; <span class="number">0</span> ? <span class="string">"&amp;p="</span> + pcid : <span class="string">""</span>)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>r就是一个随机生成的8位的随机数，但实验发现，尽管随机数相同获取到的验证码也是不同的，因此还是需要破解验证码，在目前只是接入阿里云的打码接口进行识别，以后有时间也会发布自己写的打码方法。</p>
<p>最后看第三个网络请求<br><img src="/2018/07/12/python爬虫之模拟登陆新浪微博/login.png" alt="login"><br>其中pcid，ervertime，nonce，rsakv是之前获取到的参数，door是验证码，其中su是用base64加密过的用户名，sp是base64加密之后的密码<br>返回的是一个html，用正则表达式匹配到其中的链接，然后再进行一次网络请求就可以登陆成功了。  </p>
<p>登陆成功之后可以干的事情就很多了</p>
<h3 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h3><p>Python版本：3.7<br>由于python版本的关系，，urllib的使用和python有些许区别<br>首先要做的就是设置网络请求中使用cookie<br>代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">enableCookies</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#建立一个cookies 容器</span></span><br><span class="line">        cookie_container = http.cookiejar.CookieJar()</span><br><span class="line">        <span class="comment">#将一个cookies容器和一个HTTP的cookie的处理器绑定</span></span><br><span class="line">        cookie_support = urllib.request.HTTPCookieProcessor(cookie_container)</span><br><span class="line">        <span class="comment">#创建一个opener,设置一个handler用于处理http的url打开</span></span><br><span class="line">        opener = urllib.request.build_opener(cookie_support, urllib.request.HTTPHandler)</span><br><span class="line">        <span class="comment">#安装opener，此后调用urlopen()时会使用安装过的opener对象</span></span><br><span class="line">        urllib.request.install_opener(opener)</span><br></pre></td></tr></table></figure></p>
<p>然后就是写加密的算法，导入一个base64的包<br>用户名的加密如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_encrypted_name</span><span class="params">(self)</span>:</span></span><br><span class="line">    username_urllike   = urllib.request.quote(self.username)</span><br><span class="line">    username_encrypted = base64.b64encode(bytes(username_urllike,encoding=<span class="string">'utf-8'</span>))</span><br><span class="line">    <span class="keyword">return</span> username_encrypted.decode(<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure></p>
<p>密码的加密更复杂，混合了servertime和nonce<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_encrypted_pw</span><span class="params">(self,data)</span>:</span></span><br><span class="line">    rsa_e = <span class="number">65537</span> <span class="comment">#0x10001</span></span><br><span class="line">    pw_string = str(data[<span class="string">'servertime'</span>]) + <span class="string">'\t'</span> + str(data[<span class="string">'nonce'</span>]) + <span class="string">'\n'</span> + str(self.password)</span><br><span class="line">    key = rsa.PublicKey(int(data[<span class="string">'pubkey'</span>],<span class="number">16</span>),rsa_e) <span class="comment">#创建公钥</span></span><br><span class="line">    pw_encypted = rsa.encrypt(pw_string.encode(<span class="string">'utf-8'</span>), key)  <span class="comment">#加密</span></span><br><span class="line">    self.password = <span class="string">''</span>   <span class="comment">#清空password</span></span><br><span class="line">    passwd = binascii.b2a_hex(pw_encypted) <span class="comment">#加密信息转化为16进制</span></span><br><span class="line">    print(passwd)</span><br><span class="line">    <span class="keyword">return</span> passwd</span><br></pre></td></tr></table></figure></p>
<p>然后进行第一个网络请求<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_prelogin_args</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="comment">#该函数用于模拟预登录过程,并获取服务器返回的 nonce , servertime , pub_key 等信息  </span></span><br><span class="line">    json_pattern = re.compile(<span class="string">'\((.*)\)'</span>)</span><br><span class="line">    url = <span class="string">'http://login.sina.com.cn/sso/prelogin.php?entry=weibo&amp;callback=sinaSSOController.preloginCallBack&amp;su=&amp;'</span> + self.get_encrypted_name() + <span class="string">'&amp;rsakt=mod&amp;checkpin=1&amp;client=ssologin.js(v1.4.19)'</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        request = urllib.request.Request(url)</span><br><span class="line">        response = urllib.request.urlopen(request)</span><br><span class="line">        raw_data = response.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">        json_data = json_pattern.search(raw_data).group(<span class="number">1</span>)</span><br><span class="line">        data = json.loads(json_data)</span><br><span class="line">        print(data)</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line">    <span class="keyword">except</span> urllib.error <span class="keyword">as</span> e:</span><br><span class="line">        print(<span class="string">"%d"</span>%e.code)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">None</span></span><br></pre></td></tr></table></figure></p>
<p>得到服务器返回的nonce , servertime , pub_key等信息</p>
<p>进行第二个网络请求，获取验证码，并接入识别验证码平台，得到验证码的字符串<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_door</span><span class="params">(self,pcid)</span>:</span></span><br><span class="line">    url = <span class="string">"https://login.sina.com.cn/cgi/pin.php?r="</span>+str(random.randint(<span class="number">10000000</span>,<span class="number">99999999</span>))+<span class="string">"&amp;s=0&amp;p="</span>+pcid</span><br><span class="line">    print(url)</span><br><span class="line">    r= urllib.request.Request(url)</span><br><span class="line">    res = urllib.request.urlopen(r)</span><br><span class="line">    content = res.read()</span><br><span class="line">    print(base64.b64encode(content))                 </span><br><span class="line">    host = <span class="string">'http://txyzmsb.market.alicloudapi.com'</span></span><br><span class="line">    path = <span class="string">'/yzm'</span></span><br><span class="line">    appcode = <span class="string">'你的应用密钥'</span></span><br><span class="line">    fields = urllib.parse.urlencode(&#123;</span><br><span class="line">        <span class="string">'v_pic'</span>:base64.b64encode(content),</span><br><span class="line">        <span class="string">'v_type'</span>:<span class="string">'ne5'</span></span><br><span class="line">    &#125;).encode(encoding=<span class="string">'UTF8'</span>)</span><br><span class="line">    url = host + path</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'Authorization'</span>: <span class="string">'APPCODE '</span> + appcode,</span><br><span class="line">        <span class="string">'Content-Type'</span>: <span class="string">'application/x-www-form-urlencoded; charset=UTF-8'</span></span><br><span class="line">    &#125;</span><br><span class="line">    req = urllib.request.Request(url, fields,headers)</span><br><span class="line">    f = urllib.request.urlopen(req)</span><br><span class="line">    content = f.read()</span><br><span class="line">    print(content)</span><br><span class="line">    data = json.loads(content)</span><br><span class="line">    <span class="keyword">return</span> data[<span class="string">'v_code'</span>]</span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line">进行第三个网络请求，完成登陆</span><br><span class="line">```Python</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_post_data</span><span class="params">(self,raw)</span>:</span></span><br><span class="line">    door = self.get_door(raw[<span class="string">'pcid'</span>])</span><br><span class="line">    post_data = &#123;</span><br><span class="line">        <span class="string">"entry"</span>:<span class="string">"weibo"</span>,</span><br><span class="line">        <span class="string">"gateway"</span>:<span class="string">"1"</span>,</span><br><span class="line">        <span class="string">"from"</span>:<span class="string">""</span>,</span><br><span class="line">        <span class="string">"savestate"</span>:<span class="string">"7"</span>,</span><br><span class="line">        <span class="string">"qrcode_flag"</span>:<span class="string">"false"</span>,</span><br><span class="line">        <span class="string">"useticket"</span>:<span class="string">"1"</span>,</span><br><span class="line">        <span class="string">"pagerefer"</span>:<span class="string">"https://login.sina.com.cn/crossdomain2.php?action=logout&amp;r=https%3A%2F%2Fweibo.com%2Flogout.php%3Fbackurl%3D%252F"</span>,</span><br><span class="line">        <span class="string">"pcid"</span>:raw[<span class="string">'pcid'</span>],</span><br><span class="line">        <span class="string">"door"</span>:door,</span><br><span class="line">        <span class="string">"vsnf"</span>:<span class="string">"1"</span>,</span><br><span class="line">        <span class="string">"su"</span>:self.get_encrypted_name(),</span><br><span class="line">        <span class="string">"service"</span>:<span class="string">"miniblog"</span>,</span><br><span class="line">        <span class="string">"servertime"</span>:raw[<span class="string">'servertime'</span>],</span><br><span class="line">        <span class="string">"nonce"</span>:raw[<span class="string">'nonce'</span>],</span><br><span class="line">        <span class="string">"pwencode"</span>:<span class="string">"rsa2"</span>,</span><br><span class="line">        <span class="string">"rsakv"</span>:raw[<span class="string">'rsakv'</span>],</span><br><span class="line">        <span class="string">"sp"</span>:self.get_encrypted_pw(raw),</span><br><span class="line">        <span class="string">"sr"</span>:<span class="string">"1920*1080"</span>,</span><br><span class="line">        <span class="string">"encoding"</span>:<span class="string">"UTF-8"</span>,</span><br><span class="line">        <span class="string">"prelt"</span>:<span class="string">"25"</span>,</span><br><span class="line">        <span class="string">"url"</span>:<span class="string">"https://weibo.com/ajaxlogin.php?framelogin=1&amp;callback=parent.sinaSSOController.feedBackUrlCallBack"</span>,</span><br><span class="line">        <span class="string">"returntype"</span>:<span class="string">"META"</span></span><br><span class="line">    &#125;</span><br><span class="line">    data = urllib.parse.urlencode(post_data).encode(<span class="string">'utf-8'</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(self)</span>:</span></span><br><span class="line">    url = <span class="string">'http://login.sina.com.cn/sso/login.php?client=ssologin.js(v1.4.19)'</span></span><br><span class="line">    self.enableCookies()</span><br><span class="line">    data = self.get_prelogin_args()</span><br><span class="line">    post_data = self.build_post_data(data)</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"User-Agent"</span>:<span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36"</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        request = urllib.request.Request(url=url,data=post_data,headers=headers)</span><br><span class="line">        response = urllib.request.urlopen(request)</span><br><span class="line">        html = response.read().decode(<span class="string">'GBK'</span>)</span><br><span class="line">        print(html)</span><br><span class="line">    <span class="keyword">except</span> urllib.error <span class="keyword">as</span> e:</span><br><span class="line">        print(e.code)</span><br><span class="line"></span><br><span class="line">    p = re.compile(<span class="string">'location\.replace\(\'(.*?)\'\)'</span>)</span><br><span class="line">    p2 = re.compile(<span class="string">r'"userdomain":"(.*?)"'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        login_url = p.search(html).group(<span class="number">1</span>)</span><br><span class="line">        print(login_url)</span><br><span class="line">        request = urllib.request.Request(login_url)</span><br><span class="line">        response = urllib.request.urlopen(request)</span><br><span class="line">        page = response.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">        print(page)</span><br><span class="line">        login_url = <span class="string">'http://weibo.com/'</span> + p2.search(page).group(<span class="number">1</span>)</span><br><span class="line">        request = urllib.request.Request(login_url)</span><br><span class="line">        response = urllib.request.urlopen(request)</span><br><span class="line">        final = response.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">        print(<span class="string">"Login success!"</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        print(<span class="string">'Login error!'</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2018/02/21/深层神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/02/21/深层神经网络/" itemprop="url">
                  深层神经网络
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-21T18:20:38+08:00">
                2018-02-21
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/02/21/深层神经网络/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/02/21/深层神经网络/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>上节课我们主要介绍了浅层神经网络。首先介绍神经网络的基本结构，包括输入层，隐藏层和输出层。然后以简单的2 layer NN为例，详细推导了其正向传播过程和反向传播过程，使用梯度下降的方法优化神经网络参数。同时，我们还介绍了不同的激活函数，比较各自优缺点，讨论了激活函数必须是非线性的原因。最后介绍了神经网络参数随机初始化的必要性，特别是权重W，不同神经元的W不能初始化为同一零值。本节课是对上节课的延伸和扩展，讨论更深层的神经网络。</p>
<h3 id="Deep-L-layer-neural-network"><a href="#Deep-L-layer-neural-network" class="headerlink" title="Deep L-layer neural network"></a>Deep L-layer neural network</h3><p>深层神经网络其实就是包含更多的隐藏层神经网络。如下图所示，分别列举了逻辑回归、1个隐藏层的神经网络、2个隐藏层的神经网络和5个隐藏层的神经网络它们的模型结构。<br><img src="/2018/02/21/深层神经网络/deepLNN" alt="deepLNN"></p>
<p>命名规则上，一般只参考隐藏层个数和输出层。例如，上图中的逻辑回归又叫1 layer NN，1个隐藏层的神经网络叫做2 layer NN，2个隐藏层的神经网络叫做3 layer NN，以此类推。如果是L-layer NN，则包含了L-1个隐藏层，最后的L层是输出层。</p>
<p>下面以一个4层神经网络为例来介绍关于神经网络的一些标记写法。如下图所示，首先，总层数用L表示，L=4。输入层是第0层，输出层是第L层。$n^{[l]}$表示第l层包含的单元个数，l=0,1,⋯,L。这个模型中，$n^{[0]}=n_x=3$，表示三个输入特征$x_1,x_2,x_3$。$n^{[1]}=5$，$n^{[2]}=5$，$n^{[3]}=3$，$n^{[4]}=n^{[L]}=1$。第l层的激活函数输出用$a^{[l]}$表示，$a^{[l]}=g^{[l]}(z^{[l]})$。$W^{[l]}$表示第l层的权重，用于计算$z^{[l]}$。另外，我们把输入x记为$a^{[0]}$，把输出层$\hat{y}$记为$a^{[L]}$。</p>
<p>注意，$a^{[l]}$和$W^{[l]}$中的上标l都是从1开始的，l=1,⋯,L。<br><img src="/2018/02/21/深层神经网络/DNNN" alt="DNNN"></p>
<h3 id="Forward-Propagation-in-a-Deep-Network"><a href="#Forward-Propagation-in-a-Deep-Network" class="headerlink" title="Forward Propagation in a Deep Network"></a>Forward Propagation in a Deep Network</h3><p>接下来，我们来推导一下深层神经网络的正向传播过程。仍以上面讲过的4层神经网络为例，对于单个样本：</p>
<p>第1层，l=1：</p>
<script type="math/tex; mode=display">z^{[1]}=W^{[1]}x+b^{[1]}=W^{[1]}a^{[0]}+b^{[1]}</script><script type="math/tex; mode=display">a^{[1]}=g^{[1]}(z^{[1]})</script><p>第2层，l=2：</p>
<script type="math/tex; mode=display">z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}</script><script type="math/tex; mode=display">a^{[2]}=g^{[2]}(z^{[2]})</script><p>第3层，l=3：</p>
<script type="math/tex; mode=display">z^{[3]}=W^{[3]}a^{[2]}+b^{[3]}</script><script type="math/tex; mode=display">a^{[3]}=g^{[3]}(z^{[3]})</script><p>第4层，l=4：</p>
<script type="math/tex; mode=display">z^{[4]}=W^{[4]}a^{[3]}+b^{[4]}</script><script type="math/tex; mode=display">a^{[4]}=g^{[4]}(z^{[4]})</script><p>如果有m个训练样本，其向量化矩阵形式为：</p>
<p>第1层，l=1：</p>
<script type="math/tex; mode=display">Z^{[1]}=W^{[1]}X+b^{[1]}=W^{[1]}A^{[0]}+b^{[1]}</script><script type="math/tex; mode=display">A^{[1]}=g^{[1]}(Z^{[1]})</script><p>第2层，l=2：</p>
<script type="math/tex; mode=display">Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}</script><script type="math/tex; mode=display">A^{[2]}=g^{[2]}(Z^{[2]})</script><p>第3层，l=3：</p>
<script type="math/tex; mode=display">Z^{[3]}=W^{[3]}A^{[2]}+b^{[3]}</script><script type="math/tex; mode=display">A^{[3]}=g^{[3]}(Z^{[3]})</script><p>第4层，l=4：</p>
<script type="math/tex; mode=display">Z^{[4]}=W^{[4]}A^{[3]}+b^{[4]}</script><script type="math/tex; mode=display">A^{[4]}=g^{[4]}(Z^{[4]})</script><p>综上所述，对于第l层，其正向传播过程的Z[l]和A[l]可以表示为：</p>
<script type="math/tex; mode=display">Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}</script><script type="math/tex; mode=display">A^{[l]}=g^{[l]}(Z^{[l]})</script><p>其中l=1,⋯,L</p>
<h3 id="Getting-your-matrix-dimensions-right"><a href="#Getting-your-matrix-dimensions-right" class="headerlink" title="Getting your matrix dimensions right"></a>Getting your matrix dimensions right</h3><p>对于单个训练样本，输入x的维度是$(n^{[0]},1)$,神经网络的参数$W^{[l]}$和$b^{[l]}$的维度分别是：</p>
<script type="math/tex; mode=display">W^{[l]}:\ (n^{[l]},n^{[l-1]})</script><script type="math/tex; mode=display">b^{[l]}:\ (n^{[l]},1)</script><p>其中，l=1,⋯,L，$n^{[l]}$和$n^{[l−1]}$分别表示第l层和l−1层的所含单元个数。$n^{[0]}=n_x$，表示输入层特征数目。</p>
<p>顺便提一下，反向传播过程中的$dW^{[l]}$和$db^{[l]}$的维度分别是：</p>
<script type="math/tex; mode=display">dW^{[l]}:\ (n^{[l]},n^{[l-1]})</script><script type="math/tex; mode=display">db^{[l]}:\ (n^{[l]},1)</script><p>注意到，$W^{[l]}$与$dW^{[l]}$维度相同，$b^{[l]}$与$db^{[l]}$维度相同。这很容易理解。</p>
<p>正向传播过程中的$z^{[l]}$和$a^{[l]}$的维度分别是：</p>
<script type="math/tex; mode=display">z^{[l]}:\ (n^{[l]},1)</script><script type="math/tex; mode=display">a^{[l]}:\ (n^{[l]},1)</script><p>$z^{[l]}$和$a^{[l]}$的维度是一样的，且$dz^{[l]}$和$da^{[l]}$的维度均与$z^{[l]}$和$a^{[l]}$的维度一致。</p>
<p>对于m个训练样本，输入矩阵X的维度是$(n^{[0]},m)$。需要注意的是$W^{[l]}$和$b^{[l]}$的维度与只有单个样本是一致的：</p>
<script type="math/tex; mode=display">W^{[l]}:\ (n^{[l]},n^{[l-1]})</script><script type="math/tex; mode=display">b^{[l]}:\ (n^{[l]},1)</script><p>只不过在运算$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$中，$b^{[l]}$会被当成$(n^{[l]},m)$矩阵进行运算，这是因为python的广播性质，且$b^{[l]}$每一列向量都是一样的。$dW^{[l]}$和$db^{[l]}$的维度分别与$W^{[l]}$和$b^{[l]}$的相同。</p>
<p>但是，$Z^{[l]}$和$A^{[l]}$的维度发生了变化：</p>
<script type="math/tex; mode=display">Z^{[l]}:\ (n^{[l]},m)</script><script type="math/tex; mode=display">A^{[l]}:\ (n^{[l]},m)</script><p>$dZ^{[l]}$和$dA^{[l]}$的维度分别与$Z^{[l]}$和$A^{[l]}$的相同。</p>
<h3 id="Why-deep-representations"><a href="#Why-deep-representations" class="headerlink" title="Why deep representations?"></a>Why deep representations?</h3><p>我们都知道神经网络能处理很多问题，而且效果显著。其强大能力主要源自神经网络足够“深”，也就是说网络层数越多，神经网络就更加复杂和深入，学习也更加准确。接下来，我们从几个例子入手，看一下为什么深度网络能够如此强大。</p>
<p>先来看人脸识别的例子，如下图所示。经过训练，神经网络第一层所做的事就是从原始图片中提取出人脸的轮廓与边缘，即边缘检测。这样每个神经元得到的是一些边缘信息。神经网络第二层所做的事情就是将前一层的边缘进行组合，组合成人脸一些局部特征，比如眼睛、鼻子、嘴巴等。再往后面，就将这些局部特征组合起来，融合成人脸的模样。可以看出，随着层数由浅到深，神经网络提取的特征也是从边缘到局部特征到整体，由简单到复杂。可见，如果隐藏层足够多，那么能够提取的特征就越丰富、越复杂，模型的准确率就会越高。</p>
<p>语音识别模型也是这个道理。浅层的神经元能够检测一些简单的音调，然后较深的神经元能够检测出基本的音素，更深的神经元就能够检测出单词信息。如果网络够深，还能对短语、句子进行检测。记住一点，神经网络从左到右，神经元提取的特征从简单到复杂。特征复杂度与神经网络层数成正相关。特征越来越复杂，功能也越来越强大。<br><img src="/2018/02/21/深层神经网络/intutition" alt="intutition"></p>
<p>除了从提取特征复杂度的角度来说明深层网络的优势之外，深层网络还有另外一个优点，就是能够减少神经元个数，从而减少计算量。例如下面这个例子，使用电路理论，计算逻辑输出：</p>
<script type="math/tex; mode=display">y=x_1\oplus x_2\oplus x_3\oplus\cdots\oplus x_n</script><p>其中，$\oplus$表示异或操作。对于这个逻辑运算，如果使用深度网络，深度网络的结构是每层将前一层的两两单元进行异或，最后到一个输出，如下图左边所示。这样，整个深度网络的层数是$log_2(n)$，不包含输入层。总共使用的神经元个数为：</p>
<script type="math/tex; mode=display">1+2+\cdots+2^{log_2(n)-1}=1\cdot\frac{1-2^{log_2(n)}}{1-2}=2^{log_2(n)}-1=n-1</script><p>可见，输入个数是n，这种深层网络所需的神经元个数仅仅是n-1个。</p>
<p>如果不用深层网络，仅仅使用单个隐藏层，那么需要的神经元个数将是指数级别那么大。Andrew指出，由于包含了所有的逻辑位（0和1），则需要$2^{n-1}$ ?个神经元。</p>
<p>比较下来，处理同一逻辑问题，深层网络所需的神经元个数比浅层网络要少很多。这也是深层神经网络的优点之一。</p>
<p>尽管深度学习有着非常显著的优势，Andrew还是建议对实际问题进行建模时，尽量先选择层数少的神经网络模型，这也符合奥卡姆剃刀定律（Occam’s Razor）。对于比较复杂的问题，再使用较深的神经网络模型。</p>
<h3 id="Building-blocks-of-deep-neural-networks"><a href="#Building-blocks-of-deep-neural-networks" class="headerlink" title="Building blocks of deep neural networks"></a>Building blocks of deep neural networks</h3><p>下面用流程块图来解释神经网络正向传播和反向传播过程。如下图所示，对于第l层来说，正向传播过程中：</p>
<p>输入：$a^{[l-1]}$<br>输出：$a^{[l]}$<br>参数：$W^{[l]}$,$b^{[l]}$<br>缓存变量：$z^{[l]}$</p>
<p>反向传播过程中：<br>输入：$da^{[l]}$<br>输出：$da^{[l-1]}$,$dW^{[l]}$,$db^{[l]}$<br>参数：$W^{[l]}$,$b^{[l]}$<br><img src="/2018/02/21/深层神经网络/Ilayer" alt="Ilayer"></p>
<p>刚才这是第l层的流程块图，对于神经网络所有层，整体的流程块图正向传播过程和反向传播过程如下所示：<br><img src="/2018/02/21/深层神经网络/Ilayer2" alt="Ilayer2"></p>
<h3 id="Forward-and-Backward-Propagation"><a href="#Forward-and-Backward-Propagation" class="headerlink" title="Forward and Backward Propagation"></a>Forward and Backward Propagation</h3><p>我们继续接着上一部分流程块图的内容，推导神经网络正向传播过程和反向传播过程的具体表达式。</p>
<p>首先是正向传播过程，令层数为第l层，输入是$a^{[l-1]}$，输出是$a^{[l]}$，缓存变量是$z^{[l]}$。其表达式如下：</p>
<script type="math/tex; mode=display">z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}</script><script type="math/tex; mode=display">a^{[l]}=g^{[l]}(z^{[l]})</script><p>m个训练样本，向量化形式为：</p>
<script type="math/tex; mode=display">Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}</script><script type="math/tex; mode=display">A^{[l]}=g^{[l]}(Z^{[l]})</script><p>然后是反向传播过程，输入是$da^{[l]}$，输出是$da^{[l-1]}$,$dw^{[l]}$,$db^{[l]}$。其表达式如下：</p>
<script type="math/tex; mode=display">dz^{[l]}=da^{[l]}\ast g^{[l]'}(z^{[l]})</script><script type="math/tex; mode=display">dW^{[l]}=dz^{[l]}\cdot a^{[l-1]}</script><script type="math/tex; mode=display">db^{[l]}=dz^{[l]}</script><script type="math/tex; mode=display">da^{[l-1]}=W^{[l]T}\cdot dz^{[l]}</script><p>由上述第四个表达式可得$da^{[l]}=W^{[l+1]T}\cdot dz^{[l+1]}$，将$da^{[l]}$代入第一个表达式中可以得到：</p>
<script type="math/tex; mode=display">dz^{[l]}=W^{[l+1]T}\cdot dz^{[l+1]}\ast g^{[l]'}(z^{[l]})</script><p>该式非常重要，反映了$dz^{[l+1]}$与<script type="math/tex">dz^{[l]}</script>的递推关系。</p>
<p>m个训练样本，向量化形式为：</p>
<script type="math/tex; mode=display">dZ^{[l]}=dA^{[l]}\ast g^{[l]'}(Z^{[l]})</script><script type="math/tex; mode=display">dW^{[l]}=\frac1mdZ^{[l]}\cdot A^{[l-1]T}</script><script type="math/tex; mode=display">db^{[l]}=\frac1mnp.sum(dZ^{[l]},axis=1,keepdim=True)</script><script type="math/tex; mode=display">dA^{[l-1]}=W^{[l]T}\cdot dZ^{[l]}</script><script type="math/tex; mode=display">dZ^{[l]}=W^{[l+1]T}\cdot dZ^{[l+1]}\ast g^{[l]'}(Z^{[l]})</script><h3 id="Parameters-vs-Hyperparameters"><a href="#Parameters-vs-Hyperparameters" class="headerlink" title="Parameters vs Hyperparameters"></a>Parameters vs Hyperparameters</h3><p>该部分介绍神经网络中的参数（parameters）和超参数（hyperparameters）的概念。</p>
<p>神经网络中的参数就是我们熟悉的$W^{[l]}$和$b^{[l]}$。而超参数则是例如学习速率$\alpha$，训练迭代次数N，神经网络层数L，各层神经元个数$n^{[l]}$，激活函数g(z)等。之所以叫做超参数的原因是它们决定了参数$W^{[l]}$和$b^{[l]}$的值。在后面的第二门课我们还将学习其它的超参数，这里先不讨论。</p>
<p>如何设置最优的超参数是一个比较困难的、需要经验知识的问题。通常的做法是选择超参数一定范围内的值，分别代入神经网络进行训练，测试cost function随着迭代次数增加的变化，根据结果选择cost function最小时对应的超参数值。这类似于validation的方法。</p>
<h3 id="What-does-this-have-to-do-with-the-brain"><a href="#What-does-this-have-to-do-with-the-brain" class="headerlink" title="What does this have to do with the brain?"></a>What does this have to do with the brain?</h3><p>那么，神经网络跟人脑机制到底有什么联系呢？究竟有多少的相似程度？神经网络实际上可以分成两个部分：正向传播过程和反向传播过程。神经网络的每个神经元采用激活函数的方式，类似于感知机模型。这种模型与人脑神经元是类似的，可以说是一种非常简化的人脑神经元模型。如下图所示，人脑神经元可分为树突、细胞体、轴突三部分。树突接收外界电刺激信号（类比神经网络中神经元输入），传递给细胞体进行处理（类比神经网络中神经元激活函数运算），最后由轴突传递给下一个神经元（类比神经网络中神经元输出）。</p>
<p>值得一提的是，人脑神经元的结构和处理方式要复杂的多，神经网络模型只是非常简化的模型。人脑如何进行学习？是否也是通过反向传播和梯度下降算法现在还不清楚，可能会更加复杂。这是值得生物学家探索的事情。也许发现重要的新的人脑学习机制后，让我们的神经网络模型抛弃反向传播和梯度下降算法，能够实现更加准确和强大的神经网络模型！</p>
<p><img src="/2018/02/21/深层神经网络/propagation" alt="propagation"></p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了深层神经网络，是上一节浅层神经网络的拓展和归纳。首先，我们介绍了建立神经网络模型一些常用的标准的标记符号。然后，用流程块图的方式详细推导正向传播过程和反向传播过程的输入输出和参数表达式。我们也从提取特征复杂性和计算量的角度分别解释了深层神经网络为什么优于浅层神经网络。接着，我们介绍了超参数的概念，解释了超参数与参数的区别。最后，我们将神经网络与人脑做了类别，人工神经网络是简化的人脑模型。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2018/02/19/浅层神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/02/19/浅层神经网络/" itemprop="url">
                  浅层神经网络
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-19T20:37:59+08:00">
                2018-02-19
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/02/19/浅层神经网络/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/02/19/浅层神经网络/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Neural-Network-Overview"><a href="#Neural-Network-Overview" class="headerlink" title="Neural Network Overview"></a>Neural Network Overview</h3><p>前面的课程中，我们已经使用计算图的方式介绍了逻辑回归梯度下降算法的正向传播和反向传播。神经网络的结构与逻辑回归类似，只是神经网络的层数比逻辑回归多一层，多出来的中间那层称为隐藏层或中间层。这样从计算上来说，神经网络的正向传播和反向传播过程只是比逻辑回归多了一次重复的计算。正向传播过程分为两层，第一层是出入层到隐藏层，用上标[1]来表示：</p>
<script type="math/tex; mode=display">z^{[1]}=W^{[1]}x + b^{[1]}</script><script type="math/tex; mode=display">a^{[1]}=\sigma(z^{[1]})</script><p>第二层是隐藏层到输出层，用上标[2]来表示：</p>
<script type="math/tex; mode=display">z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}</script><script type="math/tex; mode=display">a^{[2]} = \sigma(z^{[2]})</script><p>在写法上值得注意的是，方括号上标[i]表示当前所处的层数；圆括号上标(i)表示第i个样本。</p>
<p>同样，反向传播过程也分成两层。第一层是输出层到隐藏层，第二层是隐藏层到输入层。其细节部分我们之后再来讨论。<br><img src="/2018/02/19/浅层神经网络/neuralnetwork" alt="neuralnetwork"></p>
<h3 id="Neural-Network-Representation"><a href="#Neural-Network-Representation" class="headerlink" title="Neural Network Representation"></a>Neural Network Representation</h3><p>下面我们以图示的方式来介绍单隐藏层的神经网络结构。如下图所示，单隐藏神经网络就是典型的浅层(shallow)神经网络<br><img src="/2018/02/19/浅层神经网络/shallowneuralnetwork" alt="shallowneuralnetwork"><br>结构上，从左到右，可以分为三层：输入层(Input layer),隐藏层(Hidden layer)和输出层(Output layer)。输入层和输出层，顾名思义，对应训练样本的输入和输出，隐藏层是抽象的非线性的中间层，在训练集中，这些中间节点的真正数值我们是不知道的，我们在训练集中看不到它们的数值，这也是被命名为隐藏层的原因。<br>在写法上，我们通常吧输入矩阵X记为$a^{[0]}$,把隐藏层输出记为$a^{[1]}$,上标从0开始。用下标表示第几个神经元，注意下标从1开始。例如$a_1^{[1]}$表示隐藏层第1个神经元，$a_2^{[1]}$表示隐藏层第二个神经元，等等。这样，隐藏层有4个神经元就可以将其输出$a^{[1]}$写成矩阵的形式：</p>
<script type="math/tex; mode=display">{a^{[1]}}= \left[ \begin{matrix} a_1^{[1]} \\ a_2^{[1]} \\ a_3^{[1]} \\ a_4^{[1]} \end{matrix} \right]</script><p>最后相应的输出层记为$a^{[2]}$,即$\hat y$。这种单隐藏层神经网络也被称为两层神经网络(2 layer NN)。之所以叫两层神经网络是因为，通常我们只会计算隐藏层输出和输出层的输出，输入层是不用计算的。这也是我们把输入层层数上标记为0的原因(a^{[0]})</p>
<p>关于隐藏层对应的权重$W^{[1]}$和常数项$b^{[1]}$,$W^{[1]}$的维度是(4,3)。这里的4对应着隐藏层神经元个数，3对应这输入层x特征向量包含元素个数。常数项$b^{[1]}$的维度是(4,1),这里的4同样对应着隐藏层神经元个数。关于输出层对应的权重$W^{[2]}$和常数项$b^{[2]}$，$W^{[2]}$的维度是(1,4),这里的1对应着输出层神经元个数，4对应着隐藏层神经元个数。常数项$b^{[2]}$的维度是(1,1),因为输出只有一个神经元。总结一下，第i层的权重$W^{[i]}$维度的行等于i层神经元的个数，列等于i-1层神经元的个数；第i层常数项$b^{[i]}$维度的行等于i层神经元的个数，列始终为1。</p>
<h3 id="Computing-a-Neural-Network’s-Output"><a href="#Computing-a-Neural-Network’s-Output" class="headerlink" title="Computing a Neural Network’s Output"></a>Computing a Neural Network’s Output</h3><p>接下来我们详细推导神经网络的计算过程。回顾一下，我们前面讲过两层神经网络可以看成是逻辑回归再重复计算一次。如下图所示，逻辑回归的正向计算可以分解成z和a的两部分：</p>
<script type="math/tex; mode=display">z=w^Tx+b</script><script type="math/tex; mode=display">a= \sigma(z)</script><p><img src="/2018/02/19/浅层神经网络/ComputingNNoutput" alt="ComputingNNoutput"></p>
<p>对于两层神经网络，从输入层到隐藏层对应一次逻辑回归运算；从隐藏层到输出层对应一次逻辑回归运算。每层计算时，要主义对应的上标和下标，一般我们记上标方括号表示layer，下标表示第几个神经元。例如$a_i^{[I]}$表示第I层的第i个神经元，注意，i从1开始，I从0开始。</p>
<p>下面，我们将从输入层到输出层的计算公式列出来</p>
<script type="math/tex; mode=display">z_1^{[1]}=w_1^{[1]T}x+b_1^{[1]},\ a_1^{[1]}=\sigma(z_1^{[1]})</script><script type="math/tex; mode=display">z_2^{[1]}=w_2^{[1]T}x+b_2^{[1]},\ a_2^{[1]}=\sigma(z_2^{[1]})</script><script type="math/tex; mode=display">z_3^{[1]}=w_3^{[1]T}x+b_3^{[1]},\ a_3^{[1]}=\sigma(z_3^{[1]})</script><script type="math/tex; mode=display">z_4^{[1]}=w_4^{[1]T}x+b_4^{[1]},\ a_4^{[1]}=\sigma(z_4^{[1]})</script><p>然后，从隐藏层到输出层的计算公式为：</p>
<script type="math/tex; mode=display">z_1^{[2]}=w_1^{[2]T}a^{[1]}+b_1^{[2]},\ a_1^{[2]}=\sigma(z_1^{[2]})</script><p>其中$a^{[1]}$为：</p>
<script type="math/tex; mode=display">{a^{[1]}}= \left[ \begin{matrix} a_1^{[1]} \\ a_2^{[1]} \\ a_3^{[1]} \\ a_4^{[1]} \end{matrix} \right]</script><p>上述每个节点的计算都对应着一次逻辑运算的过程，分别由计算z和a两部分组成。</p>
<p>为了提高程序的运算速度，我们引入向量化和矩阵运算的思想，将上述表达式转换成矩阵运算的形式：</p>
<script type="math/tex; mode=display">z^{[1]}=W^{[1]}x+b^{[1]}</script><script type="math/tex; mode=display">a^{[1]}=\sigma(z^{[1]})</script><script type="math/tex; mode=display">z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}</script><script type="math/tex; mode=display">a^{[2]}=\sigma(z^{[2]})</script><p><img src="/2018/02/19/浅层神经网络/ComputingNNoutput2" alt="ComputingNNoutput2"><br>之前也介绍过，$W^{[1]}$的维度是(4,3),$b^{[1]}$的维度是(4,1),$W^{[2]}$的维度是(1,4),$b^{[2]}$的维度是(1,1)。</p>
<h3 id="Vectorizing-across-multiple-examples"><a href="#Vectorizing-across-multiple-examples" class="headerlink" title="Vectorizing across multiple examples"></a>Vectorizing across multiple examples</h3><p>上一部分我们介绍了单个样本的神经网络正向传播矩阵运算过程。而对于m个训练样本，我们也可以使用矩阵相乘的形式来提高计算效率。而且它的形式与上一部分单个样本的矩阵运算十分相似。</p>
<p>用上标(i)来表示第i个样本，例如$X^{(i)},Z^{(i)},a^{<a href="i">2</a>}$。对于每个样本i，可以使用for循环来求解其正向输出。</p>
<p>for i=1 to m:<br>    $z^{<a href="i">1</a>}=W^{[1]}x^{(i)}+b^{[1]}$<br>    $a^{<a href="i">1</a>}=\sigma(z^{<a href="i">1</a>})$<br>    $z^{<a href="i">2</a>}=W^{[2]}a^{<a href="i">1</a>}+b^{[2]}$<br>    $a^{<a href="i">2</a>}=\sigma(z^{<a href="i">2</a>})$</p>
<p>不使用for循环，利用矩阵运算的思想，输入矩阵X的维度为(n_x,m)。这样，我们可以把上面的for循环写成矩阵运算的形式：</p>
<script type="math/tex; mode=display">Z^{[1]}=W^{[1]}X+b^{[1]}</script><script type="math/tex; mode=display">A^{[1]}=\sigma(Z^{[1]})</script><script type="math/tex; mode=display">Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}</script><script type="math/tex; mode=display">A^{[2]}=\sigma(Z^{[2]})</script><p>其中，$Z^{[1]}$的维度是（4,m），4是隐藏层神经元的个数；$A^{[1]}$的维度与$Z^{[1]}$相同；$Z^{[2]}$和$A^{[2]}$的维度均为（1,m）。对上面这四个矩阵来说，均可以这样来理解：行表示神经元个数，列表示样本数目m。</p>
<h3 id="Explanation-for-Vectorized-Implementation"><a href="#Explanation-for-Vectorized-Implementation" class="headerlink" title="Explanation for Vectorized Implementation"></a>Explanation for Vectorized Implementation</h3><p>只要记住上述四个矩阵的行表示神经元个数，列表示样本数目m就行了。</p>
<h3 id="Activation-functions"><a href="#Activation-functions" class="headerlink" title="Activation functions"></a>Activation functions</h3><p>神经网络隐藏层和输出层都需要激活函数（activation function），在之前的课程中我们都默认使用Sigmoid函数$\sigma(X)$作为激活函数。其实，还有其它激活函数可供使用，不同的激活函数有各自的优点。下面我们就来介绍几个不同的激活函数g(x)。</p>
<p><strong>sigmoid函数</strong><br><img src="/2018/02/19/浅层神经网络/sigmoid" alt="sigmoid"></p>
<p><strong>tanh函数</strong><br><img src="/2018/02/19/浅层神经网络/tanh" alt="tanh"></p>
<p><strong>ReLU函数</strong><br><img src="/2018/02/19/浅层神经网络/ReLU" alt="ReLU"></p>
<p><strong>Leaky ReLU</strong><br><img src="/2018/02/19/浅层神经网络/LeakyReLU" alt="LeakyReLU"></p>
<p>如上图所示，不同激活函数形状不同，a的取值范围也有差异。</p>
<p>如何选择合适的激活函数呢？首先我们来比较sigmoid函数和tanh函数。对于隐藏层的激活函数，一般来说，tanh函数要比sigmoid函数表现更好一些。因为tanh函数的取值范围在[-1,+1]之间，隐藏层的输出被限定在[-1,+1]之间，可以看成是在0值附近分布，均值为0。这样从隐藏层到输出层，数据起到了归一化（均值为0）的效果。因此，隐藏层的激活函数，tanh比sigmoid更好一些。而对于输出层的激活函数，因为二分类问题的输出取值为{0,+1}，所以一般会选择sigmoid作为激活函数。</p>
<p>观察sigmoid函数和tanh函数，我们发现有这样一个问题，就是当|z|很大的时候，激活函数的斜率（梯度）很小。因此，在这个区域内，梯度下降算法会运行得比较慢。在实际应用中，应尽量避免使z落在这个区域，使|z|尽可能限定在零值附近，从而提高梯度下降算法运算速度。</p>
<p>为了弥补sigmoid函数和tanh函数的这个缺陷，就出现了ReLU激活函数。ReLU激活函数在z大于零时梯度始终为1；在z小于零时梯度始终为0；z等于零时的梯度可以当成1也可以当成0，实际应用中并不影响。对于隐藏层，选择ReLU作为激活函数能够保证z大于零时梯度始终为1，从而提高神经网络梯度下降算法运算速度。但当z小于零时，存在梯度为0的缺点，实际应用中，这个缺点影响不是很大。为了弥补这个缺点，出现了Leaky ReLU激活函数，能够保证z小于零是梯度不为0。</p>
<p>最后总结一下，如果是分类问题，输出层的激活函数一般会选择sigmoid函数。但是隐藏层的激活函数通常不会选择sigmoid函数，tanh函数的表现会比sigmoid函数好一些。实际应用中，通常会会选择使用ReLU或者Leaky ReLU函数，保证梯度下降速度不会太小。其实，具体选择哪个函数作为激活函数没有一个固定的准确的答案，应该要根据具体实际问题进行验证（validation）。</p>
<h3 id="Why-do-you-need-non-linear-activation-functions"><a href="#Why-do-you-need-non-linear-activation-functions" class="headerlink" title="Why do you need non-linear activation functions"></a>Why do you need non-linear activation functions</h3><p>为什么不能用线性激活函数</p>
<p>假设所有激活函数都是线性的，为了简化计算，我们直接令激活函数g(z)=z,即a=z。那么，浅层神经网络的各层输出为：</p>
<script type="math/tex; mode=display">z^{[1]}=W^{[1]}x+b^{[1]}</script><script type="math/tex; mode=display">a^{[1]}=z^{[1]}</script><script type="math/tex; mode=display">z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}</script><script type="math/tex; mode=display">a^{[2]}=z^{[2]}</script><p>我们对上式中$a^{[2]}$进行化简计算：</p>
<script type="math/tex; mode=display">a^{[2]}=z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}=W^{[2]}(W^{[1]}x+b^{[1]})+b^{[2]}=(W^{[2]}W^{[1]})x+(W^{[2]}b^{[1]}+b^{[2]})=W'x+b'</script><p>经过推导我们发现$a^{[2]}$仍是输入变量x的线性组合。这表明，使用神经网络与直接使用线性模型的效果并没有什么两样。即便是包含多层隐藏层的神经网络，如果使用线性函数作为激活函数，最终的输出仍然是输入x的线性模型。这样的话神经网络就没有任何作用了。因此，隐藏层的激活函数必须要是非线性的。</p>
<p>另外，如果所有的隐藏层全部使用线性激活函数，只有输出层使用非线性激活函数，那么整个神经网络的结构就类似于一个简单的逻辑回归模型，而失去了神经网络模型本身的优势和价值。</p>
<p>值得一提的是，如果是预测问题而不是分类问题，输出y是连续的情况下，输出层的激活函数可以使用线性函数。如果输出y恒为正值，则也可以使用ReLU激活函数，具体情况，具体分析。</p>
<h3 id="Derivatives-of-activation-functions"><a href="#Derivatives-of-activation-functions" class="headerlink" title="Derivatives of activation functions"></a>Derivatives of activation functions</h3><p>在梯度下降反向计算过程中少不了计算激活函数的导数即梯度。</p>
<p>我们先来看一下sigmoid函数的导数：</p>
<script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{(-z)}}</script><script type="math/tex; mode=display">g'(z)=\frac{d}{dz}g(z)=g(z)(1-g(z))=a(1-a)</script><p>tanh函数的导数：</p>
<script type="math/tex; mode=display">g(z)=\frac{e^{(z)}-e^{(-z)}}{e^{(z)}+e^{(-z)}}</script><script type="math/tex; mode=display">g'(z)=\frac{d}{dz}g(z)=1-(g(z))^2=1-a^2</script><p>ReLU函数的导数</p>
<script type="math/tex; mode=display">g(z)=max(0,z)</script><script type="math/tex; mode=display">g'(z)=\begin{cases} 0, & z<0\\ 1, & z\geq0 \end{cases}</script><p>LeakyReLU函数的导数</p>
<script type="math/tex; mode=display">g(z)=max(0.01z,z)</script><script type="math/tex; mode=display">g'(z)=\begin{cases} 0.01, & z<0\\ 1, & z\geq0 \end{cases}</script><h3 id="Gradient-descent-for-neural-networks"><a href="#Gradient-descent-for-neural-networks" class="headerlink" title="Gradient descent for neural networks"></a>Gradient descent for neural networks</h3><p>在神经网络中进行梯度计算</p>
<p>仍然是浅层神经网络，包含的参数为$W^{[1]}$，$b^{[1]}$，$W^{[2]}$，$b^{[2]}$。令输入层的特征向量个数$n_x=n^{[0]}$，隐藏层神经元个数为$n^{[1]}$，输出层神经元个数为$n^{[2]}=1$。则$W^{[1]}$的维度为$(n^{[1]},n^{[0]})$，$b^{[1]}$的维度为$(n^{[1]},1)$，$W^{[2]}$的维度为$(n^{[2]},n^{[1]})$，b[2]的维度为$(n^{[2]},1)$。</p>
<p>该神经网络正向传播过程为：</p>
<script type="math/tex; mode=display">Z^{[1]}=W^{[1]}X+b^{[1]}</script><script type="math/tex; mode=display">A^{[1]}=g(Z^{[1]})</script><script type="math/tex; mode=display">Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}</script><script type="math/tex; mode=display">A^{[2]}=g(Z^{[2]})</script><p>其中，g(⋅)表示激活函数。</p>
<p>反向传播是计算导数（梯度）的过程，这里先列出来Cost function对各个参数的梯度：</p>
<script type="math/tex; mode=display">dZ^{[2]}=A^{[2]}-Y</script><script type="math/tex; mode=display">dW^{[2]}=\frac1mdZ^{[2]}A^{[1]T}</script><script type="math/tex; mode=display">db^{[2]}=\frac1mnp.sum(dZ^{[2]},axis=1,keepdim=True)</script><script type="math/tex; mode=display">dZ^{[1]}=W^{[2]T}dZ^{[2]}\ast g'(Z^{[1]})</script><script type="math/tex; mode=display">dW^{[1]}=\frac1mdZ^{[1]}X^T</script><p>$db^{[1]}=\frac1mnp.sum(dZ^{[1]},axis=1,keepdim=True)$$</p>
<p>反向传播的具体推导过程下一部分进行详细说明</p>
<h3 id="Backpropagation-intuition-optional"><a href="#Backpropagation-intuition-optional" class="headerlink" title="Backpropagation intuition(optional)"></a>Backpropagation intuition(optional)</h3><p>我们仍然使用计算图的方式来推导神经网络反向传播过程。记得之前介绍逻辑回归时，我们就引入了计算图来推导正向传播和反向传播，其过程如下图所示：<br><img src="/2018/02/19/浅层神经网络/logisticregressiongradients" alt="logisticregressiongradients"></p>
<p>由于多了一个隐藏层，神经网络的计算图要比逻辑回归的复杂一些，如下图所示。对于单个训练样本，正向过程很容易，反向过程可以根据梯度计算方法逐一推导。</p>
<script type="math/tex; mode=display">dz^{[2]}=a^{[2]}-y</script><script type="math/tex; mode=display">dW^{[2]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial W^{[2]}}=dz^{[2]}a^{[1]T}</script><script type="math/tex; mode=display">db^{[2]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial b^{[2]}}=dz^{[2]}\cdot 1=dz^{[2]}</script><script type="math/tex; mode=display">dz^{[1]}=dz^{[2]}\cdot \frac{\partial z^{[2]}}{\partial a^{[1]}}\cdot \frac{\partial a^{[1]}}{\partial z^{[1]}}=W^{[2]T}dz^{[2]}\ast g^{[1]'}(z^{[1]})</script><script type="math/tex; mode=display">dW^{[1]}=dz^{[1]}\cdot \frac{\partial z^{[1]}}{\partial W^{[1]}}=dz^{[1]}x^T</script><script type="math/tex; mode=display">db^{[1]}=dz^{[1]}\cdot \frac{\partial z^{[1]}}{\partial b^{[1]}}=dz^{[1]}\cdot 1=dz^{[1]}</script><p><img src="/2018/02/19/浅层神经网络/neuralnetworkgradients" alt="neuralnetworkgradients"><br>浅层神经网络（包含一个隐藏层），m个训练样本的正向传播过程和反向传播过程分别包含了6个表达式，其向量化矩阵形式如下图所示：<br><img src="/2018/02/19/浅层神经网络/summary" alt="summary"></p>
<h3 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h3><p>神经网络模型中的参数权重W是不能全部初始化为零的，接下来我们分析一下原因。</p>
<p>举个简单的例子，一个浅层神经网络包含两个输入，隐藏层包含两个神经元。如果权重$W^{[1]}$和$W^{[2]}$都初始化为零，即：</p>
<script type="math/tex; mode=display">W^{[1]}= \left[ \begin{matrix} 0 & 0 \\ 0 & 0 \end{matrix} \right]</script><script type="math/tex; mode=display">W^{[2]}= \left[ \begin{matrix} 0 & 0 \end{matrix} \right]</script><p>这样使得隐藏层第一个神经元的输出等于第二个神经元的输出，即$a_1^{[1]}=a_2^{[1]}$。经过推导得到$dz_1^{[1]}=dz_2^{[1]}$，以及$W_1^{[1]}=dW_2^{[1]}$。因此，这样的结果是隐藏层两个神经元对应的权重行向量$W_1^{[1]}$和$W_2^{[1]}$每次迭代更新都会得到完全相同的结果，$W_1^{[1]}$始终等于$W_2^{[1]}$，完全对称。这样隐藏层设置多个神经元就没有任何意义了。值得一提的是，参数b可以全部初始化为零，并不会影响神经网络训练效果。<br><img src="/2018/02/19/浅层神经网络/initialization" alt="initialization"></p>
<p>我们把这种权重W全部初始化为零带来的问题称为symmetry breaking problem。解决方法也很简单，就是将W进行随机初始化（b可初始化为零）。python里可以使用如下语句进行W和b的初始化：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W_1 = np.random.randn((2,2))*0.01</span><br><span class="line">b_1 = np.zero((2,1))</span><br><span class="line">W_2 = np.random.randn((1,2))*0.01</span><br><span class="line">b_2 = 0</span><br></pre></td></tr></table></figure></p>
<p>这里我们将$W_1^{[1]}$和$W_2^{[1]}$乘以0.01的目的是尽量使得权重W初始化比较小的值。之所以让W比较小，是因为如果使用sigmoid函数或者tanh函数作为激活函数的话，W比较小，得到的|z|也比较小（靠近零点），而零点区域的梯度比较大，这样能大大提高梯度下降算法的更新速度，尽快找到全局最优解。如果W较大，得到的|z|也比较大，附近曲线平缓，梯度较小，训练过程会慢很多。</p>
<p>当然，如果激活函数是ReLU或者Leaky ReLU函数，则不需要考虑这个问题。但是，如果输出层是sigmoid函数，则对应的权重W最好初始化到比较小的值。</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了浅层神经网络。首先，我们简单概述了神经网络的结构：包括输入层，隐藏层和输出层。然后，我们以计算图的方式推导了神经网络的正向输出，并以向量化的形式归纳出来。接着，介绍了不同的激活函数并做了比较，实际应用中根据不同需要选择合适的激活函数。激活函数必须是非线性的，不然神经网络模型起不了任何作用。然后，我们重点介绍了神经网络的反向传播过程以及各个参数的导数推导，并以矩阵形式表示出来。最后，介绍了权重随机初始化的重要性，必须对权重W进行随机初始化操作。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2018/02/19/神经网络基础之Python与向量化/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/02/19/神经网络基础之Python与向量化/" itemprop="url">
                  神经网络基础之Python与向量化
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-19T03:31:26+08:00">
                2018-02-19
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/02/19/神经网络基础之Python与向量化/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/02/19/神经网络基础之Python与向量化/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Vectorization"><a href="#Vectorization" class="headerlink" title="Vectorization"></a>Vectorization</h3><p>深度学习算法中，数据量很大，在程序中尽量减少使用loop循环语句，而可以使用向量运算来提高程序运行速度。</p>
<p>向量化(Vectorization)就是利用矩阵运算的思想，大大提高运算速度。例如下面所示在Python中使用向量化要比使用循环计算速度快得多。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">a = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">b = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line"></span><br><span class="line">tic = time.time()</span><br><span class="line">c = np.dot(a,b)</span><br><span class="line">toc = time.time()</span><br><span class="line"></span><br><span class="line">print(c)</span><br><span class="line">print(<span class="string">"Vectorized version:"</span> + str(<span class="number">1000</span>*(toc-tic)) + <span class="string">"ms"</span>)</span><br><span class="line"></span><br><span class="line">c = <span class="number">0</span></span><br><span class="line">tic = time.time()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000000</span>):</span><br><span class="line">    c += a[i]*b[i]</span><br><span class="line">toc = time.time()</span><br><span class="line"></span><br><span class="line">print(c)</span><br><span class="line">print(<span class="string">"for loop:"</span> + str(<span class="number">1000</span>*(toc-tic)) + <span class="string">"ms"</span>)</span><br></pre></td></tr></table></figure>
<p>输出结果类似于：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">250286.989866</span><br><span class="line">Vectorized version:1.5027523040771484ms</span><br><span class="line">250286.989866</span><br><span class="line">For loop:474.29513931274414ms</span><br></pre></td></tr></table></figure></p>
<p>从程序运行结果上来看，该例子使用for循环运行时间是使用向量运算运行时间的约300倍。因此，深度学习算法中，使用向量化矩阵运算的效率要高得多。</p>
<p>为了加快深度学习神经网络运算速度，可以使用比CPU运算能力更强大的GPU。事实上，GPU和CPU都有并行指令(parallelization instructions),称为Single Instruction Multiple Data(SIMD)。SIMD是单指令多数据流，能够复制多个操作数，并把它们打包在大型寄存器的一组指令集。SIMD能够大大提高程序运行速度，例如python的numpy库中的内建函数(build-in function) 就是使用了SIMD指令。相比而言，GPU的SIMD要比CPU更强大一些。</p>
<h3 id="More-Vectorization-Examples"><a href="#More-Vectorization-Examples" class="headerlink" title="More Vectorization Examples"></a>More Vectorization Examples</h3><p>尽量避免使用for循环而使用向量化矩阵运算。在python的numpy库中，我们通常使用np.dot()函数来进行矩阵运算。</p>
<p>我们将向量化的思想使用在逻辑回归算法，尽可能减少for循环，而只使用矩阵运算。值得注意的是，算法最顶层的迭代训练的for循环是不能替换的。而每次迭代过程对J，dw，b的计算是可以直接使用矩阵运算。</p>
<h3 id="Vectorizing-Logistic-Regression"><a href="#Vectorizing-Logistic-Regression" class="headerlink" title="Vectorizing Logistic Regression"></a>Vectorizing Logistic Regression</h3><p>整个训练样本构成的输入矩阵X的维度是$(n_X,1)$,b是一个常数值，而整个训练忘本构成的输出矩阵Y的维度是(1,m)。利用向量化的思想，所有m个样本的线性输出Z可以用矩阵表示：</p>
<script type="math/tex; mode=display">Z=w^TX+b</script><p>在python的numpy库中可以表示为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Z = np.dot(w.T,X) + b</span><br><span class="line">A = sigmoid(Z)</span><br></pre></td></tr></table></figure></p>
<p>其中，w，T表示w的转置</p>
<p>这样，我们就能够使用向量化矩阵运算代替for循环，对所有m个样本同时运算，大大提高了运算速度。</p>
<h3 id="Vectorizing-Logistic-Regression’s-Gradient-Output"><a href="#Vectorizing-Logistic-Regression’s-Gradient-Output" class="headerlink" title="Vectorizing Logistic Regression’s Gradient Output"></a>Vectorizing Logistic Regression’s Gradient Output</h3><p>再来看逻辑回归中的梯度下降算法如何转化为向量化的矩阵形式。对于所有m个样本，db可表示为：</p>
<script type="math/tex; mode=display">dZ=A-Y</script><p>db可表示为：</p>
<script type="math/tex; mode=display">db=\frac{1}{m}\sum\limits_{i=1}^mdz^{(i)}</script><p>对应的程序为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db=1/m*np.sum(dZ)</span><br></pre></td></tr></table></figure></p>
<p>dw课表示为：</p>
<script type="math/tex; mode=display">dw=\frac{1}{m}X\cdot dZ^{T}</script><p>对应的程序为<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dw=1/m*np.dot(X,dZ,T)</span><br></pre></td></tr></table></figure></p>
<p>这样，我们把整个逻辑回归中的for循环尽可能用矩阵运算代替，对于单次迭代，梯度下降算法流程如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Z=np.dot(w.T,X)+b</span><br><span class="line">A= sigmoid(Z)</span><br><span class="line">dZ= A-Y</span><br><span class="line">dw = 1/m*np.dot(X,dZ.T)</span><br><span class="line">db = 1/m*np.sum(dZ)</span><br><span class="line"></span><br><span class="line">w = w - alpha*dw</span><br><span class="line">b = b - alpha*db</span><br></pre></td></tr></table></figure></p>
<p>其中，alpha是学习因子，决定w和b的更新速度。上述代码只是单次训练更新而言的，外层还需要一个for循环，代表迭代次数。</p>
<h3 id="Broadcasting-in-Python"><a href="#Broadcasting-in-Python" class="headerlink" title="Broadcasting in Python"></a>Broadcasting in Python</h3><p>下面介绍使用python的另一种技巧：广播(Broadcasting).python中的广播机制可以由下面四条表示：<br><strong>·让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分都通过在前面加1补齐</strong><br><strong>·输出数组的shape是输入数组shape的各轴上的最大值</strong><br><strong>·如果输入数组的某个轴和输出数组的对应轴的长度相同或者其长度为1时，这个数组能够用来计算，否则出错</strong><br><strong>·当输入数组的某个轴的长度为1时，沿着此轴运算时都用此轴上的第一组值</strong><br>简而言之，就是用python中可以对不同维度的矩阵进行四则混合运算，但至少保证有一个维度是相同的。下面给出几个广播的例子，具体细节可参阅python的相关手册，这里就不赘述了。</p>
<script type="math/tex; mode=display">Broadcasting examples</script><script type="math/tex; mode=display">\begin{bmatrix} 1\\ 2\\ 3\\ 4 \end{bmatrix} + 100=\begin{bmatrix} 101\\ 102\\ 103\\ 104 \end{bmatrix}</script><script type="math/tex; mode=display">\begin{bmatrix} 1&2&3\\ 4&5&6 \end{bmatrix} + \begin{bmatrix} 100&200&300\end{bmatrix} = \begin{bmatrix} 101&202&303\\ 104&205&306 \end{bmatrix}</script><script type="math/tex; mode=display">\begin{bmatrix}1&2&3\\4&5&6\end{bmatrix} + \begin{bmatrix} 100\\200\end{bmatrix} = \begin{bmatrix} 101&102&103\\ 204&205&206 \end{bmatrix}</script><p>在python程序中为了保证矩阵运算正确，可以使用reshape()函数来对矩阵设定所需的维度。这是一个很好且有用的习惯。</p>
<h3 id="A-note-on-python-numpy-vectors"><a href="#A-note-on-python-numpy-vectors" class="headerlink" title="A note on python/numpy vectors"></a>A note on python/numpy vectors</h3><p>总结一些python的小技巧，避免不必要的code bug<br>python中，如果我们用下列语句来定义一个变量：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">5</span>)</span><br></pre></td></tr></table></figure></p>
<p>这条语句生成的a维度是(5, )。它既不是行向量也不是列向量，我们把a叫做rank 1 array。这种定义会带来一些问题。例如我们对a进行转置，还会得到a本身。所以，如果我们要定义(5,1)的列向量，最好使以下标准语句，避免使用rank 1 array。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">b = np.random.randn(<span class="number">1</span>,<span class="number">5</span>)</span><br></pre></td></tr></table></figure></p>
<p>除此之外，我们还可以使用assert语句对向量或者数组的维度进行判断，例如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">assert</span>(a.shape == (<span class="number">5</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p>
<p>assert语句会对内嵌语句进行判断，即判断a的维度是不是(5,1)的。如果不是，则程序在此处停止。使用assert语句也是一种很好的习惯，能够帮我们及时检查、发现语句是否正确。<br>另外，还可以使用reshape函数对数组设定所需的维度：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a.shape((<span class="number">5</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p>
<h3 id="Quick-tour-of-Jupyter-iPython-Notebooks"><a href="#Quick-tour-of-Jupyter-iPython-Notebooks" class="headerlink" title="Quick tour of Jupyter/iPython Notebooks"></a>Quick tour of Jupyter/iPython Notebooks</h3><p>Jupyter notebook是一个交互笔记本，支持运行40中编程语言，本课程所有的编程练习题都将在Jupyter notebook上进行，使用语言是python。</p>
<h3 id="Explanation-of-logistic-regression-cost-function-optional"><a href="#Explanation-of-logistic-regression-cost-function-optional" class="headerlink" title="Explanation of logistic regression cost function(optional)"></a>Explanation of logistic regression cost function(optional)</h3><p>接下来简要介绍逻辑回归的Cost function是怎么来的<br>首先，预测输出$\hat{y}$的表达式可以写成:</p>
<script type="math/tex; mode=display">\hat{y}=\sigma(w^Tx+b)</script><p>其中，$\sigma(z)=\frac{1}{1+exp(-z)}$。$\hat{y}$可以看成是预测输出为正类(+1)的概率：</p>
<script type="math/tex; mode=display">\hat{y}=P(y=1|X)</script><p>那么，当y=1时：</p>
<script type="math/tex; mode=display">p(y|x)= \hat{y}</script><p>当y= 0时：</p>
<script type="math/tex; mode=display">p(y|x)=\hat{y}^y(1-\hat{y})^{(1-y)}</script><p>我们把上面两个式子整合到一个式子中，得到：</p>
<script type="math/tex; mode=display">P(y|x)=\hat{y}^y(1-\hat{y})^{(1-y)}</script><p>由于log函数的单调性，可以对上式P(y|x)进行log处理</p>
<script type="math/tex; mode=display">logP(y|x)=log\hat{y}^y(1-\hat{y})^{(1-y)}=ylog\hat{y}+(1-y)log(1-\hat{y})</script><p>我们希望上述概率P(y|x)越大越好，对上式加上负号，则转化成额单个样本的Loss function，越小越好，也iu得到了我们之前介绍的逻辑回归的Loss function形式</p>
<script type="math/tex; mode=display">L=-(ylog\hat{y}+(1-y)log(1-\hat{y}))</script><p>如果对于所有m个训练样本，假设样本之间是独立同分布的(iid),我们希望总的概率越大越好：</p>
<script type="math/tex; mode=display">max\ \prod_{i=1}^m\ P(y^{(i)}|x^{(i)})</script><p>同样引入log函数，加上负号，将上式转化为Cost function：</p>
<script type="math/tex; mode=display">J(w,b)=-\frac1m\sum_{i=1}^mL(\hat y^{(i)},y^{(i)})=-\frac 1m\sum_{i=1}^my^{(i)}\ log\ \hat y^{(i)}+(1-y^{(i)})log(1-\hat y^{(i)})</script><p>上式中，$\frac{1}{m}$表示对所有m个样本的Cost function求平均，是缩放因子。</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课我们主要介绍了神经网络基础————python和向量话。在深度学习程序中，使用向量化和矩阵运算的方法能够大大提高运行速度，节省时间。以逻辑回归威力，我们将算法流程包括梯度下降转化为向量化的形式，同时，我们也介绍了python的相关编程方法和技巧。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2018/01/26/神经网络基础之逻辑回归/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/01/26/神经网络基础之逻辑回归/" itemprop="url">
                  神经网络基础之逻辑回归
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-26T17:11:36+08:00">
                2018-01-26
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/01/26/神经网络基础之逻辑回归/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/01/26/神经网络基础之逻辑回归/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>附：从本章开始常用到数学符号和公式，这里贴出在md中写公式的方法，<a href="http://blog.csdn.net/zryxh1/article/details/53161011" target="_blank" rel="noopener">md语法|LaTex数学公式</a>。遇到不知道的符号可以去在线编辑器选择查看代码 <a href="http://www.codecogs.com/latex/eqneditor.php" target="_blank" rel="noopener">Latex在线编辑器</a></strong></p>
<h3 id="Binary-Classification"><a href="#Binary-Classification" class="headerlink" title="Binary Classification"></a>Binary Classification</h3><p>逻辑回归模型一般是用来解决二分类(Binary Classification)问题。二分类就是输出y只有{0，1}两个离散值(也有{1,-1}的情况)。<br>我们以一个图像识别问题为例，判断图片中是否有猫的存在，0代表no cat，1代表cat，如下图所示。<br><img src="/2018/01/26/神经网络基础之逻辑回归/binaryclassification.png" alt="binaryclassification"><br>主要通过这个例子简要介绍神经网络模型中的一些标准化的、有效率的处理方法和notations。<br><img src="/2018/01/26/神经网络基础之逻辑回归/binaryclassification2.png" alt="binaryclassification2"><br>如上图所示，这是一个典型的二分类问题。一般来说，彩色图片包含RGB三个通道。例如该cat的图片尺寸为(64,64,3)。在神经网络模型中，我们首先要将图片输入x转化为一维的特征向量(feature vector)。方法是每个通道一行一行取，再连接起来。由于$64\times 64\times 3= 12288$，转化后的输入特征向量维度为(12288,1)。此特征向量x是列向量，维度一般记为$n_x$。<br>如果训练样本共有m张图片，那么整个训练样本X组成了矩阵，维度是$(n_x,1)$。注意，这里的矩阵X的行$n_x$代表了每个样本$x^{(i)}$特征个数，列m代表了样本个数。这里Andrew解释了X的维度之所以是$(n_x,m)$而不是$(m,n_x)$是为了之后矩阵运算的方便。而所有的训练样本的输出Y也组成了一维的行向量，写成矩阵的形式后，它的维度就是$(1,m)$。</p>
<h3 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h3><p>如何使用逻辑回归来解决二分类问题。逻辑回归中，预测值$\hat{h}=P(y=1|x)$表示为1的概率，取值范围在[0,1]之间。这是其与二分类模型不同的地方。使用线性模型，引入参数w和b。权重w的维度是$(n_x,1)$,b是一个常数项。这样，逻辑回归的线性预测输出可以写成：</p>
<script type="math/tex; mode=display">\hat{h}=w^{T}x+b</script><p>值得注意的是，很多其他机器学习资料中，可能吧常数b当作$w_0$处理，并引入$x_0=1$,这样从维度上来看，x和w都会增加一维，但在本课程中，为了简化计算和便于理解，Andrew建议还是用上式这种形式将w和b分开比较好。<br>上式的线性输出区间为整个实数范围，而逻辑回归要求输出范围在[0,1]之间，所以还需要对上式的线性函数输出进行处理，方法是引入Sigmoid函数，让输出限定在[0,1]之间。这样，逻辑回归的预测输出就可以完整写成：</p>
<script type="math/tex; mode=display">\hat{y}=Sigmoid(w^{T}x+b)=\sigma(w^{T}+b)</script><p>Sigmoid函数是一种非线性的S型函数，输出被限定在[0,1]之间，通常被用在神经网络中当作激活函数(Activation function)使用。Sigmoid函数的表达式和曲线如下：</p>
<script type="math/tex; mode=display">Sigmoid(z)=\frac{1}{1+e^{-z}}</script><p><img src="/2018/01/26/神经网络基础之逻辑回归/Sigmoid" alt="sigmoid"><br>从Sigmoid函数可以看出，当z值很大时，函数值趋向于1；当z值很小时，函数值趋向于0.且当z=0时，函数值为0.5。Sigmoid函数的一阶导数可以用其自身表示：</p>
<script type="math/tex; mode=display">\sigma(z)=\sigma(z)(1-\sigma(z))</script><p>这样，通过Sigmoid函数，就能够将逻辑回归的输出限定在[0,1]之间了。</p>
<h3 id="Logistic-Regression-Cost-Function"><a href="#Logistic-Regression-Cost-Function" class="headerlink" title="Logistic Regression Cost Function"></a>Logistic Regression Cost Function</h3><p>逻辑回归中，w和b都是未知参数，需要反复训练和优化得到。因此，我们需要定义一个cost function，包含了参数w和b。通过优化cost function ，当cost function取值最小时，得到对应的w和b</p>
<p>对于m个训练样本，我们通常使用上标来表示对应的样本，例如$(x^{(i)},y^{(i)})$来表示第i个样本。</p>
<p>如何定义所有m个样本的cost function呢，先从单个样本出发，我们希望该样本的预测值$\hat{y}$与真实值越相似越好。我们把单个样本的cost function用Loss function来表示，根据以往经验，如果使用平方误差(squared error)来衡量，如下所示：</p>
<script type="math/tex; mode=display">L(\hat{y},y) = \frac{1}{2}(\hat{y}-y)^{2}</script><p>但是对于逻辑回归，我们一般不用平方误差来衡量，因为这种Loss function一般是non-convex(非凸性)的。non-convex函数在使用梯度下降法时，容易得到局部最小值(local minmum),即局部最优化。而我们的目标是计算得到全局最优化(Global optimization).因此，我们一般选择的Loss function应该是convex的。</p>
<p>Loss function的原则和目的是衡量预测输出$\hat(y)$与真实样本输出y的接近程度。平方误差其实也可以，只是它是non-convex的，不利于使用梯度下降法来进行全局优化。因此，我们可以构建另外一种Loss function，且是convex的，如下所示：</p>
<script type="math/tex; mode=display">L(\hat{y},y)=-(ylog\hat{y}+(1-y)log(1-\hat{y}))</script><p>这个Loss function。它是衡量误差大小的，Loss function越小越好。</p>
<p>当y=1时，$L(\hat{y},y)=-log\hat{y}$,如果$\hat{y}$越接近1，$L(\hat{y},y)\approx0$,表示预测效果越好；如果$\hat{y}$越接近0，$L(\hat{y},y)\approx+\infty$,表示预测效果越差。这正是我们希望Loss function所实现的功能。</p>
<p>当y=0时，$L(\hat{y},y)=-log(1-\hat{y})$.如果$\hat{y}$越接近于0，$L(\hat{y},y)\approx0$,表示预测效果越好；如果$\hat{y}$越接近于1，$L(\hat{y},y)\approx+\infty$,表示预测效果越差。这也正是我们希望Loss function所实现的功能。</p>
<p>因此，这个Loss function能够很好地反映预测输出$\hat{y}$与真实样本输出y的接近程度，越接近的话，其Loss function的值越小。而且这个函数是convex的。上面我们只是简要分析为什么要使用这个Loss function后面的课程将详细推导这个Loss function是如何得到的</p>
<p>Loss function是针对单个样本的，对于m个样本，我们定义Cost function。Cost function是m的样本的Loss function的平均值，反映了m个样本的预测输出$\hat{y}$与真实样本输出y的平均接近程度。Cost function可表示为：</p>
<script type="math/tex; mode=display">J(w,b)=\frac{1}{m}\sum\limits_{k=1}^m[y^{(i)}log\hat{y}^{(i)}+(1-y^{(i)})log(1-\hat{y}^{(i)})]</script><p>Cost function已经推导出来了，Cost function是关于待求系数w和b的函数。我们的目标是迭代计算出最佳的w和b值，最小化Cost function，让Cost function尽可能接近于零。</p>
<p>其实逻辑回归问题可以看成是一个简单的神经网络，只包含一个神经元。这也是我们先介绍逻辑回归的原因。</p>
<h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><p>我们已经掌握Cost function的表达式，接下来将使用梯度下降(Gradient Descent)算法计算出合适的w和b值，从而最小化m个训练样本的Cost function，即J(w,b)。</p>
<p>由于J(w,b)是convex function，梯度下降算法是先随机选择一组参数w和b值，然后每次迭代的过程中分别沿着w和b的梯度(偏导数)的反方向前进一小步，不断修正w和b。每次迭代更新w和b后，都能让J(w,b)更接近全局最小值。梯度下降的过程如下图所示<br><img src="/2018/01/26/神经网络基础之逻辑回归/GradientDescent" alt="GradientDescent"><br>梯度下降算法每次迭代更新，w和b的修正表达式为</p>
<script type="math/tex; mode=display">w:=w-\alpha\frac{\partial J(w,b)}{\partial w}</script><script type="math/tex; mode=display">b:=b-\alpha\frac{\partial J(w,b)}{\partial b}</script><p>上式中，$\alpha$是学习因子(learning rate),表示梯度下降的步进长度。$\alpha$越大，w和b每次更新的“步伐”更大一些；$\alpha$越小，w和b每次更新的“步伐”更小一些。在程序代码中，我们通常使用dw来表示$frac{\partial J(w,b)}{\partial w}$，用db来表示$\frac{\partial J(w,b)}{\partial b}$。微积分里，$\frac{df}{dx}$表示对单一变量求导数，$\frac{\partial f}{\partial x}$表示对多个变量中的某个变量求偏导数。</p>
<p>梯度下降算法能够保证每次迭代w和b都能向着J(w,b)全局最小化的方向进行，其数学原理主要是运用泰勒一阶展开来证明的</p>
<h3 id="Derivatives"><a href="#Derivatives" class="headerlink" title="Derivatives"></a>Derivatives</h3><p>这一部分内容主要是Andrew对微积分、求导数进行介绍。梯度或者导数一定程度上可以看成斜率，这里不再赘述。</p>
<h3 id="More-Derivatives-Examples"><a href="#More-Derivatives-Examples" class="headerlink" title="More Derivatives Examples"></a>More Derivatives Examples</h3><p>Andrew给出更复杂求导数的例子。</p>
<h3 id="Computation-graph"><a href="#Computation-graph" class="headerlink" title="Computation graph"></a>Computation graph</h3><p>整个神经网络的训练过程实际上包含了两个过程：正向传播(Forward Propagation)和反向传播(Back Propagation)。正向传播是从输入到输出，由神经网络计算的到预测输出的过程，反向传播是从输出到输入，对参数w和b计算梯度的过程。下面，我们用计算图(Computation graph)的形式来理解这两个过程。</p>
<p>举个简单的例子，假如Cost function为J(a,b,c)=3(a+bc),包含啊a,b,c三个变量。我们用u表示bc，v表示a+u,则J=3v。它的计算图可以写成如下图所示：<br><img src="/2018/01/26/神经网络基础之逻辑回归/Computationgraph" alt="Computationgraph"><br>令a=5,b=3,c=2,则u=bc=6，v=a+u=11，J=3v=33。计算图中，这种从左到右，从输入到输出的过程就对应着神经网络或者逻辑回归中输入与权重经过运算计算得到Cost function的正向过程。</p>
<h3 id="Derivatives-with-a-Computation-Graph"><a href="#Derivatives-with-a-Computation-Graph" class="headerlink" title="Derivatives with a Computation Graph"></a>Derivatives with a Computation Graph</h3><p>上一部分介绍的是计算图的正向传播(Forward Propagation),下面我们来介绍其反向传播(Back Propagation),即计算输出对输入的偏导数。</p>
<p>还是上个计算图的例子，输入参数有3个，分别是a，b，c。</p>
<p>首先计算J对参数a的偏导数。从计算图上来看，从右到左，J是v的函数，v是a的函数，则利用求导技巧，可以得到：</p>
<script type="math/tex; mode=display">\frac{\partial J}{\partial a}=\frac{\partial J}{\partial v}\cdot\frac{\partial v}{\partial a}=3\cdot1=3</script><p>根据这种思想，然后计算J对参数b的偏导数。从计算图上来看，从右到左，J是v的函数，v是u的函数，u是b的函数。可以推导：</p>
<script type="math/tex; mode=display">\frac{\partial J}{\partial b}=\frac{\partial J}{\partial v}\cdot\frac{\partial v}{\partial u}\cdot\frac{\partial u}{\partial b}=3\cdot 1\cdot c=3\cdot 1\cdot 2=6</script><p>最后计算J对参数c的偏导数。仍从计算图上来看，从右到左，J是v的函数，v是u的函数，u是c的函数。可以推导：</p>
<script type="math/tex; mode=display">\frac{\partial J}{\partial c}=\frac{\partial J}{\partial v}\cdot\frac{\partial v}{\partial u}\cdot\frac{\partial u}{\partial c}=3\cdot 1\cdot b=3\cdot 1\cdot 3=9</script><p>为了同一格式，在程序代码中，我们使用da，db，dc来表示J对参数a，b，c的偏导数。<br><img src="/2018/01/26/神经网络基础之逻辑回归/Computationgraph2" alt="Computationgraph2"></p>
<h3 id="Logistic-Regression-Gradient-Descent"><a href="#Logistic-Regression-Gradient-Descent" class="headerlink" title="Logistic Regression Gradient Descent"></a>Logistic Regression Gradient Descent</h3><p>现在，我们将对逻辑回归进行梯度计算。对单个样本而言，逻辑回归Loss function表达式如下：</p>
<script type="math/tex; mode=display">z=w^{T}x+b</script><script type="math/tex; mode=display">\hat{y}=a=\sigma(z)</script><script type="math/tex; mode=display">L(a,y)=-(ylog(a)+(1-y)log(1-a))</script><p>首先，该逻辑回归的正向传播过程非常简单。根据上述公式，例如输入样本x有两个特征(x1,x2),相应的权重w维度也是2，即(w1,w2)。则$z=w_1x_1+w_2x_2+b$,最后的Loss function如下所示：<br><img src="/2018/01/26/神经网络基础之逻辑回归/Logisticregression" alt="Logisticregression"><br>然后，计算该逻辑回归的反向传播过程，即由Loss function计算参数w和b的偏导数。推导过程如下：</p>
<script type="math/tex; mode=display">da=\frac{\partial L}{\partial a}= -\frac{y}{a}+\frac{1-y}{1-a}</script><script type="math/tex; mode=display">dz=\frac{\partial L}{\partial z}=\frac{\partial L}{\partial a}\cdot\frac{\partial a}{\partial z}= (-\frac{y}{a}+\frac{1-y}{1-a})\cdot a(1-a)= a-y</script><p>知道了dz之后，就可以直接对$W_1,W_2$和b进行求导了。</p>
<script type="math/tex; mode=display">dw_1=\frac{\partial L}{\partial w_1}=\frac{\partial L}{\partial z}\cdot\frac{\partial z}{\partial w_1}=x_1\cdot dz=x_1(a-y)</script><script type="math/tex; mode=display">dw_2=\frac{\partial L}{\partial w_2}=\frac{\partial L}{\partial z}\cdot\frac{\partial z}{\partial w_2}=x_2\cdot dz=x_2(a-y)</script><script type="math/tex; mode=display">db=\frac{\partial L}{\partial b}=\frac{\partial L}{\partial z}\cdot\frac{\partial z}{\partial b}=1\cdot dz=a-y</script><p>则梯度下降算法可以表示为：</p>
<script type="math/tex; mode=display">w_1:=w_1-\alpha dw_1</script><script type="math/tex; mode=display">w_2:=w_2-\alpha dw_2</script><script type="math/tex; mode=display">b:=b-\alpha db</script><p><img src="/2018/01/26/神经网络基础之逻辑回归/Logisticregression2" alt="Logisticregression2"></p>
<h3 id="Gradient-Descent-on-m-examples"><a href="#Gradient-Descent-on-m-examples" class="headerlink" title="Gradient Descent on m examples"></a>Gradient Descent on m examples</h3><p>上一部分讲的是对单个样本求偏导和梯度下降。如果有m个样本，其Cost function 表达式如下：</p>
<script type="math/tex; mode=display">z^{(i)}=w^{T}x^{(i)}+b</script><script type="math/tex; mode=display">\hat{y}^{(i)}=a^{(i)}=\sigma(z^{(i)})</script><script type="math/tex; mode=display">J(w,b)=\frac{1}{m}\sum\limits_{i=1}^mL(\hat{y}^{(i)},y^{(i)})=-\frac{1}{m}\sum\limits_{i=1}^m[y^{(i)}log\hat{y}^{(i)}+(1-y^{(i)})log(1-\hat{y}^{(i)})]</script><p>Cost function关于w和b的偏导数可以写成和平均的形式</p>
<script type="math/tex; mode=display">dw_1=\frac{1}{m}\sum\limits_{i=1}^mx_1^{(i)}(a^{(i)}-y^{(i)})</script><script type="math/tex; mode=display">dw_2=\frac{1}{m}\sum\limits_{i=1}^mx_2^{(i)}(a^{(i)}-y^{(i)})</script><script type="math/tex; mode=display">db=\frac{1}{m}\sum\limits_{i=1}^m(a^{(i)}-y^{(i)})</script><p>这次，每次迭代中w和b的梯度有m个训练样本计算平均值得到。其算法流程图如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">J=0;dw1=0; dw2=0;db=0;</span><br><span class="line">for i = 1 to m</span><br><span class="line">    z(i) = wx(i)+b;</span><br><span class="line">    a(i) = sigmoid(z(i));</span><br><span class="line">    J += -[y(i)log(a(i))+(1-y(i)）log(1-a(i));</span><br><span class="line">    dz(i) = a(i)-y(i);</span><br><span class="line">    dw1 += x1(i)dz(i);</span><br><span class="line">    dw2 += x2(i)dz(i);</span><br><span class="line">    db += dz(i);</span><br><span class="line">J /= m;</span><br><span class="line">dw1 /= m;</span><br><span class="line">dw2 /= m;</span><br><span class="line">db /= m;</span><br></pre></td></tr></table></figure></p>
<p>这样经过n次迭代后，整个梯度下降算法就完成了。</p>
<p>在上述的梯度下降算法中，我们是利用for循环对每个样本进行dw1，dw2和db的累加计算最后再求平均数的。在深度学习中，样本数量m通常很大，使用for循环会让神经网络程序运行得很慢。所以，我们应该尽量避免使用for循环操作，而使用矩阵运算，能够大大提高程序运行速度。</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p>本节课主要介绍了神经网络的基础——逻辑回归。首先，我们介绍了二分类问题，以图片为例，将多维输入x转化为feature vector，输出y只有{0，1}两个离散值。介绍了逻辑回归及其对应的Cost function 形式，然后介绍了梯度下降算法，并使用计算图的方式来讲述神经网络的正向传播和反向传播两个过程，总结出最优化参数w和b的算法流程。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2018/01/17/深度学习概论/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/01/17/深度学习概论/" itemprop="url">
                  深度学习概论
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-17T16:22:22+08:00">
                2018-01-17
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/01/17/深度学习概论/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2018/01/17/深度学习概论/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>由于毕业设计选择了深度学习相关的选题，我便开始学习深度学习相关的课程，使用的课程资源是吴恩达的《神经网络与深度学习》,<a href="http://mooc.study.163.com/smartSpec/detail/1001319001.htm" target="_blank" rel="noopener">课程链接</a>,最近的博客基本会以这个课程的笔记为主。</p>
<h3 id="What-is-a-neural-network"><a href="#What-is-a-neural-network" class="headerlink" title="What is a neural network?"></a>What is a neural network?</h3><p>什么是神经网络，下面通过一个简单的例子引入神经网络模型的概念。</p>
<p>加入我们要建立房价的预测模型，一共有六个房子。我们已知输入x即每个房子的面积，还知道其对应的输出y即每个房子的价格。根据这些输入输出，我们要简历一个函数模型，来预测房价。根据这些输入输出，我们要建立一个函数模型，来预测房价：y=f(x)。首先，我们将已知的六间房子的价格和面积关系绘制在二维平面上，如下图所示：<br><img src="/2018/01/17/深度学习概论/housepriceprediction.png" alt="housepriceprediction"><br>一般地，我们会用一条之间来拟合图中这些离散点，即建立房价与面积的线性模型。但是从实际考虑，我们知道价格永远不会是负数。所以，我们对该之间做一点点修正，让它编程折线的形状，当面积小于某个值时，价格始终为零，如下图蓝色折线所示，这就是我们建立的房价预测模型<br><img src="/2018/01/17/深度学习概论/housepriceprediction2.png" alt="housepriceprediction2"><br>其实这个简单的模型(蓝色折线)就可以看成是一个神经网络，而且几乎是一个最简单的神经网络，我们把房价预测用一个最简单的神经网络模型来表示，如下图所示<br><img src="/2018/01/17/深度学习概论/networkneuron.png" alt="networkneuron"><br>该神经网络的输入x是房屋面积，输入y是房屋价格，中间包含了一个神经元(neuron)，房价预测函数(蓝色折线)。该神经元的功能就是实现函数f(x)的功能。</p>
<p>上图神经元的预测函数在神经网络应用中比较常见，我们把这个函数称为ReLU函数，即线性整流函数，如下图所示<br><img src="/2018/01/17/深度学习概论/ReLU.png" alt="ReLU"></p>
<p>上面讲的只是由单个神经元(输入x仅仅是房屋面积一个因素)组成的神经网络，而通常一个大型的神经网络往往由许多个神经元组成。</p>
<p>现在，我们把上面房价预测例子变得更复杂，而不仅仅使用房屋面积(size)一个判断因素。例如，除了考虑房屋面积之外，我们还考虑卧室数目(bedrooms)。这两点实际上与家庭成员的个数(family size)有关。还有房屋的邮政编码(zip code)，代表着该房屋位置的交通便利性，是否需要步行还是开车？即决定了可步行性(walkability)。另外，还有可能邮政编码和地区财富水平(wealth)共同影响了房屋所在地区的学校质量(school quality)。如下图所示，该神经网络共有三个神经元，分别代表了family size，walkability和school quality。每个神经元都包含了一个ReLU函数(或者其他非线性函数)。那么，根据这个模型，我们可以根据房屋面积和卧室个数来估计family size，根据邮政编码来估计walkability，根据邮政编码和财富水平来估计school quality。最后，由family size，walkability和school quality灯这些人们比较关心的因素来预测最终的房屋价格。<br><img src="/2018/01/17/深度学习概论/housepriceprediction3.png" alt="housepriceprediction3"><br>所以，在这个例子中，x是size，bedrooms，zip code，wealth这四个输入；y是房屋的预测价格。这个神经网络模型包含的神经元个数更多一些，相对之前的单个神经元的模型更加复杂。那么，在建立一个表现良好的神经网络模型之后，在给定输入x时，就能得到比较好的输出y，即房屋的预测价格。</p>
<p>实际上，上面这个例子神经网络模型结构如下所示，分别是size，bedrooms，zip code和wealth。在给定这四个输入后，神经网络所做的就是输出房屋的预测价格y。图中，三个神经元所在的位置称之称之为中间层或者隐藏层(x所在的称之为输入层，y所在的称之为输出层)，每个神经元与所有的输入x都有关联。<br><img src="/2018/01/17/深度学习概论/housepriceprediction4.png" alt="housepriceprediction4"><br>这就是基本的神经网络模型结构。在训练的过程中，只要有足够的输入x和输出y，就能训练出较好的神经网络模型，该模型在此类房价预测问题中，能够得到比较准确的结果。</p>
<h3 id="Supervised-Learning-with-Neural-Network"><a href="#Supervised-Learning-with-Neural-Network" class="headerlink" title="Supervised Learning with Neural Network"></a>Supervised Learning with Neural Network</h3><p>目前为止，由神经网络模型创造的价值基本上都是基于监督式学习(Supervised Learning)的，监督式学习与非监督式学习本质区别就是是否已知训练样本的输出y。在实际应用中，机器学习解决的大部分问题都属于监督式学习，神经网络模型也大都属于监督式学习。下面举几个监督式学习在神经网络中应用的例子。<br>第一个例子是房屋价格预测。根据训练样本的输入x和输出y，训练神经网络模型，预测房价。<br>第二个例子是线上广告，输入x是广告和用户个人信息，输入y是用户是否对广告进行点击。神经网络模型经过训练，能够根据广告类型和用户信息对用户的点击行为进行预测，从而向用户提供自己可能感兴趣的广告。<br>第三个例子是电脑视觉(computer vision)。输入x是图片像素值，输出是图片所属的不同类别。<br>第四个例子是语音识别(speech recognition).可以将一段语音信号辨识为相应的文字信息。<br>第五个例子是智能翻译，通过神经网络输入英文，然后直接输出中文。<br>第六个例子是自动驾驶。通过输入一张图片或者汽车雷达信息，神经网络通过训练来告诉你相应的路况并做出相应的决策。<br>如下图所示<br><img src="/2018/01/17/深度学习概论/监督式学习.png" alt="监督式学习"></p>
<p>对于不同的问题和应用场合，应该使用不同类型的神经网络模型。上面介绍的几个例子中，<br>对于一般的监督式学习(房价预测和线上广告问题)，我们只要使用标准的神经网络模型就可以了。图像识别处理问题，我们则要使用卷积神经网络(Convolution Neural Network)即CNN。而对于处理类似语音这样的序列信号时，则要使用循环神经网络(Recurrent Neural Network),即RNN。还有其他例如自动驾驶这样的复杂问题则需要更加复杂的混合神经网络模型。<br>下面给出Standard NN，Convolutional NN和Recurrent NN的神经网络结构图。<br><img src="/2018/01/17/深度学习概论/CNNRNN.png" alt="CNNRNN"><br>CNN一般处理图像问题，RNN一般处理语音信号。</p>
<p>数据类型一般分为两种：结构化数据(Structured Data)和非结构化数据(Unstructured Data)。<br><img src="/2018/01/17/深度学习概论/结构化数据与非结构化数据.png" alt="结构化数据与非结构化数据"></p>
<h3 id="Why-is-Deep-Learning-taking-off"><a href="#Why-is-Deep-Learning-taking-off" class="headerlink" title="Why is Deep Learning taking off?"></a>Why is Deep Learning taking off?</h3><p>深度学习和神经网络背后的技术思想已经出现了数十年了，那么为什么直到现在才开始发挥作用呢？<br>深度学习为什么强大？下面我们用一张图来说明。如下图所示，横坐标x表示数据量(Amount of data),纵坐标y表示机器学习模型的性能表现(Performance)。<br><img src="/2018/01/17/深度学习概论/Performance.png" alt="Performance"><br>上面共有4条曲线。最底下那条红色曲线代表了传统机器学习算法的表现，例如是SVM，logistic regression，decision tree等，当数据量比较小的时候，传统学习模型的表现是比较好的，但是当数据量很大的时候表现很一般。黄色曲线代表了规模较小的神经网络模型(Small NN)。它在数据量大的时候性能优于传统机器学习。蓝色曲线代表了规模中等的神经网络模型(Media NN),它在数据量更大的时候表现比Small NN更好。绿色曲线代表了更大规模的神经网络(Large NN),即深度学习模型。从图中可以看出，在数据量很大的时候，它的表现仍然是最好的，而且基本上保持了较快的上升趋势。<br>深度学习强大的原因归结为3个因素：<br><strong>Data</strong><br><strong>Computation</strong><br><strong>Algorithms</strong><br>数据量的几何级数增加<br>GPU出现，计算机运算能力大大提升。<br>举个算法改进的例子：之前神经网络神经元的激活函数是Sigmoid函数，后来改成了ReLU函数。更改的原因是对于Sigmoid函数，在远离Sigmoid函数，后来改成了ReLU函数。之所以这样更改的原因是对于Sigmoid函数，在远离零点的位置，函数曲线非常平缓，其梯度始终为0，所以造成神经网络模型学习速度变得很慢。然而，ReLU函数在x大于零的区域，其梯度始终为1，尽管x在小于零的区域梯度为0，但是在实际应用中采用ReLU函数确实比Sigmoid函数快很多。<br>构建一个深度学习的流程是首先开始产生Idea，然后把Idea转化为Code，最后进行Experiment。根据结果修改Idea，继续这种循环，直到最终训练得到表现不错的深度学习网络模型。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2017/04/02/自顶向下之TCP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/02/自顶向下之TCP/" itemprop="url">
                  自顶向下之TCP
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-02T21:22:00+08:00">
                2017-04-02
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/04/02/自顶向下之TCP/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/04/02/自顶向下之TCP/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="TCP服务"><a href="#TCP服务" class="headerlink" title="TCP服务"></a>TCP服务</h3><h4 id="进程到进程的通信"><a href="#进程到进程的通信" class="headerlink" title="进程到进程的通信"></a>进程到进程的通信</h4><p>TCP通过使用端口号来提供进程到进程间通信</p>
<h4 id="流传递服务"><a href="#流传递服务" class="headerlink" title="流传递服务"></a>流传递服务</h4><p>TCP是一个面向流的协议,TCP允许发送进程以字节流的形式传递数据，并且接受进程也以字节流的形式接受数据</p>
<h4 id="全双工通信"><a href="#全双工通信" class="headerlink" title="全双工通信"></a>全双工通信</h4><p>数据可以在同一事件双向流动，每一个方向TCP都有发送和接受缓冲区，他们能双向发送和接受段</p>
<h4 id="多路复用和多路分解"><a href="#多路复用和多路分解" class="headerlink" title="多路复用和多路分解"></a>多路复用和多路分解</h4><p>TCP在发送端执行多路复用，在接收端执行多路分解</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2017/03/19/图解TCPIP-TCP与UDP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/19/图解TCPIP-TCP与UDP/" itemprop="url">
                  图解TCP/IP之TCP与UDP
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-03-19T22:06:00+08:00">
                2017-03-19
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/03/19/图解TCPIP-TCP与UDP/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/03/19/图解TCPIP-TCP与UDP/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="TCP与UDP"><a href="#TCP与UDP" class="headerlink" title="TCP与UDP"></a>TCP与UDP</h3><h4 id="传输层的作用"><a href="#传输层的作用" class="headerlink" title="传输层的作用"></a>传输层的作用</h4><h5 id="传输层定义"><a href="#传输层定义" class="headerlink" title="传输层定义"></a>传输层定义</h5><p>IP首部有一个协议字段，用来标识网络层的上一层所采用的是哪一种传输层协议</p>
<h4 id=""><a href="#" class="headerlink" title="#"></a>#</h4>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2017/03/19/图解TCPIP-IP协议相关技术/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/19/图解TCPIP-IP协议相关技术/" itemprop="url">
                  图解TCP/IP-IP协议相关技术
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-03-19T00:28:00+08:00">
                2017-03-19
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/03/19/图解TCPIP-IP协议相关技术/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/03/19/图解TCPIP-IP协议相关技术/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="IP协议相关技术"><a href="#IP协议相关技术" class="headerlink" title="IP协议相关技术"></a>IP协议相关技术</h3><h4 id="仅凭IP无法完成通信"><a href="#仅凭IP无法完成通信" class="headerlink" title="仅凭IP无法完成通信"></a>仅凭IP无法完成通信</h4><h4 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h4><h5 id="IP地址不便记忆"><a href="#IP地址不便记忆" class="headerlink" title="IP地址不便记忆"></a>IP地址不便记忆</h5><h5 id="DNS的产生"><a href="#DNS的产生" class="headerlink" title="DNS的产生"></a>DNS的产生</h5><p>当用户输入域名时，DNS会自动检索那个注册了主机名和IP地址 的数据库，并迅速定位对应的IP地址</p>
<h5 id="域名的构成"><a href="#域名的构成" class="headerlink" title="域名的构成"></a>域名的构成</h5><p>域名的分成结构<br><strong>域名服务器</strong><br>域名服务器值管理域名的主机和相应软件，它可以管理所在分层的域的相关信息<br>其所管理的分层叫做ZONE<br>各个域的分层上都设有各自的域名服务器<br>各层域名服务器都了解以下分层所有域名服务器的IP地址，因此他们从根域名服务器开始呈树状结构相互连接<br>由于所有域名服务器都了解根域名服务器的IP地址，所以从根开始按照顺序追踪，可以访问世界上所有域名服务器的地址<br><strong>根据DNS协议，根域名服务器可以由13个IP地址表示，并且从A到M开始，然后由于IP任播可以为多个结点设置同一个IP地址，为了提高容灾能力和负载均衡能力</strong></p>
<h5 id="DNS查询"><a href="#DNS查询" class="headerlink" title="DNS查询"></a>DNS查询</h5><p>计算机pepper要访问www.ietf.org</p>
<ol>
<li>解析器为了调查IP地址，向域名服务器进行查询处理</li>
<li>接受到这个查询请求的域名服务器首先在自己的数据库进行查找，如果有则返回，没有则向上一级根域名服务器进行查询<h5 id="DNS如同互联网中的分布式数据库"><a href="#DNS如同互联网中的分布式数据库" class="headerlink" title="DNS如同互联网中的分布式数据库"></a>DNS如同互联网中的分布式数据库</h5><h4 id="ARP"><a href="#ARP" class="headerlink" title="ARP"></a>ARP</h4><h5 id="ARP概要"><a href="#ARP概要" class="headerlink" title="ARP概要"></a>ARP概要</h5>ARP是一种解决地址问题的协议，用来定位下一个应该接受数据分包的网络设备对应的MAC地址<h5 id="ARP的工作机制"><a href="#ARP的工作机制" class="headerlink" title="ARP的工作机制"></a>ARP的工作机制</h5>假定主机A要向主机B发送IP包，主机A的IP地址为172.20.1.1，主机B的IP地址为172.20.1.2<br>主机A为了获得主机B的MAC地址，起初要通过广播发送一个ARP请求，这个包包含了主机的IP地址<br>广播的包可以被同意链路上所有主机或路由器接受，所有主机都会解析这个包，只有当这个包中IP地址和自己的IP地址一致，那么这个结点就将自己的MAC地址塞进ARP响应返回给主机A<h5 id="IP地址和MAC地址缺一不可？"><a href="#IP地址和MAC地址缺一不可？" class="headerlink" title="IP地址和MAC地址缺一不可？"></a>IP地址和MAC地址缺一不可？</h5>数据链路上只要知道接收端的MAC地址不久知道数据是准备发给主机B的吗，那么还需要知道它的IP地址吗？<br>主机A想发送数据报给主机B时必须得经过路由器C，即时知道B的MAC地址没有用，路由器C会隔断来两个网络，此时主机A要先得到C的MAC地址<h5 id="RARP"><a href="#RARP" class="headerlink" title="RARP"></a>RARP</h5>从MAC地址定位到IP地址的一种协议，将打印机等小型嵌入式设备连接到网络会用到<h5 id="代理ARP"><a href="#代理ARP" class="headerlink" title="代理ARP"></a>代理ARP</h5><h4 id="ICMP"><a href="#ICMP" class="headerlink" title="ICMP"></a>ICMP</h4><h5 id="辅助IP的ICMP"><a href="#辅助IP的ICMP" class="headerlink" title="辅助IP的ICMP"></a>辅助IP的ICMP</h5>主要功能：</li>
<li>确认IP包是否成功送达目标地址</li>
<li>通知在发送过程中IP包被废弃的具体原因</li>
<li>改善网络设置<br>ICMP只负责其中IP相关的设置<br>ICMP这种通知消息会使用IP进行发送<br>ICMP的消息大致分为两类：</li>
<li>通知出错原因的错误消息</li>
<li>用于诊断的查询消息<h5 id="主要的ICMP消息"><a href="#主要的ICMP消息" class="headerlink" title="主要的ICMP消息"></a>主要的ICMP消息</h5><strong>ICMP目标不可达消息</strong><br><strong>ICMP重定向消息</strong><br><strong>ICMP超时消息</strong><br><strong>ICMP回送消息</strong><h5 id="其他ICMP消息"><a href="#其他ICMP消息" class="headerlink" title="其他ICMP消息"></a>其他ICMP消息</h5><strong>ICMP原点抑制消息</strong><br><strong>ICMP路由探索消息</strong><br><strong>ICMP地址掩码消息</strong><h5 id="ICMPv6"><a href="#ICMPv6" class="headerlink" title="ICMPv6"></a>ICMPv6</h5><strong>ICMPv6的作用</strong><br><strong>邻居探索</strong><h4 id="NAT"><a href="#NAT" class="headerlink" title="NAT"></a>NAT</h4><h5 id="NAT定义"><a href="#NAT定义" class="headerlink" title="NAT定义"></a>NAT定义</h5>Network Address Translator是用于本地网络的私有协议，在连接互联网时转而使用全局IP地址的技术，除转换IP地址外，还出现了可以转换TCP，UDP端口号的NAPT技术，由此可以实现用一个全局IP地址与多个主机通信<h5 id="NAT的工作机制"><a href="#NAT的工作机制" class="headerlink" title="NAT的工作机制"></a>NAT的工作机制</h5><h5 id="NAT-PT"><a href="#NAT-PT" class="headerlink" title="NAT-PT"></a>NAT-PT</h5><h5 id="NAT的潜在问题"><a href="#NAT的潜在问题" class="headerlink" title="NAT的潜在问题"></a>NAT的潜在问题</h5><h5 id="解决NAT的潜在问题与NAT穿越"><a href="#解决NAT的潜在问题与NAT穿越" class="headerlink" title="解决NAT的潜在问题与NAT穿越"></a>解决NAT的潜在问题与NAT穿越</h5><h4 id="IP隧道"><a href="#IP隧道" class="headerlink" title="IP隧道"></a>IP隧道</h4><img src="http://omjy3y3o5.bkt.clouddn.com/543a8ec0cfe6ab2940a9881cee2e0685.png" alt="ip"><br>在如图所示的网络环境中，网络A、B使用IPv6，如果处于中间位置的网络C支持使用IPv4的话，网络A与网络B之间将无法直接进行通信，这时候就需要采用IP隧道的功能<br>IP隧道可以降那些从网络A发过来的IPv6的包统和为一个数据，再为之追加一个IPv6的首部以后转发给网络C<br>一般情况下，紧接着IP首部的是TCP或UDP的首部，然而现在的应用中IP首部后面还是IP首部，这种网络层首部后面继续追加网络层首部的通信方法叫做IP隧道<h4 id="其他IP相关技术"><a href="#其他IP相关技术" class="headerlink" title="其他IP相关技术"></a>其他IP相关技术</h4><h5 id="IP多播相关技术"><a href="#IP多播相关技术" class="headerlink" title="IP多播相关技术"></a>IP多播相关技术</h5><h5 id="IP任播"><a href="#IP任播" class="headerlink" title="IP任播"></a>IP任播</h5><h5 id="通信质量控制"><a href="#通信质量控制" class="headerlink" title="通信质量控制"></a>通信质量控制</h5><h5 id="显式拥塞通知"><a href="#显式拥塞通知" class="headerlink" title="显式拥塞通知"></a>显式拥塞通知</h5><h5 id="Mobile-IP"><a href="#Mobile-IP" class="headerlink" title="Mobile IP"></a>Mobile IP</h5></li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://andeper.cn/2017/03/18/Android开发艺术探索-理解Window和WindowManager/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Andeper">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/update/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Andeper的个人博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/03/18/Android开发艺术探索-理解Window和WindowManager/" itemprop="url">
                  Android开发艺术探索-理解Window和WindowManager
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-03-18T16:53:00+08:00">
                2017-03-18
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/03/18/Android开发艺术探索-理解Window和WindowManager/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2017/03/18/Android开发艺术探索-理解Window和WindowManager/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="第八章：理解Window和WindowManager"><a href="#第八章：理解Window和WindowManager" class="headerlink" title="第八章：理解Window和WindowManager"></a>第八章：理解Window和WindowManager</h3><p>Window表示一个窗口的概念。Window是一个抽象类，它的具体实现是PhoneWindow，创建一个Window在WindowManager中完成，但Window的具体实现是在WindowManagerService中，WindowManager和WindowManagerService的交互是一个IPC过程。Andorid的所有视图都是通过Window来呈现的</p>
<h4 id="Window和WindowManager"><a href="#Window和WindowManager" class="headerlink" title="Window和WindowManager"></a>Window和WindowManager</h4><p>下面代码通过WindowManager添加一个Window<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Button mFloatingButton = <span class="keyword">new</span> Button(<span class="keyword">this</span>);</span><br><span class="line">mFloatingButton.setText(<span class="string">"button"</span>);</span><br><span class="line">WindowManager.LayoutParams mLayoutParams = <span class="keyword">new</span> WindowManager.LayoutParams(WindowManager.LayoutParams.WRAP_CONTENT, WindowManager.LayoutParams.WRAP_CONTENT,<span class="number">0</span>,<span class="number">0</span>, PixelFormat.TRANSPARENT);</span><br><span class="line">mLayoutParams.flags = WindowManager.LayoutParams.FLAG_NOT_FOCUSABLE| WindowManager.LayoutParams.FLAG_NOT_TOUCH_MODAL| WindowManager.LayoutParams.FLAG_SHOW_WHEN_LOCKED;</span><br><span class="line">mLayoutParams.gravity = Gravity.LEFT|Gravity.TOP;</span><br><span class="line">mLayoutParams.x = <span class="number">100</span>;</span><br><span class="line">mLayoutParams.y = <span class="number">300</span>;</span><br><span class="line">WindowManager windowManager = getWindowManager();</span><br><span class="line">windowManager.addView(mFloatingButton,mLayoutParams);</span><br></pre></td></tr></table></figure></p>
<p>上述代码将Button添加到屏幕坐标为（100，300）的位置上，WindowManager.LayoutParams中的flag和type参数比较重要<br>Flags参数代表Window的属性<br>FLAG_NOT_FOCUSABLE<br>表示Window不需要获取焦点，也不需要接收各种输入事件，此标记会同时启用FLAG_NOT_TOUCH_MODAL。最终事件会直接传递给下层的具有焦点的Window<br>FLAG_NOT_TOUCH_MODAL<br>再次模式下，系统会将当前Window区域以外的单击事件传递给底层的Window，当前Window区域以内的单击事件则自己处理，一般需要开启，否则其他Window将无法收到单击事件<br>FLAG_SHOW_WHEN_LOCKED<br>开启此模式可以让Window显示在锁屏的界面上</p>
<p>Type参数表示Window的类型，Window有三种类型，分别是应用Window、子Window和系统Window<br>Window是分层的，每个WIndow都有对应的z-ordered，在三类Window中，应用Window的层级范围是1-99，子Window的范围哦是1000-1999，系统Window的层级范围是2000-2999<br>WindowManager只有三个功能。定义在ViewManager中<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ViewManager</span></span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">addView</span><span class="params">(View view, ViewGroup.LayoutParams params)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">updateViewLayout</span><span class="params">(View view, ViewGroup.LayoutParams params)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">removeView</span><span class="params">(View view)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>WindowManager操作Window更像是在操作Window中的View</p>
<h4 id="Window的内部机制"><a href="#Window的内部机制" class="headerlink" title="Window的内部机制"></a>Window的内部机制</h4><p>每个Window都对应着一个View和一个ViewRootImpl，Window和View通过ViewRootImpl来建立联系，Window是一个抽象的概念，View才是Window存在的实体。</p>
<h5 id="Window的添加过程"><a href="#Window的添加过程" class="headerlink" title="Window的添加过程"></a>Window的添加过程</h5><p>WindowManager的实现类WindowManagerImpl没有实现Window的三大操作，而是交给WindowManagerGlobal来处理，addView方法主要分为以下几步</p>
<h6 id="检查参数是否合法，如果是子Window还需要调整布局参数"><a href="#检查参数是否合法，如果是子Window还需要调整布局参数" class="headerlink" title="检查参数是否合法，如果是子Window还需要调整布局参数"></a>检查参数是否合法，如果是子Window还需要调整布局参数</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (view == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"view must not be null"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (display == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"display must not be null"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (!(params <span class="keyword">instanceof</span> WindowManager.LayoutParams)) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"Params must be WindowManager.LayoutParams"</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> WindowManager.LayoutParams wparams = (WindowManager.LayoutParams) params;</span><br><span class="line">        <span class="keyword">if</span> (parentWindow != <span class="keyword">null</span>) &#123;</span><br><span class="line">            parentWindow.adjustLayoutParamsForSubWindow(wparams);</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h6 id="创建ViewRootImpl并将View添加到列表中"><a href="#创建ViewRootImpl并将View添加到列表中" class="headerlink" title="创建ViewRootImpl并将View添加到列表中"></a>创建ViewRootImpl并将View添加到列表中</h6><p>WindowManagerGlobal内部下面几个列表比较重要<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> ArrayList&lt;View&gt; mViews = <span class="keyword">new</span> ArrayList&lt;View&gt;();</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> ArrayList&lt;ViewRootImpl&gt; mRoots = <span class="keyword">new</span> ArrayList&lt;ViewRootImpl&gt;();</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> ArrayList&lt;WindowManager.LayoutParams&gt; mParams =</span><br><span class="line">            <span class="keyword">new</span> ArrayList&lt;WindowManager.LayoutParams&gt;();</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> ArraySet&lt;View&gt; mDyingViews = <span class="keyword">new</span> ArraySet&lt;View&gt;();</span><br></pre></td></tr></table></figure></p>
<p>mViews存储所有Window对应的View<br>mRoots存储的是所有Window对应的ViewRootImpl<br>mParams存储所有对应的布局参数<br>mDyingViews则存储那些正在被删除的View对象。<br>addView通过如下方式将Window的一系列对象添加到列表中<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root = <span class="keyword">new</span> ViewRootImpl(view.getContext(), display);</span><br><span class="line">view.setLayoutParams(wparams);</span><br><span class="line">mViews.add(view);</span><br><span class="line">mRoots.add(root);</span><br><span class="line">mParams.add(wparams);</span><br></pre></td></tr></table></figure></p>
<h6 id="通过ViewRootImpl来更新界面并完成Window的添加过程"><a href="#通过ViewRootImpl来更新界面并完成Window的添加过程" class="headerlink" title="通过ViewRootImpl来更新界面并完成Window的添加过程"></a>通过ViewRootImpl来更新界面并完成Window的添加过程</h6><p>由ViewRootImpl的setView方法来完成.<br>View的绘制过程由ViewRootImpl的setView完成的<br>在setView内部会通过requestLayout来完成异步刷新请求的<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">requestLayout</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (!mHandlingLayoutInLayoutRequest) &#123;</span><br><span class="line">            checkThread();</span><br><span class="line">            mLayoutRequested = <span class="keyword">true</span>;</span><br><span class="line">            scheduleTraversals();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></p>
<p>scheduleTraversals是View绘制的入口，接着会通过WindowSession最终来完成Window的添加过程<br>mWindowSession的类型是IWindowSession，它是一个Binder对象，真正的实现类是Session，也就是Window的添加过程是一次IPC调用<br>在Session内部会通过WindowManagerService来实现Window的添加</p>
<h5 id="Window的删除过程"><a href="#Window的删除过程" class="headerlink" title="Window的删除过程"></a>Window的删除过程</h5><p>真正删除View 的逻辑在dispatchDetachedFromWindow，重要做四件事情</p>
<ol>
<li>垃圾回收相关工作，比如清除数据和消息，移除回调</li>
<li>通过Session的remove方法删除View，这是一个IPC过程，最终会调用WindowManagerService的removeWindow方法</li>
<li>调用View的dispatchDetachedFromWindow调用回调，让用户在回调方法里做资源回收工作</li>
<li>调用WindowManagerGlobal的doRemoveView方法刷新数据，包括mRoots、mParams、以及mDyingViews</li>
</ol>
<h5 id="Window的更新过程"><a href="#Window的更新过程" class="headerlink" title="Window的更新过程"></a>Window的更新过程</h5><p>调用WindowManagerGlobal的updateViewLayout方法，首先更新View的LayoutParams并替换老的LayoutParams，接着再更新ViewRootImpl中的LayoutParams</p>
<h4 id="Window的创建过程"><a href="#Window的创建过程" class="headerlink" title="Window的创建过程"></a>Window的创建过程</h4><p>View是Android中的师徒的呈现方式，View必须附着在Window这个抽象的概念上面</p>
<h5 id="Activity的Window创建过程"><a href="#Activity的Window创建过程" class="headerlink" title="Activity的Window创建过程"></a>Activity的Window创建过程</h5><p>PhoneWindow的setContentView方法大致遵循如下几个步骤<br>1.如果没有DecorView，那么创建它<br>DecorView算是一个FrameLayout，它包含标题栏和内容栏，DecorView的创建过程由installDecor方法来完成，再方法内部会通过generateDecor方法来直接创建DecorView，这个时候DecorView还是一个空白的FrameLayout。<br>2.将View添加到DecorView的mContentParent中<br>3.回调Activity的onContentChanged方法通过Activity视图已经发生改变</p>
<h5 id="Dialog的Window创建过程"><a href="#Dialog的Window创建过程" class="headerlink" title="Dialog的Window创建过程"></a>Dialog的Window创建过程</h5><p>1.创建Window<br>2.初始化DecorView并将Dialog的视图添加到DecorView中<br>3.将DecorView添加到Window中并显示</p>
<h5 id="Toast的Window创建过程"><a href="#Toast的Window创建过程" class="headerlink" title="Toast的Window创建过程"></a>Toast的Window创建过程</h5><p>再Toast内部有两类IPC过程，第一类是Toast访问NotificationManagerService，第二类是NotificationManagerService回调Toast 的TN接口</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>


          
          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/update/avatar.jpg"
               alt="Andeper" />
          <p class="site-author-name" itemprop="name">Andeper</p>
           
              <p class="site-description motion-element" itemprop="description">欢迎来到我的技术博客</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">25</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/tags/index.html">
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">4</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Andeper</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"andeper"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  













  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  
  
  
  <link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css">

  
  
  <script src="/lib/algolia-instant-search/instantsearch.min.js"></script>
  

  <script src="/js/src/algolia-search.js?v=5.1.0"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



  

</body>
</html>
